<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="John Fox and Georges Monette" />

<meta name="date" content="2025-06-15" />

<title>Computational and technical notes on cross-validating regression models</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Computational and technical notes on
cross-validating regression models</h1>
<h4 class="author">John Fox and Georges Monette</h4>
<h4 class="date">2025-06-15</h4>



<div id="efficient-computations-for-linear-and-generalized-linear-models" class="section level2">
<h2>Efficient computations for linear and generalized linear models</h2>
<p>The most straightforward way to implement cross-validation in R for
statistical modeling functions that are written in the canonical manner
is to use <code>update()</code> to refit the model with each fold
removed. This is the approach taken in the default method for
<code>cv()</code>, and it is appropriate if the cases are independently
sampled. Refitting the model in this manner for each fold is generally
feasible when the number of folds in modest, but can be prohibitively
costly for leave-one-out cross-validation when the number of cases is
large.</p>
<p>The <code>&quot;lm&quot;</code> and <code>&quot;glm&quot;</code> methods for
<code>cv()</code> take advantage of computational efficiencies by
avoiding refitting the model with each fold removed. Consider, in
particular, the weighted linear model <span class="math inline">\(\mathbf{y}_{n \times 1} = \mathbf{X}_{n \times
p}\boldsymbol{\beta}_{p \times 1} + \boldsymbol{\varepsilon}_{n \times
1}\)</span>, where <span class="math inline">\(\boldsymbol{\varepsilon}
\sim \mathbf{N}_n \left(\mathbf{0}, \sigma^2 \mathbf{W}^{-1}_{n \times
n}\right)\)</span>. Here, <span class="math inline">\(\mathbf{y}\)</span> is the response vector, <span class="math inline">\(\mathbf{X}\)</span> the model matrix, and <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> the error
vector, each for <span class="math inline">\(n\)</span> cases, and <span class="math inline">\(\boldsymbol{\beta}\)</span> is the vector of <span class="math inline">\(p\)</span> population regression coefficients. The
errors are assumed to be multivariately normally distributed with 0
means and covariance matrix <span class="math inline">\(\sigma^2
\mathbf{W}^{-1}\)</span>, where <span class="math inline">\(\mathbf{W} =
\mathrm{diag}(w_i)\)</span> is a diagonal matrix of inverse-variance
weights. For the linear model with constant error variance, the weight
matrix is taken to be <span class="math inline">\(\mathbf{W} =
\mathbf{I}_n\)</span>, the order-<span class="math inline">\(n\)</span>
identity matrix.</p>
<p>The weighted-least-squares (WLS) estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is <span class="citation">(see, e.g., Fox, 2016, sec. 12.2.2)</span> <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <span class="math display">\[
\mathbf{b}_{\mathrm{WLS}} = \left( \mathbf{X}^T \mathbf{W} \mathbf{X}
\right)^{-1}
  \mathbf{X}^T \mathbf{W} \mathbf{y}
\]</span></p>
<p>Fitted values are then <span class="math inline">\(\widehat{\mathbf{y}} =
\mathbf{X}\mathbf{b}_{\mathrm{WLS}}\)</span>.</p>
<p>The LOO fitted value for the <span class="math inline">\(i\)</span>th
case can be efficiently computed by <span class="math inline">\(\widehat{y}_{-i} = y_i - e_i/(1 - h_i)\)</span>
where <span class="math inline">\(h_i = \mathbf{x}^T_i \left(
\mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{x}_i\)</span>
(the so-called “hatvalue”). Here, <span class="math inline">\(\mathbf{x}^T_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span>, and <span class="math inline">\(\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th row written as a column vector. This
approach can break down when one or more hatvalues are equal to 1, in
which case the formula for <span class="math inline">\(\widehat{y}_{-i}\)</span> requires division by
0.</p>
<p>To compute cross-validated fitted values when the folds contain more
than one case, we make use of the Woodbury matrix identity <span class="citation">(Hager, 1989)</span>, <span class="math display">\[
\left(\mathbf{A}_{m \times m} + \mathbf{U}_{m \times k}
\mathbf{C}_{k \times k} \mathbf{V}_{k \times m} \right)^{-1} =
\mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U} \left(\mathbf{C}^{-1} +
\mathbf{VA}^{-1}\mathbf{U} \right)^{-1} \mathbf{VA}^{-1}
\]</span> where <span class="math inline">\(\mathbf{A}\)</span> is a
nonsingular order-<span class="math inline">\(n\)</span> matrix. We
apply this result by letting <span class="math display">\[\begin{align*}
    \mathbf{A} &amp;= \mathbf{X}^T \mathbf{W} \mathbf{X} \\
    \mathbf{U} &amp;= \mathbf{X}_\mathbf{j}^T \\
    \mathbf{V} &amp;= - \mathbf{X}_\mathbf{j} \\
    \mathbf{C} &amp;= \mathbf{W}_\mathbf{j} \\
\end{align*}\]</span> where the subscript <span class="math inline">\(\mathbf{j} = (i_{j1}, \ldots, i_{jm})^T\)</span>
represents the vector of indices for the cases in the <span class="math inline">\(j\)</span>th fold, <span class="math inline">\(j =
1, \ldots, k\)</span>. The negative sign in <span class="math inline">\(\mathbf{V} = - \mathbf{X}_\mathbf{j}\)</span>
reflects the <em>removal</em>, rather than addition, of the cases in
<span class="math inline">\(\mathbf{j}\)</span>.</p>
<p>Applying the Woodbury identity isn’t quite as fast as using the
hatvalues, but it is generally much faster than refitting the model. A
disadvantage of the Woodbury identity, however, is that it entails
explicit matrix inversion and thus may be numerically unstable. The
inverse of <span class="math inline">\(\mathbf{A} = \mathbf{X}^T
\mathbf{W} \mathbf{X}\)</span> is available directly in the
<code>&quot;lm&quot;</code> object, but the second term on the right-hand side of
the Woodbury identity requires a matrix inversion with each fold
deleted. (In contrast, the inverse of each <span class="math inline">\(\mathbf{C} = \mathbf{W}_\mathbf{j}\)</span> is
straightforward because <span class="math inline">\(\mathbf{W}\)</span>
is diagonal.)</p>
<p>The Woodbury identity also requires that the model matrix be of full
rank. We impose that restriction in our code by removing redundant
regressors from the model matrix for all of the cases, but that doesn’t
preclude rank deficiency from surfacing when a fold is removed. Rank
deficiency of <span class="math inline">\(\mathbf{X}\)</span> doesn’t
disqualify cross-validation because all we need are fitted values under
the estimated model.</p>
<p><code>glm()</code> computes the maximum-likelihood estimates for a
generalized linear model by iterated weighted least squares <span class="citation">(see, e.g., Fox &amp; Weisberg, 2019, sec.
6.12)</span>. The last iteration is therefore just a WLS fit of the
“working response” on the model matrix using “working weights.” Both the
working weights and the working response at convergence are available
from the information in the object returned by <code>glm()</code>.</p>
<p>We then treat re-estimation of the model with a case or cases deleted
as a WLS problem, using the hatvalues or the Woodbury matrix identity.
The resulting fitted values for the deleted fold aren’t exact—that is,
except for the Gaussian family, the result isn’t identical to what we
would obtain by literally refitting the model—but in our (limited)
experience, the approximation is very good, especially for LOO CV, which
is when we would be most tempted to use it. Nevertheless, because these
results are approximate, the default for the <code>&quot;glm&quot;</code>
<code>cv()</code> method is to perform the exact computation, which
entails refitting the model with each fold omitted.</p>
<p>Let’s compare the efficiency of the various computational methods for
linear and generalized linear models. Consider, for example,
leave-one-out cross-validation for the quadratic regression of
<code>mpg</code> on <code>horsepower</code> in the <code>Auto</code>
data, from the introductory “Cross-validating regression models”
vignette, repeated here:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Auto&quot;</span>, <span class="at">package=</span><span class="st">&quot;ISLR2&quot;</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>m.auto <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">summary</span>(m.auto)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 2), data = Auto)</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co">#&gt; poly(horsepower, 2)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co">#&gt; poly(horsepower, 2)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;cv&quot;</span>)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cv</span>(m.auto, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>) ) <span class="co"># default method = &quot;hatvalues&quot;</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a><span class="co">#&gt; method: hatvalues</span></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co">#&gt; criterion: mse</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cv</span>(m.auto, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>, <span class="at">method =</span> <span class="st">&quot;naive&quot;</span>))</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a><span class="co">#&gt; method: naive</span></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a><span class="co">#&gt; criterion: mse</span></span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a><span class="co">#&gt; full-sample criterion = 18.985</span></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cv</span>(m.auto, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>, <span class="at">method =</span> <span class="st">&quot;Woodbury&quot;</span>))</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a><span class="co">#&gt; method: Woodbury</span></span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a><span class="co">#&gt; criterion: mse</span></span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>This is a small regression problem and all three computational
approaches are essentially instantaneous, but it is still of interest to
investigate their relative speed. In this comparison, we include the
<code>cv.glm()</code> function from the <strong>boot</strong> package
<span class="citation">(Canty &amp; Ripley, 2022; Davison &amp; Hinkley,
1997)</span>, which takes the naive approach, and for which we have to
fit the linear model as an equivalent Gaussian GLM. We use the
<code>microbenchmark()</code> function from the package of the same name
for the timings <span class="citation">(Mersmann, 2023)</span>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>m.auto.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">2</span>), <span class="at">data =</span> Auto)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>boot<span class="sc">::</span><span class="fu">cv.glm</span>(Auto, m.auto.glm)<span class="sc">$</span>delta</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co">#&gt; [1] 19.248 19.248</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>microbenchmark<span class="sc">::</span><span class="fu">microbenchmark</span>(</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>  <span class="at">hatvalues =</span> <span class="fu">cv</span>(m.auto, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>),</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>  <span class="at">Woodbury =</span> <span class="fu">cv</span>(m.auto, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>, <span class="at">method =</span> <span class="st">&quot;Woodbury&quot;</span>),</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>  <span class="at">naive =</span> <span class="fu">cv</span>(m.auto, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>, <span class="at">method =</span> <span class="st">&quot;naive&quot;</span>),</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>  <span class="at">cv.glm =</span> boot<span class="sc">::</span><span class="fu">cv.glm</span>(Auto, m.auto.glm),</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>  <span class="at">times =</span> <span class="dv">10</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co">#&gt; Warning in microbenchmark::microbenchmark(hatvalues = cv(m.auto, k = &quot;loo&quot;), :</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co">#&gt; less accurate nanosecond times to avoid potential integer overflows</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="co">#&gt; Unit: milliseconds</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="co">#&gt;       expr      min       lq     mean   median       uq      max neval cld</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co">#&gt;  hatvalues   1.8379   2.0744   2.0789   2.1138   2.1473   2.1824    10 a  </span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="co">#&gt;   Woodbury  14.1628  14.2439  16.3909  14.6271  19.1500  20.4607    10 a  </span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co">#&gt;      naive 215.9824 223.4145 224.0820 225.2306 226.3134 229.5120    10  b </span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co">#&gt;     cv.glm 376.1002 380.2579 393.1865 382.2935 388.2199 485.8424    10   c</span></span></code></pre></div>
<p>On our computer, using the hatvalues is about an order of magnitude
faster than employing Woodbury matrix updates, and more than two orders
of magnitude faster than refitting the model.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>Similarly, let’s return to the logistic-regression model fit to
Mroz’s data on women’s labor-force participation, also employed as an
example in the introductory vignette:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;Mroz&quot;</span>, <span class="at">package=</span><span class="st">&quot;carData&quot;</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>m.mroz <span class="ot">&lt;-</span> <span class="fu">glm</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> Mroz, <span class="at">family =</span> binomial)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="fu">summary</span>(m.mroz)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</span></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a><span class="co">#&gt; AIC: 921.3</span></span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cv</span>(m.mroz, <span class="co"># default method = &quot;exact&quot;</span></span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a>   <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>, </span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>   <span class="at">criterion =</span> BayesRule))</span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a><span class="co">#&gt; method: exact</span></span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a><span class="co">#&gt; criterion: BayesRule</span></span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cv</span>(m.mroz,</span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a>   <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>,</span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a>   <span class="at">criterion =</span> BayesRule,</span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a>   <span class="at">method =</span> <span class="st">&quot;Woodbury&quot;</span>))</span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a><span class="co">#&gt; method: Woodbury</span></span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a><span class="co">#&gt; criterion: BayesRule</span></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span id="cb3-47"><a href="#cb3-47" tabindex="-1"></a><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span id="cb3-48"><a href="#cb3-48" tabindex="-1"></a><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span id="cb3-49"><a href="#cb3-49" tabindex="-1"></a><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span id="cb3-50"><a href="#cb3-50" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">cv</span>(m.mroz,</span>
<span id="cb3-51"><a href="#cb3-51" tabindex="-1"></a>   <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>,</span>
<span id="cb3-52"><a href="#cb3-52" tabindex="-1"></a>   <span class="at">criterion =</span> BayesRule,</span>
<span id="cb3-53"><a href="#cb3-53" tabindex="-1"></a>   <span class="at">method =</span> <span class="st">&quot;hatvalues&quot;</span>))</span>
<span id="cb3-54"><a href="#cb3-54" tabindex="-1"></a><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span id="cb3-55"><a href="#cb3-55" tabindex="-1"></a><span class="co">#&gt; method: hatvalues</span></span>
<span id="cb3-56"><a href="#cb3-56" tabindex="-1"></a><span class="co">#&gt; criterion: BayesRule</span></span>
<span id="cb3-57"><a href="#cb3-57" tabindex="-1"></a><span class="co">#&gt; cross-validation criterion = 0.32005</span></span></code></pre></div>
<p>As for linear models, we report some timings for the various
<code>cv()</code> methods of computation in LOO CV as well as for the
<code>cv.glm()</code> function from the <strong>boot</strong> package
(which, recall, refits the model with each case removed, and thus is
comparable to <code>cv()</code> with <code>method=&quot;exact&quot;</code>):</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>microbenchmark<span class="sc">::</span><span class="fu">microbenchmark</span>(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  <span class="at">hatvalues =</span> <span class="fu">cv</span>(</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    m.mroz,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>,</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    <span class="at">criterion =</span> BayesRule,</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;hatvalues&quot;</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>  ),</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>  <span class="at">Woodbury =</span> <span class="fu">cv</span>(</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    m.mroz,</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>    <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>,</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="at">criterion =</span> BayesRule,</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;Woodbury&quot;</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>  ),</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>  <span class="at">exact =</span> <span class="fu">cv</span>(m.mroz, <span class="at">k =</span> <span class="st">&quot;loo&quot;</span>, <span class="at">criterion =</span> BayesRule),</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>  <span class="at">cv.glm =</span> boot<span class="sc">::</span><span class="fu">cv.glm</span>(Mroz, m.mroz,</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>                        <span class="at">cost =</span> BayesRule),</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>  <span class="at">times =</span> <span class="dv">10</span></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>)</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a><span class="co">#&gt; Unit: milliseconds</span></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a><span class="co">#&gt;       expr       min        lq      mean    median        uq       max neval</span></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a><span class="co">#&gt;  hatvalues    2.0085    2.0507    2.1891    2.2389    2.3146    2.3892    10</span></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a><span class="co">#&gt;   Woodbury   41.8543   42.2084   44.4863   43.0971   47.4801   47.8831    10</span></span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a><span class="co">#&gt;      exact 1735.4481 1751.7635 1786.9661 1756.7391 1844.2620 1882.9254    10</span></span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a><span class="co">#&gt;     cv.glm 1997.6330 2014.3131 2059.2029 2019.0023 2100.2874 2208.7621    10</span></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a><span class="co">#&gt;  cld</span></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a><span class="co">#&gt;  a  </span></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a><span class="co">#&gt;  a  </span></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a><span class="co">#&gt;   b </span></span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a><span class="co">#&gt;    c</span></span></code></pre></div>
<p>There is a substantial time penalty associated with exact
computations.</p>
</div>
<div id="computation-of-the-bias-corrected-cv-criterion-and-confidence-intervals" class="section level2">
<h2>Computation of the bias-corrected CV criterion and confidence
intervals</h2>
<p>Let <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}})\)</span> represent a cross-validation cost
criterion, such as mean-squared error, computed for all of the <span class="math inline">\(n\)</span> values of the response <span class="math inline">\(\mathbf{y}\)</span> based on fitted values <span class="math inline">\(\widehat{\mathbf{y}}\)</span> from the model fit
to all of the data. We require that <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}})\)</span> is the mean of casewise components, that
is, <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}) = \frac{1}{n}\sum_{i=1}^n\mathrm{cv}(y_i,
\widehat{y}_i)\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> For example, <span class="math inline">\(\mathrm{MSE}(\mathbf{y}, \widehat{\mathbf{y}}) =
\frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_i)^2\)</span>.</p>
<p>We divide the <span class="math inline">\(n\)</span> cases into <span class="math inline">\(k\)</span> folds of approximately <span class="math inline">\(n_j \approx n/k\)</span> cases each, where <span class="math inline">\(n = \sum n_j\)</span>. As above, let <span class="math inline">\(\mathbf{j}\)</span> denote the indices of the
cases in the <span class="math inline">\(j\)</span>th fold.</p>
<p>Now define <span class="math inline">\(\mathrm{CV}_j =
\mathrm{CV}(\mathbf{y}, \widehat{\mathbf{y}}^{(j)})\)</span>. The
superscript <span class="math inline">\((j)\)</span> on <span class="math inline">\(\widehat{\mathbf{y}}^{(j)}\)</span> represents
fitted values computed for all of the cases from the model with fold
<span class="math inline">\(j\)</span> omitted. Let <span class="math inline">\(\widehat{\mathbf{y}}^{(-i)}\)</span> represent the
vector of fitted values for all <span class="math inline">\(n\)</span>
cases where the fitted value for the <span class="math inline">\(i\)</span>th case is computed from the model fit
with the fold including the <span class="math inline">\(i\)</span>th
case omitted (i.e., fold <span class="math inline">\(j\)</span> for
which <span class="math inline">\(i \in \mathbf{j}\)</span>).</p>
<p>Then the cross-validation criterion is just <span class="math inline">\(\mathrm{CV} = \mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}^{(-i)})\)</span>. Following <span class="citation">Davison &amp; Hinkley (1997, pp. 293–295)</span>, the
bias-adjusted cross-validation criterion is <span class="math display">\[
\mathrm{CV}_{\mathrm{adj}} = \mathrm{CV} + \mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}) - \frac{1}{n} \sum_{j=1}^{k} n_j \mathrm{CV}_j
\]</span></p>
<p>We compute the standard error of CV as <span class="math display">\[
\mathrm{SE}(\mathrm{CV}) = \frac{1}{\sqrt n} \sqrt{ \frac{\sum_{i=1}^n
\left[ \mathrm{cv}(y_i, \widehat{y}_i^{(-i)} ) - \mathrm{CV} \right]^2
}{n - 1} }
\]</span> that is, as the standard deviation of the casewise components
of CV divided by the square-root of the number of cases.</p>
<p>We then use <span class="math inline">\(\mathrm{SE}(\mathrm{CV})\)</span> to construct a
<span class="math inline">\(100 \times (1 - \alpha)\)</span>% confidence
interval around the <em>adjusted</em> CV estimate of error: <span class="math display">\[
\left[ \mathrm{CV}_{\mathrm{adj}} - z_{1 -
\alpha/2}\mathrm{SE}(\mathrm{CV}), \mathrm{CV}_{\mathrm{adj}} + z_{1 -
\alpha/2}\mathrm{SE}(\mathrm{CV})  \right]
\]</span> where <span class="math inline">\(z_{1 - \alpha/2}\)</span> is
the <span class="math inline">\(1 - \alpha/2\)</span> quantile of the
standard-normal distribution (e.g, <span class="math inline">\(z \approx
1.96\)</span> for a 95% confidence interval, for which <span class="math inline">\(1 - \alpha/2 = .975\)</span>).</p>
<p><span class="citation">Bates, Hastie, &amp; Tibshirani (2023)</span>
show that the coverage of this confidence interval is poor for small
samples, and they suggest a much more computationally intensive
procedure, called <em>nested cross-validation</em>, to compute better
estimates of error and confidence intervals with better coverage for
small samples. We may implement Bates et al.’s approach in a later
release of the <strong>cv</strong> package. At present we use the
confidence interval above for sufficiently large <span class="math inline">\(n\)</span>, which, based on Bates et al.’s
results, we take by default to be <span class="math inline">\(n \ge
400\)</span>.</p>
</div>
<div id="why-the-complement-of-auc-isnt-a-casewise-cv-criterion" class="section level2">
<h2>Why the complement of AUC isn’t a casewise CV criterion</h2>
<p>Consider calculating AUC for folds in which a validation set contains
<span class="math inline">\(n_v\)</span> observations. To calculate AUC
in the validation set, we need the vector of prediction criteria, <span class="math inline">\(\widehat{\mathbf{y}}_{v_{(n_v \times 1)}} =
(\widehat{y}_1, ..., \widehat{y}_{n_v})^T\)</span>, and the vector of
observed responses in the validation set, <span class="math inline">\(\mathbf{y}_{v_{(n_v \times 1)}} = (y_1, \ldots,
y_{n_v})^T\)</span> with <span class="math inline">\(y_i \in \{0,1\}, \;
i = 1, \ldots, n_v\)</span>.</p>
<p>To construct the ROC curve, only the ordering of the values in <span class="math inline">\(\mathbf{\widehat{y}}_v\)</span> is relevant. Thus,
assuming that there are no ties, and reordering observations if
necessary, we can set <span class="math inline">\(\mathbf{\widehat{y}}_v
= (1, 2, \ldots, n_v)^T\)</span>.</p>
<p>If the AUC can be expressed as the casewise mean or sum of a function
<span class="math inline">\(\mathrm{cv}(\widehat{y}_i,y_i)\)</span>,
where <span class="math inline">\(\mathrm{cv}:
\{1,2,...,n_v\}\times\{0,1\} \rightarrow [0,1]\)</span>, then <span class="math display">\[\begin{equation}
\label{eq:cw}
\tag{1}
\sum_{i=1}^{n_v} \mathrm{cv}(\widehat{y}_i,y_i) =
\mathrm{AUC}(\mathbf{\widehat{y}}_v,\mathbf{y}_v)
\end{equation}\]</span> must hold for all <span class="math inline">\(2^{n_v}\)</span> possible values of <span class="math inline">\(\mathbf{y}_v = (y_1,...,y_{n_v})^T\)</span>. If
all <span class="math inline">\(y\mathrm{s}\)</span> have the same
value, either 1 or 0, then the definition of AUC is ambiguous. AUC could
be considered undefined, or it could be set to 0 if all <span class="math inline">\(y\)</span>s are 0 and to 1 if all <span class="math inline">\(y\)</span>s are 1. If AUC is considered to be
undefined in these cases, we have <span class="math inline">\(2^{n_v} -
2\)</span> admissible values for <span class="math inline">\(\mathbf{y}_v\)</span>.</p>
<p>Thus, equation (<span class="math inline">\(\ref{eq:cw}\)</span>)
produces either <span class="math inline">\(2^{n_v}\)</span> or <span class="math inline">\(2^{n_v}-2\)</span> constraints. Although there are
only <span class="math inline">\(2n_v\)</span> possible values for the
<span class="math inline">\(\mathrm{cv(\cdot)}\)</span> function,
equation (<span class="math inline">\(\ref{eq:cw}\)</span>) could,
nevertheless, have consistent solutions. We therefore need to determine
whether there is a value of <span class="math inline">\(n_v\)</span> for
which (<span class="math inline">\(\ref{eq:cw}\)</span>) has no
consistent solution for all admissible values of <span class="math inline">\(\mathbf{y}_v\)</span>. In that eventuality, we
will have shown that AUC cannot, in general, be expressed through a
casewise sum.</p>
<p>If <span class="math inline">\(n_v=3\)</span>, we show below that
(<span class="math inline">\(\ref{eq:cw}\)</span>) has no consistent
solution if we include all possibilities for <span class="math inline">\(\mathbf{y}_v\)</span>, but does if we exclude
cases where all <span class="math inline">\(y\)</span>s have the same
value. If <span class="math inline">\(n_v=4\)</span>, we show that there
are no consistent solutions in either case.</p>
<p>The following R function computes AUC from <span class="math inline">\(\mathbf{\widehat{y}}_v\)</span> and <span class="math inline">\(\mathbf{y}_v\)</span>, accommodating the cases
where <span class="math inline">\(\mathbf{y}_v\)</span> is all 0s or all
1s:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>AUC <span class="ot">&lt;-</span> <span class="cf">function</span>(y, <span class="at">yhat =</span> <span class="fu">seq_along</span>(y)) {</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  s <span class="ot">&lt;-</span> <span class="fu">sum</span>(y)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  <span class="cf">if</span> (s <span class="sc">==</span> <span class="dv">0</span>)</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>  <span class="cf">if</span> (s <span class="sc">==</span> <span class="fu">length</span>(y))</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    <span class="fu">return</span>(<span class="dv">1</span>)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>  Metrics<span class="sc">::</span><span class="fu">auc</span>(y, yhat)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>}</span></code></pre></div>
<p>We then define a function to generate all possible <span class="math inline">\(\mathbf{y}_v\)</span>s of length <span class="math inline">\(n_v\)</span> as rows of the matrix <span class="math inline">\(\mathbf{Y}_{(2^{n_v} \times n_v)}\)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>Ymat <span class="ot">&lt;-</span> <span class="cf">function</span>(n_v, <span class="at">exclude_identical =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>  <span class="fu">stopifnot</span>(n_v <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>              <span class="fu">round</span>(n_v) <span class="sc">==</span> n_v)    <span class="co"># n_v must be a positive integer</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  ret <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">0</span><span class="sc">:</span>(<span class="dv">2</span> <span class="sc">^</span> n_v <span class="sc">-</span> <span class="dv">1</span>),</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>                <span class="cf">function</span>(x)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>                  <span class="fu">as.integer</span>(<span class="fu">intToBits</span>(x)))[<span class="dv">1</span><span class="sc">:</span>n_v,]</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>  ret <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">is.matrix</span>(ret))</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    <span class="fu">t</span>(ret)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    <span class="fu">matrix</span>(ret)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>  <span class="fu">colnames</span>(ret) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;y&quot;</span>, <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(ret))</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>  <span class="cf">if</span> (exclude_identical)</span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>    ret[<span class="sc">-</span><span class="fu">c</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(ret)),]</span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>  <span class="cf">else</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>    ret</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>}</span></code></pre></div>
<p>For <span class="math inline">\(n_v=3\)</span>,</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">Ymat</span>(<span class="dv">3</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co">#&gt;      y1 y2 y3</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co">#&gt; [1,]  0  0  0</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co">#&gt; [2,]  1  0  0</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co">#&gt; [3,]  0  1  0</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co">#&gt; [4,]  1  1  0</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co">#&gt; [5,]  0  0  1</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co">#&gt; [6,]  1  0  1</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="co">#&gt; [7,]  0  1  1</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co">#&gt; [8,]  1  1  1</span></span></code></pre></div>
<p>If we exclude <span class="math inline">\(\mathbf{y}_v\)</span>s with
identical values, then</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">Ymat</span>(<span class="dv">3</span>, <span class="at">exclude_identical =</span> <span class="cn">TRUE</span>)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="co">#&gt;      y1 y2 y3</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="co">#&gt; [1,]  1  0  0</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co">#&gt; [2,]  0  1  0</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co">#&gt; [3,]  1  1  0</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co">#&gt; [4,]  0  0  1</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co">#&gt; [5,]  1  0  1</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="co">#&gt; [6,]  0  1  1</span></span></code></pre></div>
<p>Here is <span class="math inline">\(\mathbf{Y}\)</span> with
corresponding values of AUC:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">cbind</span>(<span class="fu">Ymat</span>(<span class="dv">3</span>), <span class="at">AUC =</span> <span class="fu">apply</span>(<span class="fu">Ymat</span>(<span class="dv">3</span>), <span class="dv">1</span>, AUC))</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="co">#&gt;      y1 y2 y3 AUC</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co">#&gt; [1,]  0  0  0 0.0</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co">#&gt; [2,]  1  0  0 0.0</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="co">#&gt; [3,]  0  1  0 0.5</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="co">#&gt; [4,]  1  1  0 0.0</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="co">#&gt; [5,]  0  0  1 1.0</span></span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co">#&gt; [6,]  1  0  1 0.5</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="co">#&gt; [7,]  0  1  1 1.0</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a><span class="co">#&gt; [8,]  1  1  1 1.0</span></span></code></pre></div>
<p>The values of <span class="math inline">\(\mathrm{cv}(\widehat{y}_i,
y_i)\)</span> that express AUC as a sum of casewise values are solutions
of equation (<span class="math inline">\(\ref{eq:cw}\)</span>), which
can be written as solutions of the following system of <span class="math inline">\(2^{n_v}\)</span> linear simultaneous equations in
<span class="math inline">\(2n_v\)</span> unknowns: <span class="math display">\[\begin{equation}
\label{eq:lin}
\tag{2}
(\mathbf{U} -\mathbf{Y}) \mathbf{c}_0 + \mathbf{Y} \mathbf{c}_1
=
[\mathbf{U} -\mathbf{Y}, \mathbf{Y}]
\begin{bmatrix}
\mathbf{c}_0 \\ \mathbf{c}_1
\end{bmatrix}
= \mathrm{AUC}(\mathbf{\widehat{Y}},\mathbf{Y})
\end{equation}\]</span> where <span class="math inline">\(\mathbf{U}_{(2^{n_v} \times n_v)}\)</span> is a
matrix of 1s conformable with <span class="math inline">\(\mathbf{Y}\)</span>; <span class="math inline">\(\mathbf{c}_0 = [\mathrm{cv}(1,0), c(2,0), ...,
\mathrm{cv}(n_v,0)]^T\)</span>; <span class="math inline">\(\mathbf{c}_1
= [\mathrm{cv}(1,1), c(2,1), ..., \mathrm{cv}(n_v,1)]^T\)</span>; <span class="math inline">\([\mathbf{U} -\mathbf{Y}, \mathbf{Y}]_{(2^{n_v}
\times 2n_v)}\)</span> and <span class="math inline">\(\begin{bmatrix}\begin{aligned}
\mathbf{c}_0 \\ \mathbf{c}_1
\end{aligned}
\end{bmatrix}_{(2n_v \times 1)}\)</span> are partitioned matrices; and
<span class="math inline">\(\mathbf{\widehat{Y}}_{(2^{n_v} \times
n_v)}\)</span> is a matrix each of whose rows consists of the integers 1
to <span class="math inline">\(n_v\)</span>.</p>
<p>We can test whether equation (<span class="math inline">\(\ref{eq:lin}\)</span>) has a solution for any
given <span class="math inline">\(n_v\)</span> by trying to solve it as
a least-squares problem, considering whether the residuals of the
associated linear model are all 0, using the “design matrix” <span class="math inline">\([\mathbf{U} -\mathbf{Y}, \mathbf{Y}]\)</span> to
predict the “outcome” <span class="math inline">\(\mathrm{AUC}(\mathbf{\widehat{Y}},\mathbf{Y})_{(2^{n_v}
\times 1)}\)</span>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>resids <span class="ot">&lt;-</span> <span class="cf">function</span>(n_v,</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>                   <span class="at">exclude_identical =</span> <span class="cn">FALSE</span>,</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>                   <span class="at">tol =</span> <span class="fu">sqrt</span>(.Machine<span class="sc">$</span>double.eps)) {</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">Ymat</span>(n_v, <span class="at">exclude_identical =</span> exclude_identical)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>  AUC <span class="ot">&lt;-</span> <span class="fu">apply</span>(Y, <span class="dv">1</span>, AUC)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span> <span class="sc">-</span> Y, Y)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>  opts <span class="ot">&lt;-</span> <span class="fu">options</span>(<span class="at">warn =</span> <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>  <span class="fu">on.exit</span>(<span class="fu">options</span>(opts))</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">lsfit</span>(X, AUC, <span class="at">intercept =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>  ret <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">abs</span>(<span class="fu">residuals</span>(fit)))</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>  <span class="cf">if</span> (ret <span class="sc">&lt;</span> tol) {</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    ret <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>    solution <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>    <span class="fu">names</span>(solution) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(<span class="st">&quot;c(&quot;</span>, <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span>n_v, <span class="dv">1</span><span class="sc">:</span>n_v), <span class="st">&quot;,&quot;</span>,</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>                              <span class="fu">rep</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>, <span class="at">each =</span> n_v), <span class="st">&quot;)&quot;</span>)</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>    <span class="fu">attr</span>(ret, <span class="st">&quot;solution&quot;</span>) <span class="ot">&lt;-</span> <span class="fu">zapsmall</span>(solution)</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>  }</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>  ret</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>}</span></code></pre></div>
<p>The case <span class="math inline">\(n_v=3\)</span>, excluding
identical <span class="math inline">\(y\)</span>s, has a solution:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">resids</span>(<span class="dv">3</span>, <span class="at">exclude_identical =</span> <span class="cn">TRUE</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="co">#&gt; [1] 0</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co">#&gt; attr(,&quot;solution&quot;)</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co">#&gt; c(1,0) c(2,0) c(3,0) c(1,1) c(2,1) c(3,1) </span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co">#&gt;    1.0    0.0   -0.5    0.5    0.0    0.0</span></span></code></pre></div>
<p>But, if identical <span class="math inline">\(y\)</span>s are
included, the equation is not consistent:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">resids</span>(<span class="dv">3</span>, <span class="at">exclude_identical =</span> <span class="cn">FALSE</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.125</span></span></code></pre></div>
<p>For <span class="math inline">\(n_v=4\)</span>, there are no
solutions in either case:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">resids</span>(<span class="dv">4</span>, <span class="at">exclude_identical =</span> <span class="cn">TRUE</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co">#&gt; [1] 0.083333</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="fu">resids</span>(<span class="dv">4</span>, <span class="at">exclude_identical =</span> <span class="cn">FALSE</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="co">#&gt; [1] 0.25</span></span></code></pre></div>
<p>Consequently, the widely employed AUC measure of fit for binary
regression cannot in general be used for a casewise cross-validation
criterion.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="unnumbered">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-ArlotCelisse:2010" class="csl-entry">
Arlot, S., &amp; Celisse, A. (2010). <span class="nocase">A survey of
cross-validation procedures for model selection</span>. <em>Statistics
Surveys</em>, <em>4</em>, 40–79. Retrieved from <a href="https://doi.org/10.1214/09-SS054">https://doi.org/10.1214/09-SS054</a>
</div>
<div id="ref-BatesHastieTibshirani:2023" class="csl-entry">
Bates, S., Hastie, T., &amp; Tibshirani, R. (2023). Cross-validation:
What does it estimate and how well does it do it? <em>Journal of the
American Statistical Association</em>, <em>in press</em>. Retrieved from
<a href="https://doi.org/10.1080/01621459.2023.2197686">https://doi.org/10.1080/01621459.2023.2197686</a>
</div>
<div id="ref-CantyRipley2022" class="csl-entry">
Canty, A., &amp; Ripley, B. D. (2022). <em>Boot: Bootstrap
<span>R</span> (<span>S</span>-plus) functions</em>.
</div>
<div id="ref-DavisonHinkley:1997" class="csl-entry">
Davison, A. C., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and
their applications</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Fox:2016" class="csl-entry">
Fox, J. (2016). <em>Applied regression analysis and generalized linear
models</em> (Second edition). Thousand Oaks <span>CA</span>: Sage.
</div>
<div id="ref-FoxWeisberg:2019" class="csl-entry">
Fox, J., &amp; Weisberg, S. (2019). <em>An <span>R</span> companion to
applied regression</em> (Third edition). Thousand Oaks <span>CA</span>:
Sage.
</div>
<div id="ref-Hager:1989" class="csl-entry">
Hager, W. W. (1989). Updating the inverse of a matrix.
<em><span>SIAM</span> Review</em>, <em>31</em>(2), 221–239.
</div>
<div id="ref-Mersmann:2023" class="csl-entry">
Mersmann, O. (2023). <em>Microbenchmark: Accurate timing functions</em>.
Retrieved from <a href="https://CRAN.R-project.org/package=microbenchmark">https://CRAN.R-project.org/package=microbenchmark</a>
</div>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>This is a definitional formula, which assumes that the
model matrix <span class="math inline">\(\mathbf{X}\)</span> is of full
column rank, and which can be subject to numerical instability when
<span class="math inline">\(\mathbf{X}\)</span> is ill-conditioned.
<code>lm()</code> uses the singular-value decomposition of the model
matrix to obtain computationally more stable results.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Out of impatience, we asked
<code>microbenchmark()</code> to execute each command only 10 times
rather than the default 100. With the exception of the last columns, the
output is self-explanatory. The last column shows which methods have
average timings that are statistically distinguishable. Because of the
small number of repetitions (i.e., 10), the <code>&quot;hatvalues&quot;</code> and
<code>&quot;Woodbury&quot;</code> methods aren’t distinguishable, but the
difference between these methods persists when we perform more
repetitions—we invite the reader to redo this computation with the
default <code>times=100</code> repetitions.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p><span class="citation">Arlot &amp; Celisse (2010)</span>
term the casewise loss, <span class="math inline">\(\mathrm{cv}(y_i,
\widehat{y}_i)\)</span>, the “contrast function.”<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
