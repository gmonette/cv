[{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"adding-a-cost-criterion","dir":"Articles","previous_headings":"","what":"Adding a cost criterion","title":"Extending the cv package","text":"cost criterion suitable use cv() cvSelect() take two arguments, y (observed response vector) yhat (vector fitted predicted response values), return numeric index lack fit. cv package supplies several criteria: mse(y, yhat), returns mean-squared prediction error numeric response; rmse(y, yhat), returns (square-)root mean-squared error; medAbsErr(y, yhat), returns median absolute error; BayesRule(y, yhat) (non-error-checking version, BayesRule2(y, yhat)), suitable use binary regression model, y binary response coded 0 “failure” 1 “success”; yhat predicted probability success; proportion incorrectly classified cases returned. illustrate using different prediction cost criterion, ’ll base cost criterion area receiver operating characteristic (“ROC”) curve logistic regression. ROC curve graphical representation classification power binary regression model, area ROC curve (“AUC”), varies 0 1, common summary measure based ROC (see \"Receiver operating characteristic\", 2023). Metrics package (Hamner & Frasco, 2018) includes variety measures useful model selection, including auc() function. convert AUC cost measure taking complement: apply AUCcomp() Mroz logistic regression, discussed introductory vignette cross-validating regression models, reproduce . Using Mroz data frame carData package (Fox & Weisberg, 2019): Cross-validating cost measure straightforward: expected, cross-validated complement AUC somewhat less optimistic criterion computed model fit whole data set. explain vignette “Cross-validating regression models,” cv() function differentiates CV criteria averages casewise components criteria . Computation bias corrections confidence intervals limited former. show technical computational vignette AUC, hence complement, expressed averages casewise components. cv() looks \"casewise loss\" attribute value returned CV criterion function. attribute exists, criterion treated mean casewise components, cv() uses unexported function getLossFn() construct function returns casewise components criterion. illustrate mse(): scheme work, “casewise loss” attribute must character string (vector character strings), \"(y - yhat)^2\", evaluates expression function y yhat, computes vector casewise components CV criterion.","code":"AUCcomp <- function(y, yhat) 1 - Metrics::auc(y, yhat) data(\"Mroz\", package=\"carData\") m.mroz <- glm(lfp ~ ., data=Mroz, family=binomial) summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4 AUCcomp(with(Mroz, as.numeric(lfp == \"yes\")), fitted(m.mroz)) #> [1] 0.26362 library(\"cv\") #> Loading required package: doParallel #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel cv(m.mroz, criterion=AUCcomp, seed=3639) #> R RNG seed set to 3639 #> 10-Fold Cross Validation #> method: exact #> criterion: AUCcomp #> cross-validation criterion = 0.27471 #> full-sample criterion = 0.26362 mse #> function (y, yhat)  #> { #>     result <- mean((y - yhat)^2) #>     attr(result, \"casewise loss\") <- \"(y - yhat)^2\" #>     result #> } #> <bytecode: 0x12aa49270> #> <environment: namespace:cv> cv:::getLossFn(mse(rnorm(100), rnorm(100))) #> function (y, yhat)  #> { #>     (y - yhat)^2 #> } #> <environment: 0x12aabbc80>"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"independently-sampled-cases","dir":"Articles","previous_headings":"Adding a model class not covered by the default cv() method","what":"Independently sampled cases","title":"Extending the cv package","text":"Suppose want cross-validate multinomial logistic regression model fit multinom() function nnet package (Venables & Ripley, 2002). borrow example Fox (2016, sec. 14.2.1), data British Election Panel Study vote choice 2001 British election. Data example BEPS data frame carData package: polytomous (multi-category) response variable vote, factor levels \"Conservative\", \"Labour\", \"Liberal Democrat\". predictors vote : age, years; econ.cond.national econ.cond.household, respondent’s ratings state economy, 1 5 scales. Blair, Hague, Kennedy, ratings leaders Labour, Conservative, Liberal Democratic parties, 1 5 scales. Europe, 11-point scale attitude towards European integration, high scores representing “Euro-skepticism.” political.knowledge, knowledge parties’ positions European integration, scores 0 3. gender, \"female\" \"male\". model fit data includes interaction Europe political.knowledge; predictors enter model additively: predictors, including Europe \\(\\times\\) political.knowledge interaction, associated small \\(p\\)-values; Anova() function car package (Fox & Weisberg, 2019). ’s “effect plot”, using effects package (Fox & Weisberg, 2019) visualize Europe \\(\\times\\) political.knowledge interaction “stacked-area” graph:  cross-validate multinomial-logit model need appropriate cost criterion. None criteria supplied cv package—example, neither mse(), appropriate numeric response, BayesRule(), appropriate binary response—. One possibility adapt Bayes rule polytomous response: predict() method \"multinom\" models called argument type=\"class\" reports Bayes-rule prediction case—, response category highest predicted probability. BayesRuleMulti() function calculates proportion misclassified cases. value mean casewise components, attach \"casewise loss\" attribute result (explained preceding section). marginal proportions response categories marginal Bayes-rule prediction, everyone vote Labour, produces error rate \\(1 - 0.47213 = 0.52787\\). multinomial-logit model appears substantially better , performance hold cross-validation? check first whether default cv() method works “---box” \"multinom\" model: default method GetResponse() (function supplied cv package—see ?GetResponse) fails \"multinom\" object. straightforward solution supply GetResponse.multinom() method returns factor response (using get_response() function insight package, Lüdecke, Waggoner, & Makowski, 2019), try : traceback() (shown) reveals problem default method cv() calls \"multinom\" method predict() argument type=\"response\", correct argument type=\"class\". therefore must write “multinom” method cv(), proves simple: , simply call default cv() method type argument properly set. addition supplying correct type argument, method sets default criterion cv.multinom() method BayesRuleMulti. Adding argument criterion.name=deparse(substitute(criterion)) inessential, insures printed output include name criterion function ’s employed, whether ’s default BayesRuleMulti something else. Prior invoking NextMethod(), called update() trace=FALSE suppress iteration history reported default multinom()—tedious see iteration history fold. : cross-validated polytomous Bayes-rule criterion confirms fitted model substantially better marginal Bayes-rule prediction everyone votes Labour.","code":"data(\"BEPS\", package=\"carData\") head(BEPS) #>               vote age economic.cond.national economic.cond.household Blair #> 1 Liberal Democrat  43                      3                       3     4 #> 2           Labour  36                      4                       4     4 #> 3           Labour  35                      4                       4     5 #> 4           Labour  24                      4                       2     2 #> 5           Labour  41                      2                       2     1 #> 6           Labour  47                      3                       4     4 #>   Hague Kennedy Europe political.knowledge gender #> 1     1       4      2                   2 female #> 2     4       4      5                   2   male #> 3     2       3      3                   2   male #> 4     1       3      4                   0 female #> 5     1       4      6                   2   male #> 6     4       2      4                   2   male library(\"nnet\") m.beps <- multinom(   vote ~ age + gender + economic.cond.national +     economic.cond.household + Blair + Hague + Kennedy +     Europe * political.knowledge,   data = BEPS ) #> # weights:  36 (22 variable) #> initial  value 1675.383740  #> iter  10 value 1240.047788 #> iter  20 value 1163.199642 #> iter  30 value 1116.519687 #> final  value 1116.519666  #> converged car::Anova(m.beps) #> Analysis of Deviance Table (Type II tests) #>  #> Response: vote #>                            LR Chisq Df Pr(>Chisq)     #> age                            13.9  2    0.00097 *** #> gender                          0.5  2    0.79726     #> economic.cond.national         30.6  2    2.3e-07 *** #> economic.cond.household         5.7  2    0.05926 .   #> Blair                         135.4  2    < 2e-16 *** #> Hague                         166.8  2    < 2e-16 *** #> Kennedy                        68.9  2    1.1e-15 *** #> Europe                         78.0  2    < 2e-16 *** #> political.knowledge            55.6  2    8.6e-13 *** #> Europe:political.knowledge     50.8  2    9.3e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 plot(   effects::Effect(     c(\"Europe\", \"political.knowledge\"),     m.beps,     xlevels = list(Europe = 1:11, political.knowledge = 0:3),     fixed.predictors = list(given.values = c(gendermale = 0.5))   ),   lines = list(col = c(\"blue\", \"red\", \"orange\")),   axes = list(x = list(rug = FALSE), y = list(style = \"stacked\")) ) head(BEPS$vote) #> [1] Liberal Democrat Labour           Labour           Labour           #> [5] Labour           Labour           #> Levels: Conservative Labour Liberal Democrat yhat <- predict(m.beps, type = \"class\") head(yhat) #> [1] Labour           Labour           Labour           Labour           #> [5] Liberal Democrat Labour           #> Levels: Conservative Labour Liberal Democrat BayesRuleMulti <- function(y, yhat) {   result <- mean(y != yhat)   attr(result, \"casewise loss\") <- \"y != yhat\"   result }  BayesRuleMulti(BEPS$vote, yhat) #> [1] 0.31869 #> attr(,\"casewise loss\") #> [1] \"y != yhat\" xtabs(~ vote, data=BEPS)/nrow(BEPS) #> vote #>     Conservative           Labour Liberal Democrat  #>          0.30295          0.47213          0.22492 cv(m.beps, seed=3465, criterion=BayesRuleMulti) #> Error in GetResponse.default(model): non-vector response GetResponse.multinom <- function(model, ...) {   insight::get_response(model) }  head(GetResponse(m.beps)) #> [1] Liberal Democrat Labour           Labour           Labour           #> [5] Labour           Labour           #> Levels: Conservative Labour Liberal Democrat cv(m.beps, seed=3465, criterion=BayesRuleMulti) #> R RNG seed set to 3465 #> # weights:  36 (22 variable) #> initial  value 1507.296060  #> iter  10 value 1134.575036 #> iter  20 value 1037.413231 #> iter  30 value 1007.705242 #> iter  30 value 1007.705235 #> iter  30 value 1007.705235 #> final  value 1007.705235  #> converged #> Error in match.arg(type): 'arg' should be one of \"class\", \"probs\" cv.multinom <-   function (model, data, criterion = BayesRuleMulti, k, reps,             seed, ...) {     model <- update(model, trace = FALSE)     NextMethod(       type = \"class\",       criterion = criterion,       criterion.name = deparse(substitute(criterion))     )   } cv(m.beps, seed=3465) #> R RNG seed set to 3465 #> 10-Fold Cross Validation #> criterion: BayesRuleMulti #> cross-validation criterion = 0.32459 #> bias-adjusted cross-validation criterion = 0.32368 #> 95% CI for bias-adjusted CV criterion = (0.30017, 0.34718) #> full-sample criterion = 0.31869"},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"calling-cvcompute","dir":"Articles","previous_headings":"Adding a model class not covered by the default cv() method > Independently sampled cases","what":"Calling cvCompute()","title":"Extending the cv package","text":"cv() methods independently sampled cases, cv.default(), cv.lm(), cv.glm(), work setting calls cvCompute() function, exported cv package support development cv() methods additional classes regression models. cases, however, preceding cv.multinom() example, suffice much simpler set suitable call cv.default() via NextMethod(). illustrate use cvCompute() directly, write alternative, necessarily complicated, version cv.multinom(). Notice separate “helper” functions defined non-parallel parallel computations.1 new version cv.multinom() produces results version calls cv.default():2","code":"cv.multinom <- function(model,                         data = insight::get_data(model),                         criterion = BayesRuleMulti,                         k = 10,                         reps = 1,                         seed = NULL,                         details = k <= 10,                         confint = n >= 400,                         level = 0.95,                         ncores = 1,                         start = FALSE,                         ...) {   f <- function(i) {     # helper function to compute to compute fitted values,     #  etc., for each fold i          indices.i <- fold(folds, i)     model.i <- if (start) {       update(model,              data = data[-indices.i,],              start = b,              trace = FALSE)     } else {       update(model, data = data[-indices.i,], trace = FALSE)     }     fit.all.i <- predict(model.i, newdata = data, type = \"class\")     fit.i <- fit.all.i[indices.i]     # returns:     #  fit.i: fitted values for the i-th fold     #  crit.all.i: CV criterion for all cases based on model with     #              i-th fold omitted     #  coef.i: coefficients for the model with i-th fold omitted     list(       fit.i = fit.i,       crit.all.i = criterion(y, fit.all.i),       coef.i = coef(model.i)     )   }      fPara <- function(i, multinom, ...) {     # helper function for parallel computation     #   argument multinom makes multinom() locally available     #   ... is necessary but not used     indices.i <- fold(folds, i)     model.i <- if (start) {       update(model,              data = data[-indices.i,],              start = b,              trace = FALSE)     } else {       update(model, data = data[-indices.i,], trace = FALSE)     }     fit.all.i <- predict(model.i, newdata = data, type = \"class\")     fit.i <- fit.all.i[indices.i]     list(       fit.i = fit.i,       crit.all.i = criterion(y, fit.all.i),       coef.i = coef(model.i)     )   }      n <- nrow(data)      # see ?cvCompute for definitions of arguments   cvCompute(     model = model,     data = data,     criterion = criterion,     criterion.name = deparse(substitute(criterion)),     k = k,     reps = reps,     seed = seed,     details = details,     confint = confint,     level = level,     ncores = ncores,     type = \"class\",     start = start,     f = f,     fPara = fPara,     multinom = nnet::multinom   ) } cv(m.beps, seed=3465) #> R RNG seed set to 3465 #> 10-Fold Cross Validation #> criterion: BayesRuleMulti #> cross-validation criterion = 0.32459 #> bias-adjusted cross-validation criterion = 0.32368 #> 95% CI for bias-adjusted CV criterion = (0.30017, 0.34718) #> full-sample criterion = 0.31869"},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"mixed-effects-models","dir":"Articles","previous_headings":"Adding a model class not covered by the default cv() method","what":"Mixed-effects models","title":"Extending the cv package","text":"Adding cv() method mixed-model class somewhat complicated. provide cvMixed() function facilitate process, see works, consider \"lme\" method cv package: Notice cv.lme() sets call cvMixed(), computational work. arguments cvMixed() familiar: model mixed-model object, class \"lme\". package name package mixed-modeling function used fit model, lme(), resides—.e., \"nlme\"; cvMixed() uses argument retrieve package namespace. data data set model fit, default extracted get_data() function insight package. criterion CV criterion, defaulting mse() function. k number CV folds, defaulting \"loo\" CV clusters 10 CV cases. reps number times CV process repeated, defaulting 1. seed seed R’s random-number generator, defaulting randomly selected (saved) value. ncores number cores use parallel computation; 1, default, computation isn’t parallelized. clusterVariables character vector names variables defining clusters; missing, CV based cases rather clusters. remaining two arguments unfamiliar: predict.clusters.args named list arguments passed predict() function obtain predictions full data set model fit subset data cluster-based CV. first two arguments object newdata. typically necessary tell cvMixed() base predictions fixed effects; case \"lme\" models, done setting level = 0. Similarly, predict.cases.args named list arguments passed predict() case-based CV. Setting level = 1 includes random effects predictions. blups fixed.effects used compute detailed fold-based statistics case-based cluster-based CV, respectively. Finally, additional arguments, absorbed ..., passed update() model refit fold omitted. cvMixed() returns object class \"cv\". Now imagine want support new class mixed-effects models. concrete, illustrate glmmPQL() function MASS package (Venables & Ripley, 2002), fits generalized-linear mixed-effects models penalized quasi-likelihood.3 coincidentally, arguments glmmPQL() similar lme() (additional family argument), former iteratively invokes latter; cv.glmmPQL() resemble cv.lme(). turns , neither default method GetResponse() insight::get_data() work \"glmmPQL\" objects. objects include \"data\" element, however, can simply extract element default data argument cv.glmmPQL() method. get response variable complicated: refit fixed part model GLM regression constant right-hand side, extract response ; need response variable, limit number GLM iterations 1 suppress warning messages non-convergence: Writing cv() method straightforward: set argument verbose=FALSE suppress glmmPQL()’s iteration counter cvMixed() calls update(). Let’s apply newly minted method logistic regression random intercept example appears ?glmmPQL: compare result obtained glmer() lme4 package: two sets estimates similar, identical Finally, try cv.glmmPQL() method, cross-validating clusters cases, compare glmer():","code":"cv:::cv.lme #> function (model, data = insight::get_data(model), criterion = mse,  #>     k = NULL, reps = 1L, seed, details = NULL, ncores = 1L, clusterVariables,  #>     blups = coef, fixed.effects = nlme::fixef, ...)  #> { #>     cvMixed(model, package = \"nlme\", data = data, criterion = criterion,  #>         criterion.name = deparse(substitute(criterion)), k = k,  #>         reps = reps, seed = seed, details = details, ncores = ncores,  #>         clusterVariables = clusterVariables, predict.clusters.args = list(object = model,  #>             newdata = data, level = 0), predict.cases.args = list(object = model,  #>             newdata = data, level = 1), blups = blups, fixed.effects = fixed.effects,  #>         ...) #> } #> <bytecode: 0x12c3b9318> #> <environment: namespace:cv> GetResponse.glmmPQL <- function(model, ...) {   f <- formula(model)   f[[3]] <- 1 # regression constant only on RHS   model <-     suppressWarnings(glm(       f,       data = model$data,       family = model$family,       control = list(maxit = 1)     ))   cv::GetResponse(model) } cv.glmmPQL <- function(model,                        data = model$data,                        criterion = mse,                        k,                        reps = 1,                        seed,                        ncores = 1,                        clusterVariables,                        blups = coef,                        fixed.effects = nlme::fixef,                        ...) {   cvMixed(     model,     package = \"MASS\",     data = data,     criterion = criterion,     k = k,     reps = reps,     seed = seed,     ncores = ncores,     clusterVariables = clusterVariables,     predict.clusters.args = list(       object = model,       newdata = data,       level = 0,       type = \"response\"     ),     predict.cases.args = list(       object = model,       newdata = data,       level = 1,       type = \"response\"     ),     blups = blups,     fixed.effects = fixed.effects,     verbose = FALSE,     ...   ) } library(\"MASS\") m.pql <- glmmPQL(   y ~ trt + I(week > 2),   random = ~ 1 | ID,   family = binomial,   data = bacteria ) #> iteration 1 #> iteration 2 #> iteration 3 #> iteration 4 #> iteration 5 #> iteration 6 summary(m.pql) #> Linear mixed-effects model fit by maximum likelihood #>   Data: bacteria  #>   AIC BIC logLik #>    NA  NA     NA #>  #> Random effects: #>  Formula: ~1 | ID #>         (Intercept) Residual #> StdDev:      1.4106  0.78005 #>  #> Variance function: #>  Structure: fixed weights #>  Formula: ~invwt  #> Fixed effects:  y ~ trt + I(week > 2)  #>                   Value Std.Error  DF t-value p-value #> (Intercept)      3.4120   0.51850 169  6.5805  0.0000 #> trtdrug         -1.2474   0.64406  47 -1.9367  0.0588 #> trtdrug+        -0.7543   0.64540  47 -1.1688  0.2484 #> I(week > 2)TRUE -1.6073   0.35834 169 -4.4853  0.0000 #>  Correlation:  #>                 (Intr) trtdrg trtdr+ #> trtdrug         -0.598               #> trtdrug+        -0.571  0.460        #> I(week > 2)TRUE -0.537  0.047 -0.001 #>  #> Standardized Within-Group Residuals: #>      Min       Q1      Med       Q3      Max  #> -5.19854  0.15723  0.35131  0.49495  1.74488  #>  #> Number of Observations: 220 #> Number of Groups: 50 library(\"lme4\") #> Loading required package: Matrix m.glmer <- glmer(y ~ trt + I(week > 2) + (1 | ID),                  family = binomial, data = bacteria) summary(m.glmer) #> Generalized linear mixed model fit by maximum likelihood (Laplace #>   Approximation) [glmerMod] #>  Family: binomial  ( logit ) #> Formula: y ~ trt + I(week > 2) + (1 | ID) #>    Data: bacteria #>  #>      AIC      BIC   logLik deviance df.resid  #>    202.3    219.2    -96.1    192.3      215  #>  #> Scaled residuals:  #>    Min     1Q Median     3Q    Max  #> -4.561  0.136  0.302  0.422  1.128  #>  #> Random effects: #>  Groups Name        Variance Std.Dev. #>  ID     (Intercept) 1.54     1.24     #> Number of obs: 220, groups:  ID, 50 #>  #> Fixed effects: #>                 Estimate Std. Error z value Pr(>|z|)     #> (Intercept)        3.548      0.696    5.10  3.4e-07 *** #> trtdrug           -1.367      0.677   -2.02  0.04352 *   #> trtdrug+          -0.783      0.683   -1.15  0.25193     #> I(week > 2)TRUE   -1.598      0.476   -3.36  0.00078 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Correlation of Fixed Effects: #>             (Intr) trtdrg trtdr+ #> trtdrug     -0.593               #> trtdrug+    -0.537  0.487        #> I(wk>2)TRUE -0.656  0.126  0.064 # comparison of fixed effects: car::compareCoefs(m.pql, m.glmer)  #> Warning in car::compareCoefs(m.pql, m.glmer): models to be compared are of #> different classes #> Calls: #> 1: glmmPQL(fixed = y ~ trt + I(week > 2), random = ~1 | ID, family =  #>   binomial, data = bacteria) #> 2: glmer(formula = y ~ trt + I(week > 2) + (1 | ID), data = bacteria,  #>   family = binomial) #>  #>                 Model 1 Model 2 #> (Intercept)       3.412   3.548 #> SE                0.514   0.696 #>                                 #> trtdrug          -1.247  -1.367 #> SE                0.638   0.677 #>                                 #> trtdrug+         -0.754  -0.783 #> SE                0.640   0.683 #>                                 #> I(week > 2)TRUE  -1.607  -1.598 #> SE                0.355   0.476 #> cv(m.pql, clusterVariables=\"ID\", criterion=BayesRule) #> n-Fold Cross Validation based on 50 {ID} clusters #> cross-validation criterion = 0.19545 #> bias-adjusted cross-validation criterion = 0.19545 #> full-sample criterion = 0.19545 cv(m.pql, data=bacteria, criterion=BayesRule, seed=1490) #> R RNG seed set to 1490 #> 10-Fold Cross Validation #> cross-validation criterion = 0.20909 #> bias-adjusted cross-validation criterion = 0.20727 #> full-sample criterion = 0.14545 cv(m.glmer, clusterVariables=\"ID\", criterion=BayesRule) #> n-Fold Cross Validation based on 50 {ID} clusters #> criterion: BayesRule #> cross-validation criterion = 0.19545 #> bias-adjusted cross-validation criterion = 0.19545 #> full-sample criterion = 0.19545 cv(m.glmer, data=bacteria, criterion=BayesRule, seed=1490) #> R RNG seed set to 1490 #> 10-Fold Cross Validation #> criterion: BayesRule #> cross-validation criterion = 0.19545 #> bias-adjusted cross-validation criterion = 0.19364 #> full-sample criterion = 0.15"},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"adding-a-model-selection-procedure","dir":"Articles","previous_headings":"","what":"Adding a model-selection procedure","title":"Extending the cv package","text":"selectStepAIC() function supplied cv package, based stepAIC() function nnet package (Venables & Ripley, 2002) stepwise model selection, suitable procedure argument cvSelect(). use selectStepAIC() illustrated vignette cross-validating model selection. ’ll employ selectStepAIC() “template” writing CV model-selection procedure. see code function, type cv::selectStepAIC R command prompt, examine sources cv package https://github.com/gmonette/cv (code selectStepAIC() https://github.com/gmonette/cv/blob/main/R/cv-select.R). Another approach model selection -subsets regression. regsubsets() function leaps package (Lumley & Miller, 2020) implements efficient algorithm selecting best-fitting linear least-squares regressions subsets predictors sizes, 1 maximum number candidate predictors.4 illustrate use regsubsets(), employ swiss data frame supplied leaps package: data set includes following variables, 47 French-speaking Swiss provinces circa 1888: Fertility: standardized fertility measure. Agriculture: percentage male population engaged agriculture. Examination: percentage draftees Swiss army receiving highest grade examination. Education: percentage draftees primary-school education. Catholic: percentage population Catholic. Infant.Mortality: infant-mortality rate, expressed percentage live births surviving less year. Following Lumley & Miller (2020), treat Fertility response variables predictors linear least-squares regression: Thus, MSE model fit complete data considerably smaller CV estimate MSE. Can better selecting subset predictors, taking account additional uncertainty induced model selection? First, let’s apply best-subset selection complete data set: Selecting best model size. graph, produced subsets() function car package, shows model smallest BIC “best” model 4 predictors, including Agriculture, Education, Catholic, Infant.Mortality, Examination: MSE selected model (course) slightly higher full model fit previously, cross-validated MSE bit lower; explain vignette cross-validating model selection, however, isn’t kosher select cross-validate model data. ’s function named selectSubsets(), meant used cvSelect(), suitable cross-validating model-selection process: slightly tricky point scoping issues, predict() doesn’t work model fit omitting \\(\\)th fold, fitted values cases computed directly \\(\\widehat{\\mathbf{y}}_{-} = \\mathbf{X} \\mathbf{b}_{-}\\), \\(\\mathbf{X}\\) model-matrix cases, \\(\\mathbf{b}_{-}\\) vector least-squares coefficients selected model \\(\\)th fold omitted. Additionally, command lm(y[-indices] ~ X[-indices, x.names.] - 1), selected model \\(\\)th fold deleted, produces awkward coefficient names like \"X[-indices, x.names.]Infant.Mortality\". Purely aesthetic reasons, command sub(\"X\\\\[-indices, x.names.\\\\]\", \"\", names(coefs)) fixes awkward names, removing extraneous text, \"X[-indices, x.names.]\". Applying selectSubsets() full data produces full-data cross-validated MSE (obtained previously): Similarly, applying function imaginary “fold” 5 cases returns MSE cases fold, based model selected fit cases omitting fold; MSE cases, based model; coefficients selected model, includes 4 5 predictors (intercept): , using selectSubsets() cross-validation, invoking cv.function() method cv(), get: Cross-validation shows model selection exacts penalty MSE. Examining models selected 10 folds reveals uncertainty identifying predictors “best” model, Agriculture sometimes appearing sometimes : well, fold-wise MSE varies considerably, reflecting small size swiss data set (47 cases).","code":"library(\"leaps\") head(swiss) #>              Fertility Agriculture Examination Education Catholic #> Courtelary        80.2        17.0          15        12     9.96 #> Delemont          83.1        45.1           6         9    84.84 #> Franches-Mnt      92.5        39.7           5         5    93.40 #> Moutier           85.8        36.5          12         7    33.77 #> Neuveville        76.9        43.5          17        15     5.16 #> Porrentruy        76.1        35.3           9         7    90.57 #>              Infant.Mortality #> Courtelary               22.2 #> Delemont                 22.2 #> Franches-Mnt             20.2 #> Moutier                  20.3 #> Neuveville               20.6 #> Porrentruy               26.6 nrow(swiss) #> [1] 47 m.swiss <- lm(Fertility ~ ., data=swiss) summary(m.swiss) #>  #> Call: #> lm(formula = Fertility ~ ., data = swiss) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -15.274  -5.262   0.503   4.120  15.321  #>  #> Coefficients: #>                  Estimate Std. Error t value Pr(>|t|)     #> (Intercept)       66.9152    10.7060    6.25  1.9e-07 *** #> Agriculture       -0.1721     0.0703   -2.45   0.0187 *   #> Examination       -0.2580     0.2539   -1.02   0.3155     #> Education         -0.8709     0.1830   -4.76  2.4e-05 *** #> Catholic           0.1041     0.0353    2.95   0.0052 **  #> Infant.Mortality   1.0770     0.3817    2.82   0.0073 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 7.17 on 41 degrees of freedom #> Multiple R-squared:  0.707,  Adjusted R-squared:  0.671  #> F-statistic: 19.8 on 5 and 41 DF,  p-value: 5.59e-10 cv(m.swiss, seed=8433) #> R RNG seed set to 8433 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 59.683 #> bias-adjusted cross-validation criterion = 58.846 #> full-sample criterion = 44.788 swiss.sub <- regsubsets(Fertility ~ ., data=swiss) summary(swiss.sub) #> Subset selection object #> Call: regsubsets.formula(Fertility ~ ., data = swiss) #> 5 Variables  (and intercept) #>                  Forced in Forced out #> Agriculture          FALSE      FALSE #> Examination          FALSE      FALSE #> Education            FALSE      FALSE #> Catholic             FALSE      FALSE #> Infant.Mortality     FALSE      FALSE #> 1 subsets of each size up to 5 #> Selection Algorithm: exhaustive #>          Agriculture Examination Education Catholic Infant.Mortality #> 1  ( 1 ) \" \"         \" \"         \"*\"       \" \"      \" \"              #> 2  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \" \"              #> 3  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \"*\"              #> 4  ( 1 ) \"*\"         \" \"         \"*\"       \"*\"      \"*\"              #> 5  ( 1 ) \"*\"         \"*\"         \"*\"       \"*\"      \"*\" (bics <- summary(swiss.sub)$bic) #> [1] -19.603 -28.611 -35.656 -37.234 -34.553 which.min(bics) #> [1] 4 car::subsets(swiss.sub, legend=\"topright\") m.best <- update(m.swiss, . ~ . - Examination) summary(m.best) #>  #> Call: #> lm(formula = Fertility ~ Agriculture + Education + Catholic +  #>     Infant.Mortality, data = swiss) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.676  -6.052   0.751   3.166  16.142  #>  #> Coefficients: #>                  Estimate Std. Error t value Pr(>|t|)     #> (Intercept)       62.1013     9.6049    6.47  8.5e-08 *** #> Agriculture       -0.1546     0.0682   -2.27   0.0286 *   #> Education         -0.9803     0.1481   -6.62  5.1e-08 *** #> Catholic           0.1247     0.0289    4.31  9.5e-05 *** #> Infant.Mortality   1.0784     0.3819    2.82   0.0072 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 7.17 on 42 degrees of freedom #> Multiple R-squared:  0.699,  Adjusted R-squared:  0.671  #> F-statistic: 24.4 on 4 and 42 DF,  p-value: 1.72e-10 cv(m.best, seed=8433) # use same folds as before #> R RNG seed set to 8433 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 58.467 #> bias-adjusted cross-validation criterion = 57.778 #> full-sample criterion = 45.916 selectSubsets <- function(data = insight::get_data(model),                           model,                           indices,                           criterion = mse,                           details = TRUE,                           seed,                           save.model = FALSE,                           ...) {   if (inherits(model, \"lm\", which = TRUE) != 1)     stop(\"selectSubsets is appropriate only for 'lm' models\")      y <- GetResponse(model)   formula <- formula(model)   X <- model.matrix(model)      if (missing(indices)) {     if (missing(seed) || is.null(seed))       seed <- sample(1e6, 1L)     # select the best model from the full data by BIC     sel <- leaps::regsubsets(formula, data = data, ...)     bics <- summary(sel)$bic     best <- coef(sel, 1:length(bics))[[which.min(bics)]]     x.names <- names(best)     # fit the best model; intercept is already in X, hence - 1:     m.best <- lm(y ~ X[, x.names] - 1)     fit.all <- predict(m.best, newdata = data)     return(list(       criterion = criterion(y, fit.all),       model = if (save.model)         m.best # return best model       else         NULL     ))   }      # select the best model omitting the i-th fold (given by indices)   sel.i <- leaps::regsubsets(formula, data[-indices,], ...)   bics.i <- summary(sel.i)$bic   best.i <- coef(sel.i, 1:length(bics.i))[[which.min(bics.i)]]   x.names.i <- names(best.i)   m.best.i <- lm(y[-indices] ~ X[-indices, x.names.i] - 1)   # predict() doesn't work here:   fit.all.i <- as.vector(X[, x.names.i] %*% coef(m.best.i))   fit.i <- fit.all.i[indices]   # return the fitted values for i-th fold, CV criterion for all cases,   #   and the regression coefficients   list(     fit.i = fit.i,     # fitted values for i-th fold     crit.all.i = criterion(y, fit.all.i),     # CV crit for all cases     coefficients = if (details) {       # regression coefficients       coefs <- coef(m.best.i)              # fix coefficient names       names(coefs) <- sub(\"X\\\\[-indices, x.names.i\\\\]\", \"\",                           names(coefs))              coefs     }  else {       NULL     }   ) } selectSubsets(model=m.swiss) #> $criterion #> [1] 45.916 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" #>  #> $model #> NULL selectSubsets(model=m.swiss, indices=seq(5, 45, by=10)) #> $fit.i #> [1] 62.922 67.001 73.157 83.778 32.251 #>  #> $crit.all.i #> [1] 46.297 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" #>  #> $coefficients #>      (Intercept)      Agriculture        Education         Catholic  #>         63.80452         -0.15895         -1.04218          0.13066  #> Infant.Mortality  #>          1.01895 (cv.swiss <- cv(   selectSubsets,   working.model = m.swiss,   data = swiss,   seed = 8433 # use same folds )) #> R RNG seed set to 8433 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 65.835 #> bias-adjusted cross-validation criterion = 63.644 #> full-sample criterion = 45.916 compareFolds(cv.swiss) #> CV criterion by folds: #>   fold.1   fold.2   fold.3   fold.4   fold.5   fold.6   fold.7   fold.8  #>  76.6964  61.3105 131.1616   9.0662  52.9403  41.0853  51.8768 136.9498  #>   fold.9  fold.10  #>  24.1808  82.2587  #>  #> Coefficients by folds: #>         (Intercept) Catholic Education Infant.Mortality Agriculture #> Fold 1      59.0852   0.1397   -1.0203           1.2985       -0.17 #> Fold 2      67.0335   0.1367   -1.0499           0.9413       -0.20 #> Fold 3      55.0453   0.1221   -0.8757           1.3541       -0.15 #> Fold 4      62.5543   0.1236   -0.9719           1.0679       -0.16 #> Fold 5      50.4643   0.1057   -0.7863           1.2144             #> Fold 6      68.0289   0.1195   -1.0073           0.8294       -0.17 #> Fold 7      66.5219   0.1357   -1.0827           0.9523       -0.19 #> Fold 8      46.3507   0.0776   -0.7637           1.4463             #> Fold 9      62.2632   0.1230   -1.0067           1.1000       -0.17 #> Fold 10     52.5112   0.1005   -0.7232           1.0809"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv-mixed.html","id":"example-the-high-school-and-beyond-data","dir":"Articles","previous_headings":"","what":"Example: The High-School and Beyond data","title":"Cross-validating mixed-effects models","text":"Following use Raudenbush & Bryk (2002), data 1982 High School Beyond (HSB) survey become staple literature mixed-effects models. HSB data used Fox & Weisberg (2019, sec. 7.2.2) illustrate application linear mixed models hierarchical data, ’ll closely follow example . HSB data included MathAchieve MathAchSchool data sets nlme package (Pinheiro & Bates, 2000). MathAchieve includes individual-level data 7185 students 160 high schools, MathAchSchool includes school-level data: first students school number 1224 last school 9586. ’ll use School, SES (students’ socioeconomic status), MathAch (score standardized math-achievement test) variables MathAchieve data set, Sector (\"Catholic\" \"Public\") MathAchSchool data set. data-management required fitting mixed-effects model HSB data, use dplyr package (Wickham, François, Henry, Müller, & Vaughan, 2023): process, created two new school-level variables: meanses, average SES students school; cses, school-average SES centered mean. details, see Fox & Weisberg (2019, sec. 7.2.2). Still following Fox Weisberg, proceed use lmer() function lme4 package (Bates, Mächler, Bolker, & Walker, 2015) fit mixed model math achievement HSB data: can cross-validate cluster (.e., school) level, case (.e., student) level, cluster-level CV, clusterVariables argument tells cv() clusters defined. one clustering variable, say classes within schools, provided character vector variable names: clusterVariables = c(\"school\", \"class\"). cluster-level CV, default k = \"loo\", , leave one cluster time; instead specify k = 10 folds clusters, fold therefore comprising \\(160/10 = 16\\) schools. clusterVariables argument omitted, case-level CV employed, k = 10 folds default, \\(7185/10 \\approx 719\\) students. Notice one 10 models refit fold removed failed converge. Convergence problems common mixed-effects modeling. apparent issue estimated variance component close equal 0, boundary parameter space. shouldn’t disqualify fitted model kind prediction required cross-validation. also cv() method linear mixed models fit lme() function nlme package, arguments cv() case model fit lmer() glmer(). illustrate mixed model fit HSB data: used random-number generator seeds previous example cross-validating model fit lmer(), folds employed cases.3 estimated covariance components fixed effects summary output differ slightly lmer() lme() solutions, although functions seek maximize REML criterion. , course, expected different algorithms used numerical optimization. precision reported, cluster-level CV results lmer() lme() models identical, case-level CV results similar identical.","code":"data(\"MathAchieve\", package = \"nlme\") dim(MathAchieve) #> [1] 7185    6 head(MathAchieve, 3) #>   School Minority    Sex    SES MathAch MEANSES #> 1   1224       No Female -1.528   5.876  -0.428 #> 2   1224       No Female -0.588  19.708  -0.428 #> 3   1224       No   Male -0.528  20.349  -0.428 tail(MathAchieve, 3) #>      School Minority    Sex    SES MathAch MEANSES #> 7183   9586       No Female  1.332  19.641   0.627 #> 7184   9586       No Female -0.008  16.241   0.627 #> 7185   9586       No Female  0.792  22.733   0.627 data(\"MathAchSchool\", package = \"nlme\") dim(MathAchSchool) #> [1] 160   7 head(MathAchSchool, 2) #>      School Size Sector PRACAD DISCLIM HIMINTY MEANSES #> 1224   1224  842 Public   0.35   1.597       0  -0.428 #> 1288   1288 1855 Public   0.27   0.174       0   0.128 tail(MathAchSchool, 2) #>      School Size   Sector PRACAD DISCLIM HIMINTY MEANSES #> 9550   9550 1532   Public   0.45   0.791       0   0.059 #> 9586   9586  262 Catholic   1.00  -2.416       0   0.627 library(\"dplyr\") MathAchieve %>% group_by(School) %>%   summarize(mean.ses = mean(SES)) -> Temp Temp <- merge(MathAchSchool, Temp, by = \"School\") HSB <- merge(Temp[, c(\"School\", \"Sector\", \"mean.ses\")],              MathAchieve[, c(\"School\", \"SES\", \"MathAch\")], by = \"School\") names(HSB) <- tolower(names(HSB))  HSB$cses <- with(HSB, ses - mean.ses) library(\"lme4\") hsb.lmer <- lmer(mathach ~ mean.ses * cses + sector * cses                  + (cses | school), data = HSB) summary(hsb.lmer, correlation = FALSE) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: mathach ~ mean.ses * cses + sector * cses + (cses | school) #>    Data: HSB #>  #> REML criterion at convergence: 46504 #>  #> Scaled residuals:  #>    Min     1Q Median     3Q    Max  #> -3.159 -0.723  0.017  0.754  2.958  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr #>  school   (Intercept)  2.380   1.543         #>           cses         0.101   0.318    0.39 #>  Residual             36.721   6.060         #> Number of obs: 7185, groups:  school, 160 #>  #> Fixed effects: #>                     Estimate Std. Error t value #> (Intercept)           12.128      0.199   60.86 #> mean.ses               5.333      0.369   14.45 #> cses                   2.945      0.156   18.93 #> sectorCatholic         1.227      0.306    4.00 #> mean.ses:cses          1.039      0.299    3.48 #> cses:sectorCatholic   -1.643      0.240   -6.85 library(\"cv\")  cv(hsb.lmer,    k = 10,    clusterVariables = \"school\",    seed = 5240) #> R RNG seed set to 5240 #> 10-Fold Cross Validation based on 160 {school} clusters #> criterion: mse #> cross-validation criterion = 39.157 #> bias-adjusted cross-validation criterion = 39.148 #> 95% CI for bias-adjusted CV criterion = (38.066, 40.231) #> full-sample criterion = 39.006 cv(hsb.lmer, seed = 1575) #> R RNG seed set to 1575 #> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : #> Model failed to converge with max|grad| = 0.00587207 (tol = 0.002, component 1) #> boundary (singular) fit: see help('isSingular') #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 37.445 #> bias-adjusted cross-validation criterion = 37.338 #> 95% CI for bias-adjusted CV criterion = (36.288, 38.388) #> full-sample criterion = 36.068 library(\"nlme\") hsb.lme <- lme(   mathach ~ mean.ses * cses + sector * cses,   random = ~ cses | school,   data = HSB,   control = list(opt = \"optim\") ) summary(hsb.lme) #> Linear mixed-effects model fit by REML #>   Data: HSB  #>     AIC   BIC logLik #>   46525 46594 -23252 #>  #> Random effects: #>  Formula: ~cses | school #>  Structure: General positive-definite, Log-Cholesky parametrization #>             StdDev   Corr   #> (Intercept) 1.541177 (Intr) #> cses        0.018174 0.006  #> Residual    6.063492        #>  #> Fixed effects:  mathach ~ mean.ses * cses + sector * cses  #>                       Value Std.Error   DF t-value p-value #> (Intercept)         12.1282   0.19920 7022  60.886   0e+00 #> mean.ses             5.3367   0.36898  157  14.463   0e+00 #> cses                 2.9421   0.15122 7022  19.456   0e+00 #> sectorCatholic       1.2245   0.30611  157   4.000   1e-04 #> mean.ses:cses        1.0444   0.29107 7022   3.588   3e-04 #> cses:sectorCatholic -1.6421   0.23312 7022  -7.044   0e+00 #>  Correlation:  #>                     (Intr) men.ss cses   sctrCt mn.ss: #> mean.ses             0.256                             #> cses                 0.000  0.000                      #> sectorCatholic      -0.699 -0.356  0.000               #> mean.ses:cses        0.000  0.000  0.295  0.000        #> cses:sectorCatholic  0.000  0.000 -0.696  0.000 -0.351 #>  #> Standardized Within-Group Residuals: #>       Min        Q1       Med        Q3       Max  #> -3.170106 -0.724877  0.014892  0.754263  2.965498  #>  #> Number of Observations: 7185 #> Number of Groups: 160 cv(hsb.lme,    k = 10,    clusterVariables = \"school\",    seed = 5240) #> R RNG seed set to 5240 #> 10-Fold Cross Validation based on 160 {school} clusters #> criterion: mse #> cross-validation criterion = 39.157 #> bias-adjusted cross-validation criterion = 39.149 #> 95% CI for bias-adjusted CV criterion = (38.066, 40.232) #> full-sample criterion = 39.006 cv(hsb.lme, seed = 1575) #> R RNG seed set to 1575 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 37.442 #> bias-adjusted cross-validation criterion = 37.402 #> 95% CI for bias-adjusted CV criterion = (36.351, 38.453) #> full-sample criterion = 36.147"},{"path":"https://gmonette.github.io/cv/articles/cv-mixed.html","id":"example-contrived-hierarchical-data","dir":"Articles","previous_headings":"","what":"Example: Contrived hierarchical data","title":"Cross-validating mixed-effects models","text":"introduce artificial data set exemplifies aspects cross-validation particular hierarchical models. Using data set, show model comparisons employing cluster-based employing case-based cross-validation may agree “best” model. Furthermore, commonly used measures fit, mean-squared error, necessarily become smaller models become larger, even models nested, even measure fit computed whole data set. Consider researcher studying improvement skill, yodeling, example, among students enrolled four-year yodeling program. plan measure student’s skill level beginning program every year thereafter end program, resulting five annual measurements student. turns yodeling appeals students ages, students enrolling program range age 20 70. Moreover, participants’ untrained yodeling skill similar ages, rate progress training. students complete four-year program. researcher, expertise yodeling modeling, decides model response, \\(y\\), yodeling skill, function age, \\(x\\), reasoning students get older stay program, (incorrectly) age can serve proxy elapsed time. researcher knows mixed model used account clustering due expected similarity measurements taken student. start generating data, using parameters consistent description meant highlight issues arise cross-validating mixed-effects models:4 scatterplot data representative group 10 (without loss generality, first 10) 100 students, showing 95% concentration ellipse cluster:5 Hierarchical data set, showing first 10 100 students. -student effect age 0 within-student effect 1. Due large variation ages students, least-squares regression yodeling skill age (500 observations among 100 students) produces estimated slope close 0 (though small \\(p\\)-value), slope heavily weighted toward -student effect: initial mixed-effects model fit data simple random-intercepts model: shortly consider three , complex, mixed models; data-management considerations, convenient fit now, defer discussion models: proceed obtain predictions random-intercept model (mod.0) models (mod.1, mod.2, mod.3) based fixed effects alone, used cross-validation based clusters (.e., students), fixed random effects—-called best linear unbiased predictions BLUPs—used cross-validation based cases (.e., occasions within students): prepare data plotting: Predictions based random-intercept model mod.0 first 10 students shown following graph: Predictions random intercept model. fixed-effect predictions various individuals identical—estimated fixed-effects intercept estimated general mean \\(y\\)—BLUPs sums fixed-effects intercept random intercepts, slightly shrunken towards general mean. artificial data population relationship age skill, fixed-effect-predictions BLUPs different. next model, mod.1, includes fixed intercept fixed effect x along random intercept: Predictions model appear following graph: Predictions model random intercepts \\(x\\) fixed-effect predictor. BLUPs fit observed data closely, predictions based fixed effects alone, common intercept slope clusters, poor—indeed, much worse fixed-effects-predictions based simpler random-intercept model, mod.0. therefore anticipate (show later section) case-based cross-validation prefer mod1 mod0, cluster-based cross-validation prefer mod0 mod1. third model, mod.2, includes contextual effect \\(x\\)—, cluster mean xm—along \\(x\\) intercept fixed-effect part model, random intercept: model equivalent fitting y ~ (x - xm) + xm + (1 | group), model generated data coefficient contextual predictor xm set 0 (mod.3, discussed ). Predictions model mod.2 appear following graph: Predictors model random intercepts, \\(x\\), group (student) mean \\(x\\) predictors. Depending estimated variance parameters model, mixed model like mod.2 apply varying degrees shrinkage random-intercept BLUPs correspond variation heights parallel fitted lines individual students. contrived data, mod.2 applies little shrinkage, allowing substantial variability heights fitted lines, closely approach observed values student. fit mixed model mod.2 consequently similar fixed-effects model age categorical predictor individual students (.e., treating students factor, shown ). mixed model mod.2 therefore fits individual observations well, anticipate favorable assessment using individual-based cross-validation. contrast, large variability BLUPs results larger residuals predictions based fixed effects alone, expect cluster-based cross-validation won’t show advantage model mod.2 compared smaller model mod.0, includes fixed random intercepts. mixed model applied considerable shrinkage, neither cluster-based case-based cross-validation show much improvement random-intercept-model. experience, degree shrinkage vary smoothly parameters changed tends “nothing,” near tipping point, behavior estimates can affected considerably choice algorithm used fit model. Finally, mod.3 directly estimates model used generate data. mentioned, constrained version mod.2, coefficient xm set 0, x expressed deviation cluster mean xm: predictions mod.3 therefore similar mod.2: Predictions estimated model generating data. next carry case-based cross-validation, , explained, based fixed predicted random effects (.e., BLUPs), cluster-based cross-validation, based fixed effects . order reduce -model random variability comparisons models, apply cv() list models created models() function (introduced previously), performing cross-validation folds model: 10-fold cluster-based cross-validation comparing random intercept models varying fixed effects. error bars show 95% confidence interval around CV estimate MSE model. 10-fold case-based cross-validation comparing random intercept models varying fixed effects. summary, model mod.1, \\(x\\) alone without contextual mean \\(x\\), assessed fitting poorly cluster-based CV, relatively much better case-based CV. Model mod.2, includes \\(x\\) contextual mean, produces better results using cluster-based case-based CV. data-generating model, mod.3, includes fixed effect x - xm place separate terms x xm, isn’t distinguishable model mod.2, includes x xm separately, even though mod.2 unnecessary parameter (recall population coefficient xm 0 x expressed deviations contextual mean). conclusions consistent observations based graphing predictions various models, illustrate desirability assessing mixed-effect models different hierarchical levels.","code":"# Parameters: set.seed(9693) Nb <- 100     # number of groups Nw <- 5       # number of individuals within groups Bb <- 0       # between-group regression coefficient on group mean SDre <-   2.0   # between-group SD of random level relative to group mean of x SDwithin <- 0.5  # within group SD Bw <- 1          # within group effect of x Ay <- 10         # intercept for response Ax <- 20         # starting level of x Nx <- Nw * 10    # number of distinct x values  Data <- data.frame(group = factor(rep(1:Nb, each = Nw)),                    x = Ax + rep(1:Nx, length.out = Nw * Nb)) |>   within({     xm  <- ave(x, group, FUN = mean) # within-group mean     y <- Ay +       Bb * xm +                      # contextual effect       Bw * (x - xm) +                # within-group effect       rnorm(Nb, sd = SDre)[group] +  # random level by group       rnorm(Nb * Nw, sd = SDwithin)  # random error within groups   }) library(\"lattice\") library(\"latticeExtra\") plot <- xyplot(   y ~ x,   data = Data[1:Nx,],   group = group,   ylim = c(4, 16),   par.settings = list(superpose.symbol = list(pch = 1, cex =                                                 0.7)) ) +   layer(panel.ellipse(..., center.cex = 0)) plot # display graph summary(lm(y ~ x, data=Data)) #>  #> Call: #> lm(formula = y ~ x, data = Data) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -5.771 -1.658 -0.089  1.552  7.624  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  9.05043    0.34719   26.07   <2e-16 *** #> x            0.02091    0.00727    2.87   0.0042 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 2.35 on 498 degrees of freedom #> Multiple R-squared:  0.0163, Adjusted R-squared:  0.0143  #> F-statistic: 8.26 on 1 and 498 DF,  p-value: 0.00422 # random intercept only: mod.0 <- lmer(y ~ 1 + (1 | group), Data) summary(mod.0) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: y ~ 1 + (1 | group) #>    Data: Data #>  #> REML criterion at convergence: 2103.1 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.0351 -0.7264 -0.0117  0.7848  2.0438  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  group    (Intercept) 2.90     1.70     #>  Residual             2.71     1.65     #> Number of obs: 500, groups:  group, 100 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)   10.002      0.186    53.9 # effect of x and random intercept: mod.1 <- lmer(y ~ x + (1 | group), Data)  # effect of x, contextual (student) mean of x, and random intercept: mod.2 <- lmer(y ~ x + xm + (1 | group), Data)         # equivalent to y ~ I(x - xm) + xm + (1 | group)  # model generating the data (where Bb = 0) mod.3 <- lmer(y ~ I(x - xm) + (1 | group), Data) Data <- within(Data, {   fit_mod0.fe <- predict(mod.0, re.form = ~ 0) # fixed effects only   fit_mod0.re <- predict(mod.0) # fixed and random effects (BLUPs)   fit_mod1.fe <- predict(mod.1, re.form = ~ 0)   fit_mod1.re <- predict(mod.1)   fit_mod2.fe <- predict(mod.2, re.form = ~ 0)   fit_mod2.re <- predict(mod.2)   fit_mod3.fe <- predict(mod.3, re.form = ~ 0)   fit_mod3.re <- predict(mod.3) }) Data_long <- reshape(Data[1:Nx, ], direction = \"long\", sep = \".\",                timevar = \"effect\", varying = grep(\"\\\\.\", names(Data[1:Nx, ]))) Data_long$id <- 1:nrow(Data_long) Data_long <- reshape(Data_long, direction = \"long\", sep = \"_\",                timevar = \"modelcode\",  varying = grep(\"_\", names(Data_long))) Data_long$model <- factor(   c(\"~ 1\", \"~ 1 + x\", \"~ 1 + x + xm\", \"~ 1 + I(x - xm)\")   [match(Data_long$modelcode, c(\"mod0\", \"mod1\", \"mod2\", \"mod3\"))] ) (   plot +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod0\" & effect == \"fe\"),       groups = group,       type = \"l\",       lwd = 2     ) +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod0\" &  effect == \"re\"),       groups = group,       type = \"l\",       lwd = 2,       lty = 3     ) ) |> update(   main=\"Model: y ~ 1 + (1 | group)\",   key=list(     corner=c(0.05, 0.05),     text=list(c(\"fixed effects only\",\"fixed and random\")),     lines=list(lty=c(1, 3)))) summary(mod.1) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: y ~ x + (1 | group) #>    Data: Data #>  #> REML criterion at convergence: 1564.5 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.9016 -0.6350  0.0188  0.5541  2.8293  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  group    (Intercept) 192.941  13.890   #>  Residual               0.257   0.507   #> Number of obs: 500, groups:  group, 100 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept) -33.9189     1.5645   -21.7 #> x             0.9653     0.0158    61.0 #>  #> Correlation of Fixed Effects: #>   (Intr) #> x -0.460 (   plot +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod1\" & effect == \"fe\"),       groups = group,       type = \"l\",       lwd = 2     ) +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod1\" & effect == \"re\"),       groups = group,       type = \"l\",       lwd = 2,       lty = 3     ) ) |> update(   main=\"Model: y ~ 1 + x + (1 | group)\",   ylim=c(-15, 35),   key=list(     corner=c(0.95, 0.05),     text=list(c(\"fixed effects only\",\"fixed and random\")),     lines=list(lty=c(1, 3)))) summary(mod.2) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: y ~ x + xm + (1 | group) #>    Data: Data #>  #> REML criterion at convergence: 1169.2 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.9847 -0.6375  0.0019  0.5568  2.7325  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  group    (Intercept) 3.399    1.844    #>  Residual             0.255    0.505    #> Number of obs: 500, groups:  group, 100 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)   9.4787     0.6171    15.4 #> x             0.9915     0.0160    62.1 #> xm           -0.9800     0.0206   -47.7 #>  #> Correlation of Fixed Effects: #>    (Intr) x      #> x   0.000        #> xm -0.600 -0.777 (   plot +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod2\" & effect == \"fe\"),       groups = group,       type = \"l\",       lwd = 2     ) +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod2\" & effect == \"re\"),       groups = group,       type = \"l\",       lwd = 2,       lty = 3     ) ) |> update(   main=\"Model: y ~ 1 + x + xm + (1 | group)\",   ylim=c(4, 16),   key=list(     corner=c(0.05, 0.05),     text=list(c(\"fixed effects only\",\"fixed and random\")),     lines=list(lty=c(1, 3)))) summary(mod.3) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: y ~ I(x - xm) + (1 | group) #>    Data: Data #>  #> REML criterion at convergence: 1163.2 #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -2.9770 -0.6320  0.0063  0.5603  2.7249  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  group    (Intercept) 3.391    1.842    #>  Residual             0.255    0.505    #> Number of obs: 500, groups:  group, 100 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)   10.002      0.185    53.9 #> I(x - xm)      0.992      0.016    62.1 #>  #> Correlation of Fixed Effects: #>           (Intr) #> I(x - xm) 0.000 (   plot +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod3\" & effect == \"fe\"),       groups = group,       type = \"l\",       lwd = 2     ) +     xyplot(       fit ~ x,       subset(Data_long, modelcode == \"mod3\" & effect == \"re\"),       groups = group,       type = \"l\",       lwd = 2,       lty = 3     ) ) |> update(   main=\"Model: y ~ 1 + I(x - xm) + (1 | group)\",   ylim=c(4, 16),   key=list(     corner=c(0.05, 0.05),     text=list(c(\"fixed effects only\",\"fixed and random\")),     lines=list(lty=c(1, 3)))) modlist <- models(   \"~ 1\" = mod.0,   \"~ 1 + x\" = mod.1,   \"~ 1 + x + xm\" = mod.2,   \"~ 1 + I(x - xm)\" = mod.3 ) cvs_clusters <-   cv(     modlist,     data = Data,     cluster = \"group\",     k = 10,     seed = 6449   ) plot(cvs_clusters, main = \"Model Comparison, Cluster-Based CV\") cvs_cases <- cv(modlist, data = Data, seed = 9693) plot(cvs_cases, main = \"Model Comparison, Case-Based CV\")"},{"path":"https://gmonette.github.io/cv/articles/cv-mixed.html","id":"example-crossed-random-effects","dir":"Articles","previous_headings":"","what":"Example: Crossed random effects","title":"Cross-validating mixed-effects models","text":"Crossed random effects arise structure data aren’t strictly hierarchical. Nevertheless, crossed nested random effects can handled much manner, refitting mixed-effects model data fold clusters cases removed using refitted model predict response removed fold. ’ll illustrate data pig growth, introduced Diggle, Liang, & Zeger (1994, Table 3.1). data Pigs data frame cv package: 48 pigs observed weekly period 9 weeks, weight pig recorded kg. data “long” format, appropriate use lmer() function lme4 package. data regular, missing cases. following graph, showing growth trajectories pigs, similar Figure 3.1 Diggle et al. (1994); add overall least-squares line loess smooth, nearly indistinguishable: Growth trajectories 48 pigs, overall least-squares line (sold blue) loess line (broken magenta). individual “growth curves” overall trend generally linear, tendency variability pig weight increase weeks (feature data ignore mixed model fit data ). Stata mixed-effects models manual proposes model crossed random effects Pigs data (StataCorp LLC, 2023, p. 37): [S]uppose wish fit \\[ \\mathrm{weight}_{ij} = \\beta_0 + \\beta_1 \\mathrm{week}_{ij} + u_i + v_j + \\varepsilon_{ij} \\] \\(= 1, \\ldots, 9\\) weeks \\(j = 1, \\dots, 48\\) pigs \\[ u_i \\sim N(0, \\sigma^2_u); v_j \\sim N(0, \\sigma^2_v ); \\varepsilon_{ij} \\sim N(0, \\sigma^2_\\varepsilon) \\] independently. , assume overall population-average growth curve \\(\\beta_0 + \\beta_1 \\mathrm{week}\\) random pig-specific shift. words, effect due week, \\(u_i\\), systematic week common pigs. rationale behind [model] , assuming pigs measured contemporaneously, might concerned week-specific random factors weather feeding patterns significant systematic effects pigs. Although might prefer alternative model,6 think reasonable specification. Stata manual fits mixed model maximum likelihood (rather REML), duplicate results reported using lmer(): opt non-default \"bobyqa\" optimizer provides numerically stable results subsequent cross-validation example. can cross-validate model omitting folds composed pigs, folds composed weeks, folds composed pig-weeks (Pigs data set correspond individual cases, using fixed effects): can also cross-validate individual cases taking account random effects (employing 10 folds): predictions based BLUPs, accurate predictions based fixed effects.7 well, difference MSE computed model fit full data CV estimates MSE greater cluster-based predictions.","code":"head(Pigs, 9) #>   id week weight #> 1  1    1   24.0 #> 2  1    2   32.0 #> 3  1    3   39.0 #> 4  1    4   42.5 #> 5  1    5   48.0 #> 6  1    6   54.5 #> 7  1    7   61.0 #> 8  1    8   65.0 #> 9  1    9   72.0 head(xtabs( ~ id + week, data = Pigs), 3) #>    week #> id  1 2 3 4 5 6 7 8 9 #>   1 1 1 1 1 1 1 1 1 1 #>   2 1 1 1 1 1 1 1 1 1 #>   3 1 1 1 1 1 1 1 1 1 tail(xtabs( ~ id + week, data = Pigs), 3) #>     week #> id   1 2 3 4 5 6 7 8 9 #>   46 1 1 1 1 1 1 1 1 1 #>   47 1 1 1 1 1 1 1 1 1 #>   48 1 1 1 1 1 1 1 1 1 plot(weight ~ week, data = Pigs, type = \"n\") for (i in unique(Pigs$id)) {   with(Pigs, lines(     x = 1:9,     y = Pigs[id == i, \"weight\"],     col = \"gray\"   )) } abline(lm(weight ~ week, data = Pigs),        col = \"blue\",        lwd = 2) lines(   with(Pigs, loess.smooth(week, weight, span = 0.5)),   col = \"magenta\",   lty = 2,   lwd = 2 ) m.p <- lmer(   weight ~ week + (1 | id) + (1 | week),   data = Pigs,   REML = FALSE, # i.e., ML   control = lmerControl(optimizer = \"bobyqa\") ) summary(m.p) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: weight ~ week + (1 | id) + (1 | week) #>    Data: Pigs #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   2037.6   2058.0  -1013.8   2027.6      427  #>  #> Scaled residuals:  #>    Min     1Q Median     3Q    Max  #> -3.775 -0.542  0.005  0.476  3.982  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  id       (Intercept) 14.836   3.852    #>  week     (Intercept)  0.085   0.292    #>  Residual              4.297   2.073    #> Number of obs: 432, groups:  id, 48; week, 9 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept)  19.3556     0.6334    30.6 #> week          6.2099     0.0539   115.1 #>  #> Correlation of Fixed Effects: #>      (Intr) #> week -0.426 cv(m.p, clusterVariables = \"id\") #> n-Fold Cross Validation based on 48 {id} clusters #> criterion: mse #> cross-validation criterion = 19.973 #> bias-adjusted cross-validation criterion = 19.965 #> 95% CI for bias-adjusted CV criterion = (17.125, 22.805) #> full-sample criterion = 19.201 cv(m.p, clusterVariables = \"week\") #> boundary (singular) fit: see help('isSingular') #> n-Fold Cross Validation based on 9 {week} clusters #> criterion: mse #> cross-validation criterion = 19.312 #> bias-adjusted cross-validation criterion = 19.305 #> 95% CI for bias-adjusted CV criterion = (16.566, 22.044) #> full-sample criterion = 19.201 cv(   m.p,   clusterVariables = c(\"id\", \"week\"),   k = 10,   seed = 8469 ) #> R RNG seed set to 8469 #> 10-Fold Cross Validation based on 432 {id, week} clusters #> criterion: mse #> cross-validation criterion = 19.235 #> bias-adjusted cross-validation criterion = 19.233 #> 95% CI for bias-adjusted CV criterion = (16.493, 21.973) #> full-sample criterion = 19.201 cv(m.p, k = 10, seed = 8469) #> R RNG seed set to 8469 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 5.1583 #> bias-adjusted cross-validation criterion = 5.0729 #> 95% CI for bias-adjusted CV criterion = (4.123, 6.0229) #> full-sample criterion = 3.796"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv-notes.html","id":"efficient-computations-for-linear-and-generalized-linear-models","dir":"Articles","previous_headings":"","what":"Efficient computations for linear and generalized linear models","title":"Computational and technical notes on cross-validating regression models","text":"straightforward way implement cross-validation R statistical modeling functions written canonical manner use update() refit model fold removed. approach taken default method cv(), appropriate cases independently sampled. Refitting model manner fold generally feasible number folds modest, can prohibitively costly leave-one-cross-validation number cases large. \"lm\" \"glm\" methods cv() take advantage computational efficiencies avoiding refitting model fold removed. Consider, particular, weighted linear model \\(\\mathbf{y}_{n \\times 1} = \\mathbf{X}_{n \\times p}\\boldsymbol{\\beta}_{p \\times 1} + \\boldsymbol{\\varepsilon}_{n \\times 1}\\), \\(\\boldsymbol{\\varepsilon} \\sim \\mathbf{N}_n \\left(\\mathbf{0}, \\sigma^2 \\mathbf{W}^{-1}_{n \\times n}\\right)\\). , \\(\\mathbf{y}\\) response vector, \\(\\mathbf{X}\\) model matrix, \\(\\boldsymbol{\\varepsilon}\\) error vector, \\(n\\) cases, \\(\\boldsymbol{\\beta}\\) vector \\(p\\) population regression coefficients. errors assumed multivariately normally distributed 0 means covariance matrix \\(\\sigma^2 \\mathbf{W}^{-1}\\), \\(\\mathbf{W} = \\mathrm{diag}(w_i)\\) diagonal matrix inverse-variance weights. linear model constant error variance, weight matrix taken \\(\\mathbf{W} = \\mathbf{}_n\\), order-\\(n\\) identity matrix. weighted-least-squares (WLS) estimator \\(\\boldsymbol{\\beta}\\) (see, e.g., Fox, 2016, sec. 12.2.2) 1 \\[ \\mathbf{b}_{\\mathrm{WLS}} = \\left( \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\right)^{-1}   \\mathbf{X}^T \\mathbf{W} \\mathbf{y} \\] Fitted values \\(\\widehat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}_{\\mathrm{WLS}}\\). LOO fitted value \\(\\)th case can efficiently computed \\(\\widehat{y}_{-} = y_i - e_i/(1 - h_i)\\) \\(h_i = \\mathbf{x}^T_i \\left( \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i\\) (-called “hatvalue”). , \\(\\mathbf{x}^T_i\\) \\(\\)th row \\(\\mathbf{X}\\), \\(\\mathbf{x}_i\\) \\(\\)th row written column vector. approach can break one hatvalues equal 1, case formula \\(\\widehat{y}_{-}\\) requires division 0. compute cross-validated fitted values folds contain one case, make use Woodbury matrix identity (Hager, 1989), \\[ \\left(\\mathbf{}_{m \\times m} + \\mathbf{U}_{m \\times k} \\mathbf{C}_{k \\times k} \\mathbf{V}_{k \\times m} \\right)^{-1} = \\mathbf{}^{-1} - \\mathbf{}^{-1}\\mathbf{U} \\left(\\mathbf{C}^{-1} + \\mathbf{VA}^{-1}\\mathbf{U} \\right)^{-1} \\mathbf{VA}^{-1} \\] \\(\\mathbf{}\\) nonsingular order-\\(n\\) matrix. apply result letting \\[\\begin{align*}     \\mathbf{} &= \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\\\     \\mathbf{U} &= \\mathbf{X}_\\mathbf{j}^T \\\\     \\mathbf{V} &= - \\mathbf{X}_\\mathbf{j} \\\\     \\mathbf{C} &= \\mathbf{W}_\\mathbf{j} \\\\ \\end{align*}\\] subscript \\(\\mathbf{j} = (i_{j1}, \\ldots, i_{jm})^T\\) represents vector indices cases \\(j\\)th fold, \\(j = 1, \\ldots, k\\). negative sign \\(\\mathbf{V} = - \\mathbf{X}_\\mathbf{j}\\) reflects removal, rather addition, cases \\(\\mathbf{j}\\). Applying Woodbury identity isn’t quite fast using hatvalues, generally much faster refitting model. disadvantage Woodbury identity, however, entails explicit matrix inversion thus may numerically unstable. inverse \\(\\mathbf{} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\\) available directly \"lm\" object, second term right-hand side Woodbury identity requires matrix inversion fold deleted. (contrast, inverse \\(\\mathbf{C} = \\mathbf{W}_\\mathbf{j}\\) straightforward \\(\\mathbf{W}\\) diagonal.) Woodbury identity also requires model matrix full rank. impose restriction code removing redundant regressors model matrix cases, doesn’t preclude rank deficiency surfacing fold removed. Rank deficiency \\(\\mathbf{X}\\) doesn’t disqualify cross-validation need fitted values estimated model. glm() computes maximum-likelihood estimates generalized linear model iterated weighted least squares (see, e.g., Fox & Weisberg, 2019, sec. 6.12). last iteration therefore just WLS fit “working response” model matrix using “working weights.” working weights working response convergence available information object returned glm(). treat re-estimation model case cases deleted WLS problem, using hatvalues Woodbury matrix identity. resulting fitted values deleted fold aren’t exact—, except Gaussian family, result isn’t identical obtain literally refitting model—(limited) experience, approximation good, especially LOO CV, tempted use . Nevertheless, results approximate, default \"glm\" cv() method perform exact computation, entails refitting model fold omitted. Let’s compare efficiency various computational methods linear generalized linear models. Consider, example, leave-one-cross-validation quadratic regression mpg horsepower Auto data, introductory “Cross-validating regression models” vignette, repeated : small regression problem three computational approaches essentially instantaneous, still interest investigate relative speed. comparison, include cv.glm() function boot package (Canty & Ripley, 2022; Davison & Hinkley, 1997), takes naive approach, fit linear model equivalent Gaussian GLM. use microbenchmark() function package name timings (Mersmann, 2023): computer, using hatvalues order magnitude faster employing Woodbury matrix updates, two orders magnitude faster refitting model.2 Similarly, let’s return logistic-regression model fit Mroz’s data women’s labor-force participation, also employed example introductory vignette: linear models, report timings various cv() methods computation LOO CV well cv.glm() function boot package (, recall, refits model case removed, thus comparable cv() method=\"exact\"): substantial time penalty associated exact computations.","code":"data(\"Auto\", package=\"ISLR2\") m.auto <- lm(mpg ~ poly(horsepower, 2), data = Auto) summary(m.auto) #>  #> Call: #> lm(formula = mpg ~ poly(horsepower, 2), data = Auto) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.714  -2.594  -0.086   2.287  15.896  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)            23.446      0.221   106.1   <2e-16 *** #> poly(horsepower, 2)1 -120.138      4.374   -27.5   <2e-16 *** #> poly(horsepower, 2)2   44.090      4.374    10.1   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.37 on 389 degrees of freedom #> Multiple R-squared:  0.688,  Adjusted R-squared:  0.686  #> F-statistic:  428 on 2 and 389 DF,  p-value: <2e-16 library(\"cv\") #> Loading required package: doParallel #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel cv(m.auto, k = \"loo\")  # default method = \"hatvalues\" #> n-Fold Cross Validation #> method: hatvalues #> criterion: mse #> cross-validation criterion = 19.248 cv(m.auto, k = \"loo\", method = \"naive\") #> n-Fold Cross Validation #> method: naive #> criterion: mse #> cross-validation criterion = 19.248 #> bias-adjusted cross-validation criterion = 19.248 #> full-sample criterion = 18.985 cv(m.auto, k = \"loo\", method = \"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 19.248 #> bias-adjusted cross-validation criterion = 19.248 #> full-sample criterion = 18.985 m.auto.glm <- glm(mpg ~ poly(horsepower, 2), data = Auto) boot::cv.glm(Auto, m.auto.glm)$delta #> [1] 19.248 19.248 microbenchmark::microbenchmark(   hatvalues = cv(m.auto, k = \"loo\"),   Woodbury = cv(m.auto, k = \"loo\", method = \"Woodbury\"),   naive = cv(m.auto, k = \"loo\", method = \"naive\"),   cv.glm = boot::cv.glm(Auto, m.auto.glm),   times = 10 ) #> Warning in microbenchmark::microbenchmark(hatvalues = cv(m.auto, k = \"loo\"), : #> less accurate nanosecond times to avoid potential integer overflows #> Unit: microseconds #>       expr       min        lq     mean   median       uq      max neval #>  hatvalues    954.44    990.36   1142.4   1195.2   1227.1   1291.1    10 #>   Woodbury  12244.36  12579.66  13556.8  12718.5  15357.5  16250.9    10 #>      naive 220790.86 223278.66 258972.7 225179.6 339790.2 343251.2    10 #>     cv.glm 381203.85 384217.35 401333.4 387135.7 395736.6 471276.3    10 data(\"Mroz\", package=\"carData\") m.mroz <- glm(lfp ~ ., data = Mroz, family = binomial) summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4 cv(m.mroz, # default method = \"exact\"    k = \"loo\",     criterion = BayesRule) #> n-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.3183 #> 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164) #> full-sample criterion = 0.30677 cv(m.mroz,    k = \"loo\",    criterion = BayesRule,    method = \"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.3183 #> 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164) #> full-sample criterion = 0.30677 cv(m.mroz,    k = \"loo\",    criterion = BayesRule,    method = \"hatvalues\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: BayesRule #> cross-validation criterion = 0.32005 microbenchmark::microbenchmark(   hatvalues = cv(     m.mroz,     k = \"loo\",     criterion = BayesRule,     method = \"hatvalues\"   ),   Woodbury = cv(     m.mroz,     k = \"loo\",     criterion = BayesRule,     method = \"Woodbury\"   ),   exact = cv(m.mroz, k = \"loo\", criterion = BayesRule),   cv.glm = boot::cv.glm(Mroz, m.mroz,                         cost = BayesRule),   times = 10 ) #> Unit: milliseconds #>       expr       min        lq      mean    median        uq       max neval #>  hatvalues    1.1071    1.1823    1.7319    1.4125    1.4633    5.0927    10 #>   Woodbury   40.0416   42.2121   43.2522   43.2659   44.5798   45.2104    10 #>      exact 1776.2343 1797.3446 1816.2132 1815.2618 1837.8326 1851.0511    10 #>     cv.glm 2018.5416 2042.3843 2095.7281 2096.9051 2152.2171 2179.0707    10"},{"path":"https://gmonette.github.io/cv/articles/cv-notes.html","id":"computation-of-the-bias-corrected-cv-criterion-and-confidence-intervals","dir":"Articles","previous_headings":"","what":"Computation of the bias-corrected CV criterion and confidence intervals","title":"Computational and technical notes on cross-validating regression models","text":"Let \\(\\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}})\\) represent cross-validation cost criterion, mean-squared error, computed \\(n\\) values response \\(\\mathbf{y}\\) based fitted values \\(\\widehat{\\mathbf{y}}\\) model fit data. require \\(\\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}})\\) mean casewise components, , \\(\\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}}) = \\frac{1}{n}\\sum_{=1}^n\\mathrm{cv}(y_i, \\widehat{y}_i)\\).3 example, \\(\\mathrm{MSE}(\\mathbf{y}, \\widehat{\\mathbf{y}}) = \\frac{1}{n}\\sum_{=1}^n (y_i - \\widehat{y}_i)^2\\). divide \\(n\\) cases \\(k\\) folds approximately \\(n_j \\approx n/k\\) cases , \\(n = \\sum n_j\\). , let \\(\\mathbf{j}\\) denote indices cases \\(j\\)th fold. Now define \\(\\mathrm{CV}_j = \\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}}^{(j)})\\). superscript \\((j)\\) \\(\\widehat{\\mathbf{y}}^{(j)}\\) represents fitted values computed cases model fold \\(j\\) omitted. Let \\(\\widehat{\\mathbf{y}}^{(-)}\\) represent vector fitted values \\(n\\) cases fitted value \\(\\)th case computed model fit fold including \\(\\)th case omitted (.e., fold \\(j\\) \\(\\\\mathbf{j}\\)). cross-validation criterion just \\(\\mathrm{CV} = \\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}}^{(-)})\\). Following Davison & Hinkley (1997, pp. 293–295), bias-adjusted cross-validation criterion \\[ \\mathrm{CV}_{\\mathrm{adj}} = \\mathrm{CV} + \\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}}) - \\frac{1}{n} \\sum_{j=1}^{k} n_j \\mathrm{CV}_j \\] compute standard error CV \\[ \\mathrm{SE}(\\mathrm{CV}) = \\frac{1}{\\sqrt n} \\sqrt{ \\frac{\\sum_{=1}^n \\left[ \\mathrm{cv}(y_i, \\widehat{y}_i^{(-)} ) - \\mathrm{CV} \\right]^2 }{n - 1} } \\] , standard deviation casewise components CV divided square-root number cases. use \\(\\mathrm{SE}(\\mathrm{CV})\\) construct \\(100 \\times (1 - \\alpha)\\)% confidence interval around adjusted CV estimate error: \\[ \\left[ \\mathrm{CV}_{\\mathrm{adj}} - z_{1 - \\alpha/2}\\mathrm{SE}(\\mathrm{CV}), \\mathrm{CV}_{\\mathrm{adj}} + z_{1 - \\alpha/2}\\mathrm{SE}(\\mathrm{CV})  \\right] \\] \\(z_{1 - \\alpha/2}\\) \\(1 - \\alpha/2\\) quantile standard-normal distribution (e.g, \\(z \\approx 1.96\\) 95% confidence interval, \\(1 - \\alpha/2 = .975\\)). Bates, Hastie, & Tibshirani (2023) show coverage confidence interval poor small samples, suggest much computationally intensive procedure, called nested cross-validation, compute better estimates error confidence intervals better coverage small samples. may implement Bates et al.’s approach later release cv package. present use confidence interval sufficiently large \\(n\\), , based Bates et al.’s results, take default \\(n \\ge 400\\).","code":""},{"path":"https://gmonette.github.io/cv/articles/cv-notes.html","id":"why-the-complement-of-auc-isnt-a-casewise-cv-criterion","dir":"Articles","previous_headings":"","what":"Why the complement of AUC isn’t a casewise CV criterion","title":"Computational and technical notes on cross-validating regression models","text":"Consider calculating AUC folds validation set contains \\(n_v\\) observations. calculate AUC validation set, need vector prediction criteria, \\(\\widehat{\\mathbf{y}}_{v_{(n_v \\times 1)}} = (\\widehat{y}_1, ..., \\widehat{y}_{n_v})^T\\), vector observed responses validation set, \\(\\mathbf{y}_{v_{(n_v \\times 1)}} = (y_1, \\ldots, y_{n_v})^T\\) \\(y_i \\\\{0,1\\}, \\; = 1, \\ldots, n_v\\). construct ROC curve, ordering values \\(\\mathbf{\\widehat{y}}_v\\) relevant. Thus, assuming ties, reordering observations necessary, can set \\(\\mathbf{\\widehat{y}}_v = (1, 2, \\ldots, n_v)^T\\). AUC can expressed casewise mean sum function \\(\\mathrm{cv}(\\widehat{y}_i,y_i)\\), \\(\\mathrm{cv}: \\{1,2,...,n_v\\}\\times\\{0,1\\} \\rightarrow [0,1]\\), \\[\\begin{equation} \\label{eq:cw} \\tag{1} \\sum_{=1}^{n_v} \\mathrm{cv}(\\widehat{y}_i,y_i) = \\mathrm{AUC}(\\mathbf{\\widehat{y}}_v,\\mathbf{y}_v) \\end{equation}\\] must hold \\(2^{n_v}\\) possible values \\(\\mathbf{y}_v = (y_1,...,y_{n_v})^T\\). \\(y\\mathrm{s}\\) value, either 1 0, definition AUC ambiguous. AUC considered undefined, set 0 \\(y\\)s 0 1 \\(y\\)s 1. AUC considered undefined cases, \\(2^{n_v} - 2\\) admissible values \\(\\mathbf{y}_v\\). Thus, equation (\\(\\ref{eq:cw}\\)) produces either \\(2^{n_v}\\) \\(2^{n_v}-2\\) constraints. Although \\(2n_v\\) possible values \\(\\mathrm{cv(\\cdot)}\\) function, equation (\\(\\ref{eq:cw}\\)) , nevertheless, consistent solutions. therefore need determine whether value \\(n_v\\) (\\(\\ref{eq:cw}\\)) consistent solution admissible values \\(\\mathbf{y}_v\\). eventuality, shown AUC , general, expressed casewise sum. \\(n_v=3\\), show (\\(\\ref{eq:cw}\\)) consistent solution include possibilities \\(\\mathbf{y}_v\\), exclude cases \\(y\\)s value. \\(n_v=4\\), show consistent solutions either case. following R function computes AUC \\(\\mathbf{\\widehat{y}}_v\\) \\(\\mathbf{y}_v\\), accommodating cases \\(\\mathbf{y}_v\\) 0s 1s: define function generate possible \\(\\mathbf{y}_v\\)s length \\(n_v\\) rows matrix \\(\\mathbf{Y}_{(2^{n_v} \\times n_v)}\\): \\(n_v=3\\), exclude \\(\\mathbf{y}_v\\)s identical values, \\(\\mathbf{Y}\\) corresponding values AUC: values \\(\\mathrm{cv}(\\widehat{y}_i, y_i)\\) express AUC sum casewise values solutions equation (\\(\\ref{eq:cw}\\)), can written solutions following system \\(2^{n_v}\\) linear simultaneous equations \\(2n_v\\) unknowns: \\[\\begin{equation} \\label{eq:lin} \\tag{2} (\\mathbf{U} -\\mathbf{Y}) \\mathbf{c}_0 + \\mathbf{Y} \\mathbf{c}_1 = [\\mathbf{U} -\\mathbf{Y}, \\mathbf{Y}] \\begin{bmatrix} \\mathbf{c}_0 \\\\ \\mathbf{c}_1 \\end{bmatrix} = \\mathrm{AUC}(\\mathbf{\\widehat{Y}},\\mathbf{Y}) \\end{equation}\\] \\(\\mathbf{U}_{(2^{n_v} \\times n_v)}\\) matrix 1s conformable \\(\\mathbf{Y}\\); \\(\\mathbf{c}_0 = [\\mathrm{cv}(1,0), c(2,0), ..., \\mathrm{cv}(n_v,0)]^T\\); \\(\\mathbf{c}_1 = [\\mathrm{cv}(1,1), c(2,1), ..., \\mathrm{cv}(n_v,1)]^T\\); \\([\\mathbf{U} -\\mathbf{Y}, \\mathbf{Y}]_{(2^{n_v} \\times 2n_v)}\\) \\(\\begin{bmatrix}\\begin{aligned} \\mathbf{c}_0 \\\\ \\mathbf{c}_1 \\end{aligned} \\end{bmatrix}_{(2n_v \\times 1)}\\) partitioned matrices; \\(\\mathbf{\\widehat{Y}}_{(2^{n_v} \\times n_v)}\\) matrix whose rows consists integers 1 \\(n_v\\). can test whether equation (\\(\\ref{eq:lin}\\)) solution given \\(n_v\\) trying solve least-squares problem, considering whether residuals associated linear model 0, using “design matrix” \\([\\mathbf{U} -\\mathbf{Y}, \\mathbf{Y}]\\) predict “outcome” \\(\\mathrm{AUC}(\\mathbf{\\widehat{Y}},\\mathbf{Y})_{(2^{n_v} \\times 1)}\\): case \\(n_v=3\\), excluding identical \\(y\\)s, solution: , identical \\(y\\)s included, equation consistent: \\(n_v=4\\), solutions either case: Consequently, widely employed AUC measure fit binary regression general used casewise cross-validation criterion.","code":"AUC <- function(y, yhat = seq_along(y)) {   s <- sum(y)   if (s == 0)     return(0)   if (s == length(y))     return(1)   Metrics::auc(y, yhat) } Ymat <- function(n_v, exclude_identical = FALSE) {   stopifnot(n_v > 0 &&               round(n_v) == n_v)    # n_v must be a positive integer   ret <- sapply(0:(2 ^ n_v - 1),                 function(x)                   as.integer(intToBits(x)))[1:n_v,]   ret <- if (is.matrix(ret))     t(ret)   else     matrix(ret)   colnames(ret) <- paste0(\"y\", 1:ncol(ret))   if (exclude_identical)     ret[-c(1, nrow(ret)),]   else     ret } Ymat(3) #>      y1 y2 y3 #> [1,]  0  0  0 #> [2,]  1  0  0 #> [3,]  0  1  0 #> [4,]  1  1  0 #> [5,]  0  0  1 #> [6,]  1  0  1 #> [7,]  0  1  1 #> [8,]  1  1  1 Ymat(3, exclude_identical = TRUE) #>      y1 y2 y3 #> [1,]  1  0  0 #> [2,]  0  1  0 #> [3,]  1  1  0 #> [4,]  0  0  1 #> [5,]  1  0  1 #> [6,]  0  1  1 cbind(Ymat(3), AUC = apply(Ymat(3), 1, AUC)) #>      y1 y2 y3 AUC #> [1,]  0  0  0 0.0 #> [2,]  1  0  0 0.0 #> [3,]  0  1  0 0.5 #> [4,]  1  1  0 0.0 #> [5,]  0  0  1 1.0 #> [6,]  1  0  1 0.5 #> [7,]  0  1  1 1.0 #> [8,]  1  1  1 1.0 resids <- function(n_v,                    exclude_identical = FALSE,                    tol = sqrt(.Machine$double.eps)) {   Y <- Ymat(n_v, exclude_identical = exclude_identical)   AUC <- apply(Y, 1, AUC)   X <- cbind(1 - Y, Y)   opts <- options(warn = -1)   on.exit(options(opts))   fit <- lsfit(X, AUC, intercept = FALSE)   ret <- max(abs(residuals(fit)))   if (ret < tol) {     ret <- 0     solution <- coef(fit)     names(solution) <- paste0(\"c(\", c(1:n_v, 1:n_v), \",\",                               rep(0:1, each = n_v), \")\")     attr(ret, \"solution\") <- zapsmall(solution)   }   ret } resids(3, exclude_identical = TRUE) #> [1] 0 #> attr(,\"solution\") #> c(1,0) c(2,0) c(3,0) c(1,1) c(2,1) c(3,1)  #>    1.0    0.0   -0.5    0.5    0.0    0.0 resids(3, exclude_identical = FALSE) #> [1] 0.125 resids(4, exclude_identical = TRUE) #> [1] 0.083333 resids(4, exclude_identical = FALSE) #> [1] 0.25"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv-selection.html","id":"a-preliminary-example","dir":"Articles","previous_headings":"","what":"A preliminary example","title":"Cross-validating model selection","text":"following example similar spirit one employed Hastie et al. (2009). Suppose randomly generate \\(n = 1000\\) independent observations response variable variable \\(y \\sim N(\\mu = 10, \\sigma^2 = 0)\\), independently sample \\(1000\\) observations \\(p = 100\\) “predictors,” \\(x_1, \\ldots, x_{100}\\), \\(x_j \\sim N(0, 1)\\). response nothing predictors population linear-regression model \\(y_i = \\alpha + \\beta_1 x_{i1} + \\cdots + \\beta_{100} x_{,100} + \\varepsilon_i\\) \\(\\alpha = 10\\) \\(\\beta_j = 0\\). Least-squares provides accurate estimates regression constant \\(\\alpha = 10\\) error variance \\(\\sigma^2 = 1\\) “null model” including regression constant; moreover, omnibus \\(F\\)-test correct null hypothesis \\(\\beta\\)s 0 “full model” 100 \\(x\\)s associated large \\(p\\)-value: Next, using stepAIC() function MASS package (Venables & Ripley, 2002), let us perform forward stepwise regression select “best” model, starting null model, using AIC model-selection criterion (see help page stepAIC() details):1 resulting model 15 predictors, modest \\(R^2 = .044\\), small \\(p\\)-value omnibus \\(F\\)-test (, course, entirely spurious data used select test model). MSE selected model smaller true error variance \\(\\sigma^2 = 1\\), estimated error variance selected model, \\(\\widehat{\\sigma}^2 = 0.973^2 = 0.947\\). cross-validate selected model, also obtain optimistic estimate predictive power (although confidence interval bias-adjusted MSE includes 1): \"function\" method cv() allows us cross-validate whole model-selection procedure, first argument cv() model-selection function capable refitting model fold omitted returning CV criterion. selectStepAIC() function, cv package based stepAIC(), suitable use cv(): arguments cv() : data, data set model fit; seed, optional seed R’s pseudo-random-number generator; cv(), seed isn’t supplied user, seed randomly selected saved; additional arguments required model-selection function, starting working.model argument, direction model selection, scope models considered (model regression constant model 100 predictors). default, cv() performs 10-fold CV, produces estimate MSE model-selection procedure even larger true error variance, \\(\\sigma^2 = 1\\). Also default, number folds 10 fewer, cv() saves details data folds. example, compareFolds() function reveals variables retained model-selection process several folds quite different:","code":"set.seed(24361) # for reproducibility D <- data.frame(y = rnorm(1000, mean = 10),                 X = matrix(rnorm(1000 * 100), 1000, 100)) head(D[, 1:6]) #>         y      X.1      X.2      X.3       X.4       X.5 #> 1 10.0316 -1.23886 -0.26487 -0.03539 -2.576973  0.811048 #> 2  9.6650  0.12287 -0.17744  0.37290 -0.935138  0.628673 #> 3 10.0232 -0.95052 -0.73487 -1.05978  0.882944  0.023918 #> 4  8.9910  1.13571  0.32411  0.11037  1.376303 -0.422114 #> 5  9.0712  1.49474  1.87538  0.10575  0.292140 -0.184568 #> 6 11.3493 -0.18453 -0.78037 -1.23804 -0.010949  0.691034 m.full <- lm(y ~ ., data = D) m.null <- lm(y ~ 1, data = D) anova(m.null, m.full) #> Analysis of Variance Table #>  #> Model 1: y ~ 1 #> Model 2: y ~ X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 +  #>     X.11 + X.12 + X.13 + X.14 + X.15 + X.16 + X.17 + X.18 + X.19 +  #>     X.20 + X.21 + X.22 + X.23 + X.24 + X.25 + X.26 + X.27 + X.28 +  #>     X.29 + X.30 + X.31 + X.32 + X.33 + X.34 + X.35 + X.36 + X.37 +  #>     X.38 + X.39 + X.40 + X.41 + X.42 + X.43 + X.44 + X.45 + X.46 +  #>     X.47 + X.48 + X.49 + X.50 + X.51 + X.52 + X.53 + X.54 + X.55 +  #>     X.56 + X.57 + X.58 + X.59 + X.60 + X.61 + X.62 + X.63 + X.64 +  #>     X.65 + X.66 + X.67 + X.68 + X.69 + X.70 + X.71 + X.72 + X.73 +  #>     X.74 + X.75 + X.76 + X.77 + X.78 + X.79 + X.80 + X.81 + X.82 +  #>     X.83 + X.84 + X.85 + X.86 + X.87 + X.88 + X.89 + X.90 + X.91 +  #>     X.92 + X.93 + X.94 + X.95 + X.96 + X.97 + X.98 + X.99 + X.100 #>   Res.Df RSS  Df Sum of Sq    F Pr(>F) #> 1    999 974                           #> 2    899 888 100      85.2 0.86   0.82 summary(m.null) #>  #> Call: #> lm(formula = y ~ 1, data = D) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -3.458 -0.681  0.019  0.636  2.935  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   9.9370     0.0312     318   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.987 on 999 degrees of freedom library(\"MASS\")  # for stepAIC() m.select <- stepAIC(   m.null,   direction = \"forward\",   trace = FALSE,   scope = list(lower =  ~ 1, upper = formula(m.full)) ) summary(m.select) #>  #> Call: #> lm(formula = y ~ X.99 + X.90 + X.87 + X.40 + X.65 + X.91 + X.53 +  #>     X.45 + X.31 + X.56 + X.61 + X.60 + X.46 + X.35 + X.92, data = D) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -3.262 -0.645  0.024  0.641  3.118  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   9.9372     0.0310  320.80   <2e-16 *** #> X.99         -0.0910     0.0308   -2.95   0.0032 **  #> X.90         -0.0820     0.0314   -2.62   0.0090 **  #> X.87         -0.0694     0.0311   -2.24   0.0256 *   #> X.40         -0.0476     0.0308   -1.55   0.1221     #> X.65         -0.0552     0.0315   -1.76   0.0795 .   #> X.91          0.0524     0.0308    1.70   0.0894 .   #> X.53         -0.0492     0.0305   -1.61   0.1067     #> X.45          0.0554     0.0318    1.74   0.0818 .   #> X.31          0.0452     0.0311    1.46   0.1457     #> X.56          0.0543     0.0327    1.66   0.0972 .   #> X.61         -0.0508     0.0317   -1.60   0.1091     #> X.60         -0.0513     0.0319   -1.61   0.1083     #> X.46          0.0516     0.0327    1.58   0.1153     #> X.35          0.0470     0.0315    1.49   0.1358     #> X.92          0.0443     0.0310    1.43   0.1533     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.973 on 984 degrees of freedom #> Multiple R-squared:  0.0442, Adjusted R-squared:  0.0296  #> F-statistic: 3.03 on 15 and 984 DF,  p-value: 8.34e-05 library(\"cv\") #> Loading required package: doParallel #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel mse(D$y, fitted(m.select)) #> [1] 0.93063 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" library(\"cv\")  cv(m.select, seed = 2529) #> R RNG seed set to 2529 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 0.95937 #> bias-adjusted cross-validation criterion = 0.95785 #> 95% CI for bias-adjusted CV criterion = (0.87661, 1.0391) #> full-sample criterion = 0.93063 cv.select <- cv(   selectStepAIC,   data = D,   seed = 3791,   working.model = m.null,   direction = \"forward\",   scope = list(lower =  ~ 1, upper = formula(m.full)) ) #> R RNG seed set to 3791 cv.select #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 1.0687 #> bias-adjusted cross-validation criterion = 1.0612 #> 95% CI for bias-adjusted CV criterion = (0.97172, 1.1506) #> full-sample criterion = 0.93063 compareFolds(cv.select) #> CV criterion by folds: #>  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10  #> 1.26782 1.12837 1.04682 1.31007 1.06899 0.87916 0.88380 0.95026 1.21070 0.94130  #>  #> Coefficients by folds: #>         (Intercept)    X.87    X.90    X.99    X.91    X.54    X.53    X.56 #> Fold 1       9.9187 -0.0615 -0.0994 -0.0942  0.0512  0.0516                 #> Fold 2       9.9451 -0.0745 -0.0899 -0.0614          0.0587          0.0673 #> Fold 3       9.9423 -0.0783 -0.0718 -0.0987  0.0601                  0.0512 #> Fold 4       9.9410 -0.0860 -0.0831 -0.0867  0.0570         -0.0508         #> Fold 5       9.9421 -0.0659 -0.0849 -0.1004  0.0701  0.0511 -0.0487  0.0537 #> Fold 6       9.9633 -0.0733 -0.0874 -0.0960  0.0555  0.0629 -0.0478         #> Fold 7       9.9279 -0.0618 -0.0960 -0.0838  0.0533         -0.0464         #> Fold 8       9.9453 -0.0610 -0.0811 -0.0818          0.0497 -0.0612  0.0560 #> Fold 9       9.9173 -0.0663 -0.0894 -0.1100  0.0504  0.0524          0.0747 #> Fold 10      9.9449 -0.0745 -0.0906 -0.0891  0.0535  0.0482 -0.0583  0.0642 #>            X.40    X.45    X.65    X.68    X.92    X.15    X.26    X.46    X.60 #> Fold 1                  -0.0590                 -0.0456  0.0658  0.0608         #> Fold 2                                   0.0607          0.0487                 #> Fold 3  -0.0496         -0.0664          0.0494                                 #> Fold 4  -0.0597  0.0579 -0.0531          0.0519 -0.0566                 -0.0519 #> Fold 5                           0.0587                          0.0527 -0.0603 #> Fold 6  -0.0596  0.0552          0.0474                                         #> Fold 7           0.0572          0.0595                                         #> Fold 8           0.0547 -0.0617  0.0453  0.0493 -0.0613  0.0591  0.0703 -0.0588 #> Fold 9  -0.0552  0.0573 -0.0635  0.0492         -0.0513  0.0484         -0.0507 #> Fold 10 -0.0558                          0.0529                  0.0710         #>            X.61     X.8    X.28    X.29    X.31    X.35    X.70    X.89    X.17 #> Fold 1  -0.0490          0.0616 -0.0537                  0.0638                 #> Fold 2           0.0671                  0.0568                  0.0523         #> Fold 3  -0.0631          0.0616                                                 #> Fold 4           0.0659         -0.0549          0.0527                  0.0527 #> Fold 5           0.0425                  0.0672  0.0613          0.0493         #> Fold 6           0.0559         -0.0629  0.0498          0.0487                 #> Fold 7                                                           0.0611  0.0472 #> Fold 8  -0.0719                                          0.0586                 #> Fold 9                   0.0525                                                 #> Fold 10 -0.0580                                  0.0603                         #>            X.25     X.4    X.64    X.81    X.97    X.11     X.2    X.33    X.47 #> Fold 1                                   0.0604          0.0575                 #> Fold 2   0.0478          0.0532  0.0518                                         #> Fold 3                           0.0574                          0.0473         #> Fold 4                   0.0628                                                 #> Fold 5   0.0518                                                                 #> Fold 6                                           0.0521                         #> Fold 7           0.0550                                                         #> Fold 8                                                                          #> Fold 9                                   0.0556                          0.0447 #> Fold 10          0.0516                                                         #>             X.6    X.72    X.73    X.77    X.79 X.88 #> Fold 1   0.0476                                      #> Fold 2                   0.0514                      #> Fold 3                                               #> Fold 4                                  -0.0473      #> Fold 5           0.0586                         0.07 #> Fold 6                          -0.0489              #> Fold 7                                               #> Fold 8                                               #> Fold 9                                               #> Fold 10"},{"path":"https://gmonette.github.io/cv/articles/cv-selection.html","id":"polynomial-regression-for-the-auto-data-revisited-recursive-cross-validation","dir":"Articles","previous_headings":"","what":"Polynomial regression for the Auto data revisited: recursive cross-validation","title":"Cross-validating model selection","text":"Cross-validated 10-fold LOO MSE function polynomial degree, \\(p\\) (repeated) select 7th degree polynomial model, intending use prediction, CV estimate MSE model optimistic. One solution cross-validate process using CV select “best” model—, apply CV CV recursively. function selectModelList(), suitable use cv(), implements idea. Applying selectModelList() Auto polynomial-regression models, using 10-fold CV, obtain: expected, recursive CV produces larger estimate MSE selected 7th degree polynomial model CV applied directly model. can equivalently call cv() list models first argument set recursive=TRUE:","code":"recursiveCV.auto <- cv(   selectModelList,   Auto,   working.model = models(m.1, m.2, m.3, m.4, m.5,                          m.6, m.7, m.8, m.9, m.10),   save.model = TRUE,   seed = 2120 ) #> R RNG seed set to 2120 recursiveCV.auto #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 19.105 #> bias-adjusted cross-validation criterion = 19.68 #> full-sample criterion = 18.746 recursiveCV.auto$selected.model #>  #> Call: #> lm(formula = mpg ~ poly(horsepower, 7), data = Auto) #>  #> Coefficients: #>          (Intercept)  poly(horsepower, 7)1  poly(horsepower, 7)2   #>                23.45               -120.14                 44.09   #> poly(horsepower, 7)3  poly(horsepower, 7)4  poly(horsepower, 7)5   #>                -3.95                 -5.19                 13.27   #> poly(horsepower, 7)6  poly(horsepower, 7)7   #>                -8.55                  7.98 cv(m.7, seed = 2120) # same seed for same folds #> R RNG seed set to 2120 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 18.898 #> bias-adjusted cross-validation criterion = 18.854 #> full-sample criterion = 18.078 recursiveCV.auto.alt <- cv(   models(m.1, m.2, m.3, m.4, m.5,          m.6, m.7, m.8, m.9, m.10),   data = Auto,   seed = 2120,   recursive = TRUE,   save.model = TRUE ) #> R RNG seed set to 2120 all.equal(recursiveCV.auto, recursiveCV.auto.alt) #> [1] \"Component \\\"criterion\\\": 1 string mismatch\""},{"path":"https://gmonette.github.io/cv/articles/cv-selection.html","id":"mrozs-logistic-regression-revisited","dir":"Articles","previous_headings":"","what":"Mroz’s logistic regression revisited","title":"Cross-validating model selection","text":"Next, let’s apply model selection Mroz’s logistic regression married women’s labor-force participation, also discussed introductory vignette cross-validating regression models. First, recall logistic regression model fit Mroz data: Applying stepwise model selection Mroz’s logistic regression, using BIC model-selection criterion (via argument k=log(nrow(Mroz)) stepAIC()) selects 5 7 original predictors: Bayes rule applied selected model misclassifies 32% cases Mroz data. Cross-validating selected model produces similar, slightly larger, estimate misclassification, 33%: estimate predictive performance optimistic? proceed apply model-selection procedure cross-validation, producing less result: Setting AIC=FALSE call cv() uses BIC rather AIC model-selection criterion. turns , exactly predictors selected 10 folds omitted, several coefficient estimates similar, show using compareFolds(): example, therefore, appear obtain realistic estimate model performance directly selected model, little added uncertainty induced model selection.","code":"data(\"Mroz\", package = \"carData\") m.mroz <- glm(lfp ~ ., data = Mroz, family = binomial) summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4 m.mroz.sel <- stepAIC(m.mroz, k = log(nrow(Mroz)),                       trace = FALSE) summary(m.mroz.sel) #>  #> Call: #> glm(formula = lfp ~ k5 + age + wc + lwg + inc, family = binomial,  #>     data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)   2.9019     0.5429    5.35  9.0e-08 *** #> k5           -1.4318     0.1932   -7.41  1.3e-13 *** #> age          -0.0585     0.0114   -5.13  2.9e-07 *** #> wcyes         0.8724     0.2064    4.23  2.4e-05 *** #> lwg           0.6157     0.1501    4.10  4.1e-05 *** #> inc          -0.0337     0.0078   -4.32  1.6e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  906.46  on 747  degrees of freedom #> AIC: 918.5 #>  #> Number of Fisher Scoring iterations: 3 BayesRule(Mroz$lfp == \"yes\",           predict(m.mroz.sel, type = \"response\")) #> [1] 0.31873 #> attr(,\"casewise loss\") #> [1] \"y != round(yhat)\" cv(m.mroz.sel, criterion = BayesRule, seed = 345266) #> R RNG seed set to 345266 #> 10-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.33068 #> bias-adjusted cross-validation criterion = 0.33332 #> 95% CI for bias-adjusted CV criterion = (0.2997, 0.36695) #> full-sample criterion = 0.31873 m.mroz.sel.cv <- cv(   selectStepAIC,   Mroz,   seed = 6681,   criterion = BayesRule,   working.model = m.mroz,   AIC = FALSE ) #> R RNG seed set to 6681 m.mroz.sel.cv #> 10-Fold Cross Validation #> criterion: BayesRule #> cross-validation criterion = 0.33068 #> bias-adjusted cross-validation criterion = 0.33452 #> 95% CI for bias-adjusted CV criterion = (0.3009, 0.36815) #> full-sample criterion = 0.31873 compareFolds(m.mroz.sel.cv) #> CV criterion by folds: #>  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10  #> 0.27632 0.40789 0.34211 0.36000 0.28000 0.37333 0.32000 0.29333 0.33333 0.32000  #>  #> Coefficients by folds: #>         (Intercept)     age     inc      k5     lwg wcyes #> Fold 1       2.5014 -0.0454 -0.0388 -1.3613  0.5653  0.85 #> Fold 2       3.0789 -0.0659 -0.0306 -1.5335  0.6923  0.79 #> Fold 3       3.0141 -0.0595 -0.0305 -1.3994  0.5428  0.86 #> Fold 4       2.7251 -0.0543 -0.0354 -1.4474  0.6298  1.09 #> Fold 5       2.7617 -0.0566 -0.0320 -1.4752  0.6324  0.74 #> Fold 6       3.0234 -0.0621 -0.0348 -1.4537  0.6618  0.94 #> Fold 7       2.9615 -0.0600 -0.0351 -1.4127  0.5835  0.97 #> Fold 8       2.9598 -0.0603 -0.0329 -1.3865  0.6210  0.69 #> Fold 9       3.2481 -0.0650 -0.0381 -1.4138  0.6093  0.94 #> Fold 10      2.7724 -0.0569 -0.0295 -1.4503  0.6347  0.85"},{"path":"https://gmonette.github.io/cv/articles/cv-selection.html","id":"cross-validating-choice-of-transformations-in-regression","dir":"Articles","previous_headings":"","what":"Cross-validating choice of transformations in regression","title":"Cross-validating model selection","text":"cv package also provides cv() procedure, selectTrans(), choosing transformations predictors response regression. background: Weisberg (2014, sec. 8.2) explains, technical advantages (numeric) predictors linear regression analysis linearly related. predictors aren’t linearly related, relationships can often straightened power transformations. Transformations can selected graphical examination data, analytic methods. relationships predictors linearized, can advantageous similarly transform response variable towards normality. Selecting transformations analytically raises possibility automating process, required cross-validation. One , principle, apply graphical methods select transformations fold, data analyst couldn’t forget choices made previous folds, process wouldn’t really applied independently folds. illustrate, adapt example appearing several places Fox & Weisberg (2019) (example Chapter 3 transforming data), using data prestige characteristics 102 Canadian occupations circa 1970. data Prestige data frame carData package: variables Prestige data set : education: average years education incumbents occupation, 1971 Canadian Census. income: average dollars annual income occupation, Census. women: percentage occupational incumbents women, also Census. prestige: average prestige rating occupation 0–100 “thermometer” scale, Canadian social survey conducted around time. type, type occupation, census, Census occupational code, used example. object regression analysis Prestige data (original purpose) predict occupational prestige variables data set. scatterplot matrix (using scatterplotMatrix() function car package) numeric variables data reveals distributions income women positively skewed, relationships among three predictors, predictors response (.e., prestige), nonlinear: Scatterplot matrix Prestige data. powerTransform() function car package transforms variables towards multivariate normality generalization Box Cox’s maximum-likelihood-like approach (Box & Cox, 1964). Several “families” power transformations can used, including original Box-Cox family, simple powers (roots), two adaptations Box-Cox family data may include negative values zeros: Box-Cox--negatives family Yeo-Johnson family; see Weisberg (2014, Chapter 8), Fox & Weisberg (2019, Chapter 3) details. women zero values, use Yeo-Johnson family: thus evidence desirability transforming income (\\(1/3\\) power) women (\\(0.16\\) power—close “0” power, .e., log transformation), education. Applying “rounded” power transformations makes predictors better-behaved: Scatterplot matrix Prestige data predictors transformed. Comparing MSE regressions original transformed predictors shows advantage latter: Similarly, component+residual plots two regressions, produced crPlots() function car package, suggest partial relationship prestige income nearly linear transformed data, transformation women fails capture appears slight quadratic partial relationship; partial relationship prestige education close linear regressions: Component+residual plots Prestige regression original predictors. Component+residual plots Prestige regression transformed predictors. transformed predictors towards multinormality, now consider whether ’s evidence transforming response (using powerTransform() Box Cox’s original method), discover ’s : selectTrans() function cv package automates process selecting predictor response transformations. function takes data set “working” model arguments, along candidate predictors response transformation, transformation family employ. predictors argument missing response transformed, response argument missing, supplied predictors transformed. default family transforming predictors \"bcPower\"—original Box-Cox family—default family.y transforming response; specify family=\"yjPower zeros women. selectTrans() returns result applying lack--fit criterion model selected transformation applied, default criterion=mse: selectTrans() also takes optional indices argument, making suitable computations subset data (.e., CV fold), hence use cv() (see ?selectTrans details): results suggest predictive power transformed regression reliably greater untransformed regression (though case, cross-validated MSE considerably higher MSE computed whole data). Examining selected transformations fold reveals predictor education response prestige never transformed; \\(1/3\\) power selected income folds; transformation selected women varies narrowly across folds \\(0\\)th power (.e., log) \\(1/3\\) power.","code":"data(\"Prestige\", package = \"carData\") head(Prestige) #>                     education income women prestige census type #> gov.administrators      13.11  12351 11.16     68.8   1113 prof #> general.managers        12.26  25879  4.02     69.1   1130 prof #> accountants             12.77   9271 15.70     63.4   1171 prof #> purchasing.officers     11.42   8865  9.11     56.8   1175 prof #> chemists                14.62   8403 11.68     73.5   2111 prof #> physicists              15.64  11030  5.13     77.6   2113 prof summary(Prestige) #>    education         income          women          prestige        census     #>  Min.   : 6.38   Min.   :  611   Min.   : 0.00   Min.   :14.8   Min.   :1113   #>  1st Qu.: 8.45   1st Qu.: 4106   1st Qu.: 3.59   1st Qu.:35.2   1st Qu.:3120   #>  Median :10.54   Median : 5930   Median :13.60   Median :43.6   Median :5135   #>  Mean   :10.74   Mean   : 6798   Mean   :28.98   Mean   :46.8   Mean   :5402   #>  3rd Qu.:12.65   3rd Qu.: 8187   3rd Qu.:52.20   3rd Qu.:59.3   3rd Qu.:8312   #>  Max.   :15.97   Max.   :25879   Max.   :97.51   Max.   :87.2   Max.   :9517   #>    type    #>  bc  :44   #>  prof:31   #>  wc  :23   #>  NA's: 4   #>            #> library(\"car\") #> Loading required package: carData scatterplotMatrix(   ~ prestige + income + education + women,   data = Prestige,   smooth = list(spread = FALSE) ) trans <- powerTransform(cbind(income, education, women) ~ 1,                         data = Prestige,                         family = \"yjPower\") summary(trans) #> yjPower Transformations to Multinormality  #>           Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd #> income       0.2678        0.33       0.1051       0.4304 #> education    0.5162        1.00      -0.2822       1.3145 #> women        0.1630        0.16       0.0112       0.3149 #>  #>  Likelihood ratio test that all transformation parameters are equal to 0 #>                              LRT df    pval #> LR test, lambda = (0 0 0) 15.739  3 0.00128 P <- Prestige[, c(\"prestige\", \"income\", \"education\", \"women\")] (lambdas <- trans$roundlam) #>    income education     women  #>   0.33000   1.00000   0.16302 names(lambdas) <- c(\"income\", \"education\", \"women\") for (var in c(\"income\", \"education\", \"women\")) {   P[, var] <- yjPower(P[, var], lambda = lambdas[var]) } summary(P) #>     prestige        income       education         women      #>  Min.   :14.8   Min.   :22.2   Min.   : 6.38   Min.   :0.00   #>  1st Qu.:35.2   1st Qu.:44.2   1st Qu.: 8.45   1st Qu.:1.73   #>  Median :43.6   Median :50.3   Median :10.54   Median :3.36   #>  Mean   :46.8   Mean   :50.8   Mean   :10.74   Mean   :3.50   #>  3rd Qu.:59.3   3rd Qu.:56.2   3rd Qu.:12.65   3rd Qu.:5.59   #>  Max.   :87.2   Max.   :83.6   Max.   :15.97   Max.   :6.83 scatterplotMatrix(   ~ prestige + income + education + women,   data = P,   smooth = list(spread = FALSE) ) m.pres <- lm(prestige ~ income + education + women, data = Prestige) m.pres.trans <- lm(prestige ~ income + education + women, data = P) mse(Prestige$prestige, fitted(m.pres)) #> [1] 59.153 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" mse(P$prestige, fitted(m.pres.trans)) #> [1] 50.6 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" crPlots(m.pres) crPlots(m.pres.trans) summary(powerTransform(m.pres.trans)) #> bcPower Transformation to Normality  #>    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd #> Y1    1.0194           1       0.6773       1.3615 #>  #> Likelihood ratio test that transformation parameter is equal to 0 #>  (log transformation) #>                          LRT df     pval #> LR test, lambda = (0) 32.217  1 1.38e-08 #>  #> Likelihood ratio test that no transformation is needed #>                            LRT df  pval #> LR test, lambda = (1) 0.012384  1 0.911 selectTrans(   data = Prestige,   model = m.pres,   predictors = c(\"income\", \"education\", \"women\"),   response = \"prestige\",   family = \"yjPower\" ) #> $criterion #> [1] 50.6 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" #>  #> $model #> NULL cvs <- cv(   selectTrans,   data = Prestige,   working.model = m.pres,   seed = 1463,   predictors = c(\"income\", \"education\", \"women\"),   response = \"prestige\",   family = \"yjPower\" ) #> R RNG seed set to 1463 cvs #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 54.487 #> bias-adjusted cross-validation criterion = 54.308 #> full-sample criterion = 50.6 cv(m.pres, seed = 1463) # untransformed model with same folds #> R RNG seed set to 1463 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 63.293 #> bias-adjusted cross-validation criterion = 63.073 #> full-sample criterion = 59.153 compareFolds(cvs) #> CV criterion by folds: #>  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10  #>  63.453  79.257  20.634  94.569  19.902  55.821  26.555  75.389  55.702  50.215  #>  #> Coefficients by folds: #>         lam.education lam.income lam.women lambda #> Fold 1          1.000      0.330     0.330      1 #> Fold 2          1.000      0.330     0.169      1 #> Fold 3          1.000      0.330     0.330      1 #> Fold 4          1.000      0.330     0.330      1 #> Fold 5          1.000      0.330     0.000      1 #> Fold 6          1.000      0.330     0.330      1 #> Fold 7          1.000      0.330     0.330      1 #> Fold 8          1.000      0.330     0.000      1 #> Fold 9          1.000      0.330     0.000      1 #> Fold 10         1.000      0.330     0.000      1"},{"path":"https://gmonette.github.io/cv/articles/cv-selection.html","id":"selecting-both-transformations-and-predictorsvenables","dir":"Articles","previous_headings":"","what":"Selecting both transformations and predictors2","title":"Cross-validating model selection","text":"mentioned, Hastie et al. (2009, sec. 7.10.2: “Wrong Right Way Cross-validation”) explain honest cross-validation take account model specification selection. Statistical modeling least partly craft, one imagine applying craft successive partial data sets, fold removed. resulting procedure tedious, though possibly worth effort, also difficult realize practice: , can hardly erase memory statistical modeling choices analyzing partial data sets. Alternatively, ’re able automate process model selection, can realistically apply CV mechanically. ’s preceding two sections, first predictor selection selection transformations regression. section, consider case select variable transformations proceed select predictors. ’s insufficient apply steps sequentially, first, example, using cv() selectTrans() selectStepAIC(); rather apply whole model-selection procedure fold omitted. selectTransAndStepAIC() function, also supplied cv package, exactly . illustrate process, return Auto data set: previously used Auto preliminary example employed CV inform selection order polynomial regression mpg horsepower. , consider generally problem predicting mpg variables Auto data. begin bit data management, examine pairwise relationships among numeric variables data set: Scatterplot matrix numeric variables Auto data comment proceed: origin clearly categorical converting factor natural, imagine treating cylinders year numeric predictors. , however, 5 distinct values cylinders (ranging 3 8), cars 3 5 cylinders rare. none cars 7 cylinders. similarly 13 distinct years 1970 1982 data, relationship mpg year difficult characterize.3 ’s apparent variables positively skewed many pairwise relationships among nonlinear. begin “working model” specifies linear partial relationships response numeric predictors: Component+residual plots working model fit Auto data component+residual plots, created crPlots() function previously loaded car package, clearly reveal inadequacy model. proceed transform numeric predictors towards multi-normality: apply (rounded) transformations—, turns , logs—data re-estimate model: Finally, perform Box-Cox regression transform response (also obtaining log transformation): transformed numeric variables much better-behaved: Scatterplot matrix transformed numeric variables Auto data partial relationships model fit transformed data much nearly linear: Component+residual plots model fit transformed Auto data transformed numeric predictors response, proceed use stepAIC() function MASS package perform predictor selection, employing BIC model-selection criterion (setting k argument stepAIC() \\(\\log(n)\\)): selected model includes three numeric predictors, horsepower, weight, acceleration, along factors year origin. can calculate MSE model, expect result optimistic used whole data help specify model considerably smaller MSE original working model: perhaps subtle point compute MSE selected model original mpg response scale rather log scale, make selected model comparable working model. ’s slightly uncomfortable given skewed distribution mpg. alternative use median absolute error instead mean-squared error, employing medAbsErr() function cv package: Now let’s use cv() selectTransAndStepAIC() automate cross-validate whole model-specification process: , selectTrans(), predictors response arguments specify candidate variables transformation, AIC=FALSE uses BIC model selection. starting model, m.auto, working model fit Auto data. CV criterion isn’t bias-adjusted median absolute error isn’t mean casewise error components. noteworthy points: selectTransStepAIC() automatically computes CV cost criteria, median absolute error, untransformed response scale. estimate median absolute error obtain cross-validating whole model-specification process little larger median absolute error computed model fit Auto data separately selecting transformations predictors response selecting predictors whole data set. look transformations predictors selected 10 folds omitted (.e., output compareFolds()), see little uncertainty choosing variable transformations (lam.*s \\(x\\)s lambda \\(y\\) output), considerably uncertainty subsequently selecting predictors: horsepower, weight, year always included among selected predictors; acceleration displacement included respectively 4 3 10 selected models; cylinders origin included 1 10 models. Recall selected predictors full data, obtained model horsepower, weight, acceleration, year, origin.","code":"summary(Auto) #>       mpg         cylinders     displacement   horsepower        weight     #>  Min.   : 9.0   Min.   :3.00   Min.   : 68   Min.   : 46.0   Min.   :1613   #>  1st Qu.:17.0   1st Qu.:4.00   1st Qu.:105   1st Qu.: 75.0   1st Qu.:2225   #>  Median :22.8   Median :4.00   Median :151   Median : 93.5   Median :2804   #>  Mean   :23.4   Mean   :5.47   Mean   :194   Mean   :104.5   Mean   :2978   #>  3rd Qu.:29.0   3rd Qu.:8.00   3rd Qu.:276   3rd Qu.:126.0   3rd Qu.:3615   #>  Max.   :46.6   Max.   :8.00   Max.   :455   Max.   :230.0   Max.   :5140   #>                                                                             #>   acceleration       year        origin                     name     #>  Min.   : 8.0   Min.   :70   Min.   :1.00   amc matador       :  5   #>  1st Qu.:13.8   1st Qu.:73   1st Qu.:1.00   ford pinto        :  5   #>  Median :15.5   Median :76   Median :1.00   toyota corolla    :  5   #>  Mean   :15.5   Mean   :76   Mean   :1.58   amc gremlin       :  4   #>  3rd Qu.:17.0   3rd Qu.:79   3rd Qu.:2.00   amc hornet        :  4   #>  Max.   :24.8   Max.   :82   Max.   :3.00   chevrolet chevette:  4   #>                                             (Other)           :365 xtabs( ~ year, data = Auto) #> year #> 70 71 72 73 74 75 76 77 78 79 80 81 82  #> 29 27 28 40 26 30 34 28 36 29 27 28 30 xtabs( ~ origin, data = Auto) #> origin #>   1   2   3  #> 245  68  79 xtabs( ~ cylinders, data = Auto) #> cylinders #>   3   4   5   6   8  #>   4 199   3  83 103 Auto$cylinders <- factor(Auto$cylinders,                          labels = c(\"3.4\", \"3.4\", \"5.6\", \"5.6\", \"8\")) Auto$year <- as.factor(Auto$year) Auto$origin <- factor(Auto$origin,                       labels = c(\"America\", \"Europe\", \"Japan\")) rownames(Auto) <- make.names(Auto$name, unique = TRUE) Auto$name <- NULL  scatterplotMatrix(   ~ mpg + displacement + horsepower + weight + acceleration,   smooth = list(spread = FALSE),   data = Auto ) m.auto <- lm(mpg ~ ., data = Auto) summary(m.auto) #>  #> Call: #> lm(formula = mpg ~ ., data = Auto) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -9.006 -1.745 -0.092  1.525 10.950  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept)  37.034132   1.969393   18.80  < 2e-16 *** #> cylinders5.6 -2.602941   0.655200   -3.97  8.5e-05 *** #> cylinders8   -0.582458   1.171452   -0.50  0.61934     #> displacement  0.017425   0.006734    2.59  0.01004 *   #> horsepower   -0.041353   0.013379   -3.09  0.00215 **  #> weight       -0.005548   0.000632   -8.77  < 2e-16 *** #> acceleration  0.061527   0.088313    0.70  0.48643     #> year71        0.968058   0.837390    1.16  0.24841     #> year72       -0.601435   0.825115   -0.73  0.46652     #> year73       -0.687689   0.740272   -0.93  0.35351     #> year74        1.375576   0.876500    1.57  0.11741     #> year75        0.929929   0.859072    1.08  0.27974     #> year76        1.559893   0.822505    1.90  0.05867 .   #> year77        2.909416   0.841729    3.46  0.00061 *** #> year78        3.175198   0.798940    3.97  8.5e-05 *** #> year79        5.019299   0.845759    5.93  6.8e-09 *** #> year80        9.099763   0.897293   10.14  < 2e-16 *** #> year81        6.688660   0.885218    7.56  3.3e-13 *** #> year82        8.071125   0.870668    9.27  < 2e-16 *** #> originEurope  2.046664   0.517124    3.96  9.1e-05 *** #> originJapan   2.144887   0.507717    4.22  3.0e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 2.92 on 371 degrees of freedom #> Multiple R-squared:  0.867,  Adjusted R-squared:  0.86  #> F-statistic:  121 on 20 and 371 DF,  p-value: <2e-16 Anova(m.auto) #> Anova Table (Type II tests) #>  #> Response: mpg #>              Sum Sq  Df F value  Pr(>F)     #> cylinders       292   2   17.09 7.9e-08 *** #> displacement     57   1    6.70  0.0100 *   #> horsepower       82   1    9.55  0.0021 **  #> weight          658   1   76.98 < 2e-16 *** #> acceleration      4   1    0.49  0.4864     #> year           3017  12   29.40 < 2e-16 *** #> origin          190   2   11.13 2.0e-05 *** #> Residuals      3173 371                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 crPlots(m.auto) num.predictors <-   c(\"displacement\", \"horsepower\", \"weight\", \"acceleration\") tr.x <- powerTransform(Auto[, num.predictors]) summary(tr.x) #> bcPower Transformations to Multinormality  #>              Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd #> displacement   -0.0509           0      -0.2082       0.1065 #> horsepower     -0.1249           0      -0.2693       0.0194 #> weight         -0.0870           0      -0.2948       0.1208 #> acceleration    0.3061           0      -0.0255       0.6376 #>  #> Likelihood ratio test that transformation parameters are equal to 0 #>  (all log transformations) #>                                LRT df  pval #> LR test, lambda = (0 0 0 0) 4.8729  4 0.301 #>  #> Likelihood ratio test that no transformations are needed #>                                LRT df   pval #> LR test, lambda = (1 1 1 1) 390.08  4 <2e-16 A <- Auto powers <- tr.x$roundlam for (pred in num.predictors) {   A[, pred] <- bcPower(A[, pred], lambda = powers[pred]) } head(A) #>                           mpg cylinders displacement horsepower weight #> chevrolet.chevelle.malibu  18         8       5.7268     4.8675 8.1617 #> buick.skylark.320          15         8       5.8579     5.1059 8.2142 #> plymouth.satellite         18         8       5.7621     5.0106 8.1421 #> amc.rebel.sst              16         8       5.7170     5.0106 8.1412 #> ford.torino                17         8       5.7104     4.9416 8.1458 #> ford.galaxie.500           15         8       6.0615     5.2883 8.3759 #>                           acceleration year  origin #> chevrolet.chevelle.malibu       2.4849   70 America #> buick.skylark.320               2.4423   70 America #> plymouth.satellite              2.3979   70 America #> amc.rebel.sst                   2.4849   70 America #> ford.torino                     2.3514   70 America #> ford.galaxie.500                2.3026   70 America m <- update(m.auto, data = A) summary(powerTransform(m)) #> bcPower Transformation to Normality  #>    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd #> Y1    0.0024           0      -0.1607       0.1654 #>  #> Likelihood ratio test that transformation parameter is equal to 0 #>  (log transformation) #>                              LRT df  pval #> LR test, lambda = (0) 0.00080154  1 0.977 #>  #> Likelihood ratio test that no transformation is needed #>                          LRT df   pval #> LR test, lambda = (1) 124.13  1 <2e-16 m <- update(m, log(mpg) ~ .) summary(m) #>  #> Call: #> lm(formula = log(mpg) ~ cylinders + displacement + horsepower +  #>     weight + acceleration + year + origin, data = A) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -0.3341 -0.0577  0.0041  0.0607  0.3808  #>  #> Coefficients: #>              Estimate Std. Error t value Pr(>|t|)     #> (Intercept)    8.8965     0.3582   24.84  < 2e-16 *** #> cylinders5.6  -0.0636     0.0257   -2.47    0.014 *   #> cylinders8    -0.0769     0.0390   -1.97    0.049 *   #> displacement   0.0280     0.0515    0.54    0.587     #> horsepower    -0.2901     0.0563   -5.15  4.2e-07 *** #> weight        -0.5427     0.0819   -6.62  1.2e-10 *** #> acceleration  -0.1421     0.0563   -2.52    0.012 *   #> year71         0.0250     0.0289    0.87    0.387     #> year72        -0.0168     0.0289   -0.58    0.562     #> year73        -0.0426     0.0260   -1.64    0.103     #> year74         0.0493     0.0304    1.62    0.106     #> year75         0.0472     0.0296    1.59    0.112     #> year76         0.0709     0.0284    2.49    0.013 *   #> year77         0.1324     0.0293    4.52  8.2e-06 *** #> year78         0.1447     0.0278    5.21  3.1e-07 *** #> year79         0.2335     0.0292    7.99  1.7e-14 *** #> year80         0.3238     0.0317   10.22  < 2e-16 *** #> year81         0.2565     0.0309    8.29  2.1e-15 *** #> year82         0.3076     0.0304   10.13  < 2e-16 *** #> originEurope   0.0492     0.0195    2.52    0.012 *   #> originJapan    0.0441     0.0195    2.26    0.024 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.104 on 371 degrees of freedom #> Multiple R-squared:  0.911,  Adjusted R-squared:  0.906  #> F-statistic:  189 on 20 and 371 DF,  p-value: <2e-16 Anova(m) #> Anova Table (Type II tests) #>  #> Response: log(mpg) #>              Sum Sq  Df F value  Pr(>F)     #> cylinders      0.07   2    3.05   0.048 *   #> displacement   0.00   1    0.30   0.587     #> horsepower     0.29   1   26.54 4.2e-07 *** #> weight         0.48   1   43.88 1.2e-10 *** #> acceleration   0.07   1    6.37   0.012 *   #> year           4.45  12   34.13 < 2e-16 *** #> origin         0.08   2    3.71   0.025 *   #> Residuals      4.03 371                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 scatterplotMatrix(   ~ log(mpg) + displacement + horsepower + weight   + acceleration,   smooth = list(spread = FALSE),   data = A ) crPlots(m) m.step <- stepAIC(m, k=log(nrow(A)), trace=FALSE) summary(m.step) #>  #> Call: #> lm(formula = log(mpg) ~ horsepower + weight + acceleration +  #>     year + origin, data = A) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -0.3523 -0.0568  0.0068  0.0674  0.3586  #>  #> Coefficients: #>              Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   9.43459    0.26153   36.07  < 2e-16 *** #> horsepower   -0.27625    0.05614   -4.92  1.3e-06 *** #> weight       -0.60907    0.05600  -10.88  < 2e-16 *** #> acceleration -0.13138    0.05319   -2.47  0.01397 *   #> year71        0.02798    0.02894    0.97  0.33412     #> year72       -0.00711    0.02845   -0.25  0.80274     #> year73       -0.03953    0.02601   -1.52  0.12947     #> year74        0.05275    0.02999    1.76  0.07936 .   #> year75        0.05320    0.02928    1.82  0.07004 .   #> year76        0.07432    0.02821    2.63  0.00878 **  #> year77        0.13793    0.02888    4.78  2.6e-06 *** #> year78        0.14588    0.02753    5.30  2.0e-07 *** #> year79        0.23604    0.02908    8.12  7.0e-15 *** #> year80        0.33527    0.03115   10.76  < 2e-16 *** #> year81        0.26287    0.03056    8.60  < 2e-16 *** #> year82        0.32339    0.02961   10.92  < 2e-16 *** #> originEurope  0.05582    0.01678    3.33  0.00097 *** #> originJapan   0.04355    0.01748    2.49  0.01314 *   #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.105 on 374 degrees of freedom #> Multiple R-squared:  0.909,  Adjusted R-squared:  0.905  #> F-statistic:  220 on 17 and 374 DF,  p-value: <2e-16 Anova(m.step) #> Anova Table (Type II tests) #>  #> Response: log(mpg) #>              Sum Sq  Df F value  Pr(>F)     #> horsepower     0.27   1   24.21 1.3e-06 *** #> weight         1.30   1  118.28 < 2e-16 *** #> acceleration   0.07   1    6.10  0.0140 *   #> year           4.76  12   36.05 < 2e-16 *** #> origin         0.14   2    6.21  0.0022 **  #> Residuals      4.11 374                     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 mse(Auto$mpg, exp(fitted(m.step))) #> [1] 6.5121 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" mse(Auto$mpg, fitted(m.auto)) #> [1] 8.0932 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\" medAbsErr(Auto$mpg, exp(fitted(m.step))) #> [1] 1.3396 medAbsErr(Auto$mpg, fitted(m.auto)) #> [1] 1.6661 num.predictors #> [1] \"displacement\" \"horsepower\"   \"weight\"       \"acceleration\" cvs <- cv(   selectTransStepAIC,   data = Auto,   seed = 76692,   working.model = m.auto,   predictors = num.predictors,   response = \"mpg\",   AIC = FALSE,   criterion = medAbsErr ) #> R RNG seed set to 76692 cvs #> 10-Fold Cross Validation #> criterion: medAbsErr #> cross-validation criterion = 1.4951 #> full-sample criterion = 1.3396 compareFolds(cvs) #> CV criterion by folds: #>  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10  #>  1.5639  1.5766  1.3698  1.2381  1.2461  1.5826  1.3426  1.1905  1.1355  1.5854  #>  #> Coefficients by folds: #>         (Intercept) horsepower lam.acceleration lam.displacement lam.horsepower #> Fold 1      9.71384   -0.17408          0.50000          0.00000        0.00000 #> Fold 2      9.21713   -0.31480          0.00000          0.00000        0.00000 #> Fold 3      9.61824   -0.19248          0.00000          0.00000        0.00000 #> Fold 4      8.69910   -0.25523          0.50000          0.00000        0.00000 #> Fold 5      9.14403   -0.14934          0.00000          0.00000        0.00000 #> Fold 6      9.63481   -0.16739          0.50000          0.00000        0.00000 #> Fold 7      9.98933   -0.36847          0.00000          0.00000       -0.15447 #> Fold 8      9.06301   -0.29721          0.00000          0.00000        0.00000 #> Fold 9      8.88315   -0.22684          0.00000          0.00000        0.00000 #> Fold 10     9.61727   -0.17086          0.00000          0.00000        0.00000 #>         lam.weight   lambda   weight   year71   year72   year73   year74 #> Fold 1     0.00000  0.00000 -0.74636  0.03764 -0.00327 -0.02477  0.05606 #> Fold 2     0.00000  0.00000 -0.47728  0.02173 -0.01488 -0.03770  0.04312 #> Fold 3     0.00000  0.00000 -0.72085  0.01128 -0.02569 -0.03872  0.05187 #> Fold 4     0.00000  0.00000 -0.53846  0.02153 -0.02922 -0.05181  0.04136 #> Fold 5     0.00000  0.00000 -0.69081  0.02531 -0.01062 -0.04625  0.05039 #> Fold 6     0.00000  0.00000 -0.74049  0.02456  0.00759 -0.03412  0.06266 #> Fold 7     0.00000  0.00000 -0.72843  0.02532 -0.01271 -0.04144  0.04568 #> Fold 8     0.00000  0.00000 -0.46392  0.02702 -0.02041 -0.05605  0.04437 #> Fold 9     0.00000  0.00000 -0.47136  0.00860 -0.03620 -0.04835  0.01906 #> Fold 10    0.00000  0.00000 -0.73550  0.02937 -0.00899 -0.03814  0.05408 #>           year75   year76   year77   year78   year79   year80   year81   year82 #> Fold 1   0.07080  0.07250  0.14420  0.14281  0.23266  0.35127  0.25635  0.30546 #> Fold 2   0.04031  0.06718  0.13094  0.14917  0.21871  0.33192  0.26196  0.30943 #> Fold 3   0.03837  0.06399  0.11593  0.12601  0.20499  0.32821  0.24478  0.29204 #> Fold 4   0.04072  0.05537  0.12292  0.14083  0.22878  0.32947  0.25140  0.27248 #> Fold 5   0.05596  0.07044  0.13356  0.14724  0.24675  0.33331  0.26938  0.32594 #> Fold 6   0.06940  0.07769  0.14211  0.14647  0.23532  0.34761  0.26737  0.33062 #> Fold 7   0.03614  0.07385  0.12976  0.14040  0.23976  0.33998  0.27652  0.30659 #> Fold 8   0.06573  0.08135  0.13158  0.13987  0.23011  0.32880  0.25886  0.30538 #> Fold 9   0.03018  0.05846  0.10536  0.11722  0.20665  0.31533  0.23352  0.29375 #> Fold 10  0.04881  0.07862  0.14101  0.14313  0.23258  0.35649  0.26214  0.32421 #>         acceleration displacement cylinders5.6 cylinders8 originEurope #> Fold 1                                                                 #> Fold 2      -0.18909     -0.09197                                      #> Fold 3                                                                 #> Fold 4      -0.03484                  -0.09080   -0.10909              #> Fold 5                                                         0.06261 #> Fold 6                                                                 #> Fold 7                                                                 #> Fold 8      -0.17676     -0.10542                                      #> Fold 9      -0.14514     -0.13452                                      #> Fold 10                                                                #>         originJapan #> Fold 1              #> Fold 2              #> Fold 3              #> Fold 4              #> Fold 5         0.04 #> Fold 6              #> Fold 7              #> Fold 8              #> Fold 9              #> Fold 10"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross-validation","title":"Cross-validating regression models","text":"Cross-validation (CV) essentially simple intuitively reasonable approach estimating predictive accuracy regression models. CV developed many standard sources regression modeling “machine learning”—particularly recommend James, Witten, Hastie, & Tibshirani (2021, secs. 5.1, 5.3)—describe method briefly taking computational issues examples. See Arlot & Celisse (2010) wide-ranging, technical, survey cross-validation related methods emphasizes statistical properties CV. Validating research replication independently collected data common scientific norm. Emulating process single study data-division less common: data randomly divided two, possibly equal-size, parts; first part used develop fit statistical model; second part used assess adequacy model fit first part data. Data-division, however, suffers two problems: (1) Dividing data decreases sample size thus increases sampling error; (2), even disconcertingly, particularly smaller samples, results can vary substantially based random division data: See Harrell (2015, sec. 5.3) remarks data-division cross-validation. Cross-validation speaks issues. CV, data randomly divided equally possible several, say \\(k\\), parts, called “folds.” statistical model fit \\(k\\) times, leaving fold turn. fitted model used predict response variable cases omitted fold. CV criterion “cost” measure, mean-squared error (“MSE”) prediction, computed using predicted values. extreme \\(k = n\\), number cases data, thus omitting individual cases refitting model \\(n\\) times—procedure termed “leave-one-(LOO) cross-validation.” \\(n\\) models fit \\(n - 1\\) cases, LOO CV produces nearly unbiased estimate prediction error. \\(n\\) regression models highly statistical dependent, however, based nearly data, resulting estimate prediction error relatively large variance. contrast, estimated prediction error \\(k\\)-fold CV \\(k = 5\\) \\(10\\) (commonly employed choices) somewhat biased smaller variance. also possible correct \\(k\\)-fold CV bias (see ).","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"polynomial-regression-for-the-auto-data","dir":"Articles","previous_headings":"Examples","what":"Polynomial regression for the Auto data","title":"Cross-validating regression models","text":"data example drawn ISLR2 package R, associated James et al. (2021). presentation close (though identical) original source (James et al., 2021, secs. 5.1, 5.3), demonstrates use cv() function cv package.1 Auto dataset contains information 392 cars: exception origin (don’t use ), variables largely self-explanatory, except possibly units measurement: details see help(\"Auto\", package=\"ISLR2\"). ’ll focus relationship mpg (miles per gallon) horsepower, displayed following scatterplot: mpg vs horsepower Auto data relationship two variables monotone, decreasing, nonlinear. Following James et al. (2021), ’ll consider approximating relationship polynomial regression, degree polynomial \\(p\\) ranging 1 (linear regression) 10.2 Polynomial fits \\(p = 1\\) \\(5\\) shown following figure: mpg vs horsepower Auto data linear fit clearly inappropriate; fits \\(p = 2\\) (quadratic) \\(4\\) similar; fit \\(p = 5\\) may -fit data chasing one two relatively high mpg values right (see CV results reported ). following graph shows two measures estimated (squared) error function polynomial-regression degree: mean-squared error (“MSE”), defined \\(\\mathsf{MSE} = \\frac{1}{n}\\sum_{=1}^n (y_i - \\widehat{y}_i)^2\\), usual residual variance, defined \\(\\widehat{\\sigma}^2 = \\frac{1}{n - p - 1} \\sum_{=1}^n (y_i - \\widehat{y}_i)^2\\). former necessarily declines \\(p\\) (, strictly, can’t increase \\(p\\)), latter gets slightly larger largest values \\(p\\), “best” value, small margin, \\(p = 7\\). Estimated squared error function polynomial degree, \\(p\\) code graph uses mse() function cv package compute MSE fit.","code":"data(\"Auto\", package=\"ISLR2\") head(Auto) #>   mpg cylinders displacement horsepower weight acceleration year origin #> 1  18         8          307        130   3504         12.0   70      1 #> 2  15         8          350        165   3693         11.5   70      1 #> 3  18         8          318        150   3436         11.0   70      1 #> 4  16         8          304        150   3433         12.0   70      1 #> 5  17         8          302        140   3449         10.5   70      1 #> 6  15         8          429        198   4341         10.0   70      1 #>                        name #> 1 chevrolet chevelle malibu #> 2         buick skylark 320 #> 3        plymouth satellite #> 4             amc rebel sst #> 5               ford torino #> 6          ford galaxie 500 dim(Auto) #> [1] 392   9 plot(mpg ~ horsepower, data=Auto) plot(mpg ~ horsepower, data = Auto) horsepower <- with(Auto,                    seq(min(horsepower), max(horsepower),                        length = 1000)) for (p in 1:5) {   m <- lm(mpg ~ poly(horsepower, p), data = Auto)   mpg <- predict(m, newdata = data.frame(horsepower = horsepower))   lines(horsepower,         mpg,         col = p + 1,         lty = p,         lwd = 2) } legend(   \"topright\",   legend = 1:5,   col = 2:6,   lty = 1:5,   lwd = 2,   title = \"Degree\",   inset = 0.02 ) library(\"cv\") # for mse() and other functions #> Loading required package: doParallel #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel var <- mse <- numeric(10) for (p in 1:10) {   m <- lm(mpg ~ poly(horsepower, p), data = Auto)   mse[p] <- mse(Auto$mpg, fitted(m))   var[p] <- summary(m)$sigma ^ 2 }  plot(   c(1, 10),   range(mse, var),   type = \"n\",   xlab = \"Degree of polynomial, p\",   ylab = \"Estimated Squared Error\" ) lines(   1:10,   mse,   lwd = 2,   lty = 1,   col = 2,   pch = 16,   type = \"b\" ) lines(   1:10,   var,   lwd = 2,   lty = 2,   col = 3,   pch = 17,   type = \"b\" ) legend(   \"topright\",   inset = 0.02,   legend = c(expression(hat(sigma) ^ 2), \"MSE\"),   lwd = 2,   lty = 2:1,   col = 3:2,   pch = 17:16 )"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"using-cv","dir":"Articles","previous_headings":"Examples > Polynomial regression for the Auto data","what":"Using cv()","title":"Cross-validating regression models","text":"generic cv() function \"lm\" method, default performs \\(k = 10\\)-fold CV: \"lm\" method default uses mse() CV criterion Woodbury matrix identity (Hager, 1989) update regression fold deleted without literally refit model. (Computational details discussed separate vignette.) function reports CV estimate MSE, biased-adjusted estimate MSE (bias adjustment explained final section), MSE also computed original, full-sample regression. division data 10 folds random, cv() explicitly (randomly) generates saves seed R’s pseudo-random number generator, make results replicable. user can also specify seed directly via seed argument cv(). perform LOO CV, can set k argument cv() number cases data, k=392, , conveniently, k=\"loo\" k=\"n\": LOO CV linear model, cv() default uses hatvalues model fit full data LOO updates, reports CV estimate MSE. Alternative methods use Woodbury matrix identity “naive” approach literally refitting model case omitted. three methods produce exact results linear model (within precision floating-point computations): \"naive\" \"Woodbury\" methods also return bias-adjusted estimate MSE full-sample MSE, bias isn’t issue LOO CV.","code":"m.auto <- lm(mpg ~ poly(horsepower, 2), data = Auto) summary(m.auto) #>  #> Call: #> lm(formula = mpg ~ poly(horsepower, 2), data = Auto) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.714  -2.594  -0.086   2.287  15.896  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)            23.446      0.221   106.1   <2e-16 *** #> poly(horsepower, 2)1 -120.138      4.374   -27.5   <2e-16 *** #> poly(horsepower, 2)2   44.090      4.374    10.1   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.37 on 389 degrees of freedom #> Multiple R-squared:  0.688,  Adjusted R-squared:  0.686  #> F-statistic:  428 on 2 and 389 DF,  p-value: <2e-16 cv(m.auto) #> R RNG seed set to 841111 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 19.25 #> bias-adjusted cross-validation criterion = 19.236 #> full-sample criterion = 18.985 cv(m.auto, k = \"loo\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: mse #> cross-validation criterion = 19.248 cv(m.auto, k = \"loo\", method = \"naive\") #> n-Fold Cross Validation #> method: naive #> criterion: mse #> cross-validation criterion = 19.248 #> bias-adjusted cross-validation criterion = 19.248 #> full-sample criterion = 18.985 cv(m.auto, k = \"loo\", method = \"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 19.248 #> bias-adjusted cross-validation criterion = 19.248 #> full-sample criterion = 18.985"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"comparing-competing-models","dir":"Articles","previous_headings":"Examples > Polynomial regression for the Auto data","what":"Comparing competing models","title":"Cross-validating regression models","text":"cv() function also method can applied list regression models data, composed using models() function. \\(k\\)-fold CV, folds used competing models, reduces random error comparison. result can also obtained specifying common seed R’s random-number generator applying cv() separately model, employing list models convenient \\(k\\)-fold LOO CV (random component composition \\(n\\) folds). illustrate polynomial regression models varying degree Auto data (discussed previously), beginning fitting saving 10 models: convoluted code within loop produce 10 models insures model formulas form, e.g., mpg ~ poly(horsepower, 2) rather mpg ~ poly(horsepower, p), useful us separate vignette, consider cross-validating model-selection process. apply cv() list 10 models (data argument required): didn’t supply names models calls models() function, names model.1, model.2, etc., generated function. Finally, extract graph adjusted MSEs \\(10\\)-fold CV MSEs LOO CV (see section manipulating \"cv\" objects: Cross-validated 10-fold LOO MSE function polynomial degree, \\(p\\) Alternatively, can use plot() method \"cvModList\" objects compare models, though separate graphs 10-fold LOO CV: Cross-validated 10-fold LOO MSE function polynomial degree, \\(p\\) example, 10-fold LOO CV produce generally similar results, also results similar produced estimated error variance \\(\\widehat{\\sigma}^2\\) model, reported (except highest-degree polynomials, CV results clearly suggest -fitting).","code":"for (p in 1:10) {   command <- paste0(\"m.\", p, \"<- lm(mpg ~ poly(horsepower, \", p,                     \"), data=Auto)\")   eval(parse(text = command)) } objects(pattern = \"m\\\\.[0-9]\") #>  [1] \"m.1\"  \"m.10\" \"m.2\"  \"m.3\"  \"m.4\"  \"m.5\"  \"m.6\"  \"m.7\"  \"m.8\"  \"m.9\" summary(m.2) # for example, the quadratic fit #>  #> Call: #> lm(formula = mpg ~ poly(horsepower, 2), data = Auto) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.714  -2.594  -0.086   2.287  15.896  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)            23.446      0.221   106.1   <2e-16 *** #> poly(horsepower, 2)1 -120.138      4.374   -27.5   <2e-16 *** #> poly(horsepower, 2)2   44.090      4.374    10.1   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.37 on 389 degrees of freedom #> Multiple R-squared:  0.688,  Adjusted R-squared:  0.686  #> F-statistic:  428 on 2 and 389 DF,  p-value: <2e-16 # 10-fold CV cv.auto.10 <- cv(   models(m.1, m.2, m.3, m.4, m.5,          m.6, m.7, m.8, m.9, m.10),   data = Auto,   seed = 2120 ) cv.auto.10[1:2] # for the linear and quadratic models #>  #> Model model.1: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 24.246 #> bias-adjusted cross-validation criterion = 24.23 #> full-sample criterion = 23.944  #>  #> Model model.2: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.346 #> bias-adjusted cross-validation criterion = 19.327 #> full-sample criterion = 18.985 # LOO CV cv.auto.loo <- cv(models(m.1, m.2, m.3, m.4, m.5,                          m.6, m.7, m.8, m.9, m.10),                   data = Auto,                   k = \"loo\") cv.auto.loo[1:2] # linear and quadratic models #>  #> Model model.1: #> n-Fold Cross Validation #> method: hatvalues #> cross-validation criterion = 24.232 #> Model model.2: #> n-Fold Cross Validation #> method: hatvalues #> cross-validation criterion = 19.248 cv.mse.10 <- as.data.frame(cv.auto.10,                             rows=\"cv\",                                         columns=\"criteria\"                            )$adjusted.criterion cv.mse.loo <- as.data.frame(cv.auto.loo,                             rows=\"cv\",                                         columns=\"criteria\"                            )$criterion plot(   c(1, 10),   range(cv.mse.10, cv.mse.loo),   type = \"n\",   xlab = \"Degree of polynomial, p\",   ylab = \"Cross-Validated MSE\" ) lines(   1:10,   cv.mse.10,   lwd = 2,   lty = 1,   col = 2,   pch = 16,   type = \"b\" ) lines(   1:10,   cv.mse.loo,   lwd = 2,   lty = 2,   col = 3,   pch = 17,   type = \"b\" ) legend(   \"topright\",   inset = 0.02,   legend = c(\"10-Fold CV\", \"LOO CV\"),   lwd = 2,   lty = 2:1,   col = 3:2,   pch = 17:16 ) plot(cv.auto.10, main=\"Polynomial Regressions, 10-Fold CV\",      axis.args=list(labels=1:10), xlab=\"Degree of Polynomial, p\") plot(cv.auto.loo, main=\"Polynomial Regressions, LOO CV\",      axis.args=list(labels=1:10), xlab=\"Degree of Polynomial, p\")"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"logistic-regression-for-the-mroz-data","dir":"Articles","previous_headings":"Examples","what":"Logistic regression for the Mroz data","title":"Cross-validating regression models","text":"Mroz data set carData package (associated Fox & Weisberg, 2019) used several authors illustrate binary logistic regression; see, particular Fox & Weisberg (2019). data originally drawn U.S. Panel Study Income Dynamics pertain married women. cases data set: response variable logistic regression lfp, labor-force participation, factor coded \"yes\" \"\". remaining variables predictors: k5, number children 5 years old younger woman’s household; k618, number children 6 18 years old; age, years; wc, wife’s college attendance, \"yes\" \"\"; hc, husband’s college attendance; lwg, woman’s log wage rate employed, imputed wage rate, (variable Fox & Weisberg, 2019 show problematically defined); inc, family income, $1000s, exclusive wife’s income. use glm() function fit binary logistic regression Mroz data: addition usually summary output GLM, show result applying BayesRule() function cv package predictions derived fitted model. Bayes rule, predicts “success” binary regression model fitted probability success [.e., \\(\\phi = \\Pr(y = 1)\\)] \\(\\widehat{\\phi} \\ge .5\\) “failure” \\(\\widehat{\\phi} \\lt .5\\).3 first argument BayesRule() binary {0, 1} response, second argument predicted probability success. BayesRule() returns proportion predictions error, appropriate “cost” function. value returned BayesRule() associated “attribute” named \"casewise loss\" set \"y != round(yhat)\", signifying Bayes rule CV criterion computed mean casewise values, 0 prediction case matches observed value 1 (signifying prediction error). mse() function numeric responses also calculated casewise average. criteria, median absolute error, computed medAbsErr() function cv package, aren’t averages casewise components. distinction important , knowledge, statistical theory cross-validation, example, Davison & Hinkley (1997), Bates, Hastie, & Tibshirani (2023), Arlot & Celisse (2010), developed CV criteria like MSE means casewise components. consequence, limit computation bias adjustment confidence intervals (see ) criteria casewise averages. example, fitted logistic regression incorrectly predicts 31% responses; expect estimate optimistic given model used “predict” data fit. \"glm\" method cv() largely similar \"lm\" method, although default algorithm, selected explicitly method=\"exact\", refits model fold removed (thus equivalent method=\"naive\" \"lm\" models). generalized linear models, method=\"Woodbury\" (LOO CV) method=\"hatvalues\" provide approximate results (see computational technical vignette details): ensure two methods use 10 folds, specify seed R’s random-number generator explicitly; , common experience, \"exact\" \"Woodbury\" algorithms produce nearly identical results. CV estimates prediction error slightly higher estimate based cases. printed output includes 95% confidence interval bias-adjusted Bayes rule CV criterion. Bates et al. (2023) show confidence intervals unreliable models fit small samples, default cv() computes sample size 400 larger CV criterion employed average casewise components, case Bayes rule. See final section vignette details computation confidence intervals bias-adjusted CV criteria. results applying LOO CV Mroz model, using exact approximate methods: number decimal digits shown, three methods produce identical results example.","code":"data(\"Mroz\", package = \"carData\") head(Mroz, 3) #>   lfp k5 k618 age wc hc    lwg   inc #> 1 yes  1    0  32 no no 1.2102 10.91 #> 2 yes  0    2  30 no no 0.3285 19.50 #> 3 yes  1    3  35 no no 1.5141 12.04 tail(Mroz, 3) #>     lfp k5 k618 age wc hc     lwg    inc #> 751  no  0    0  43 no no 0.88814  9.952 #> 752  no  0    0  60 no no 1.22497 24.984 #> 753  no  0    3  39 no no 0.85321 28.363 m.mroz <- glm(lfp ~ ., data = Mroz, family = binomial) summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4 BayesRule(ifelse(Mroz$lfp == \"yes\", 1, 0),           fitted(m.mroz, type = \"response\")) #> [1] 0.30677 #> attr(,\"casewise loss\") #> [1] \"y != round(yhat)\" cv(m.mroz, criterion = BayesRule, seed = 248) #> R RNG seed set to 248 #> 10-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31952 #> 95% CI for bias-adjusted CV criterion = (0.28607, 0.35297) #> full-sample criterion = 0.30677 cv(m.mroz,    criterion = BayesRule,    seed = 248,    method = \"Woodbury\") #> R RNG seed set to 248 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31926 #> 95% CI for bias-adjusted CV criterion = (0.28581, 0.35271) #> full-sample criterion = 0.30677 cv(m.mroz, k = \"loo\", criterion = BayesRule) #> n-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.3183 #> 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164) #> full-sample criterion = 0.30677 cv(m.mroz,    k = \"loo\",    criterion = BayesRule,    method = \"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.3183 #> 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164) #> full-sample criterion = 0.30677 cv(m.mroz,    k = \"loo\",    criterion = BayesRule,    method = \"hatvalues\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: BayesRule #> cross-validation criterion = 0.32005"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"replicating-cross-validation","dir":"Articles","previous_headings":"","what":"Replicating cross-validation","title":"Cross-validating regression models","text":"Assuming number cases \\(n\\) multiple number folds \\(k\\)—slightly simplifying assumption—number possible partitions cases folds \\(\\frac{n!}{[(n/k)!]^k}\\), number grows large quickly. example, \\(n = 10\\) \\(k = 5\\), folds size \\(n/k = 2\\), \\(113,400\\) possible partitions; \\(n=100\\) \\(k=5\\), \\(n/k = 20\\), still small problem, number possible partitions truly astronomical, \\(1.09\\times 10^{66}\\). partition folds ’s employed selected randomly, resulting CV criterion estimates subject sampling error. (exception LOO cross-validation, random.) get sense magnitude sampling error, can repeat CV procedure different randomly selected partitions folds. CV functions cv package capable repeated cross-validation, number repetitions controlled reps argument, defaults 1. , example, 10-fold CV Mroz logistic regression, repeated 5 times: reps > 1, result returned cv() object class \"cvList\"—literally list \"cv\" objects. results reported repetition averaged across repetitions, standard deviations CV criterion biased-adjusted CV criterion given parentheses. example, therefore little variation across repetitions, increasing confidence reliability results. Notice seed ’s set cv() command pertains first repetition seeds remaining repetitions selected pseudo-randomly.4 Setting first seed, however, makes entire process easily replicable, seed repetition stored corresponding element \"cvList\" object (isn’t, however, saved example). ’s also possible replicate CV comparing competing models via cv() method \"modList\" objects. Recall comparison polynomial regressions varying degree fit Auto data; performed 10-fold CV 10 models. , replicate process 5 times model graph results: Replicated cross-validated 10-fold CV function polynomial degree, \\(p\\) graph shows average CV criterion range competing models.","code":"cv(   m.mroz,   criterion = BayesRule,   seed = 248,   reps = 5,   method = \"Woodbury\" ) #> R RNG seed set to 248 #> R RNG seed set to 68134 #> R RNG seed set to 767359 #> R RNG seed set to 556270 #> R RNG seed set to 882966 #>  #> Replicate 1: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.31301 #> 95% CI for bias-adjusted CV criterion = (0.27967, 0.34635) #> full-sample criterion = 0.30677  #>  #> Replicate 2: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.31607 #> bias-adjusted cross-validation criterion = 0.3117 #> 95% CI for bias-adjusted CV criterion = (0.27847, 0.34493) #> full-sample criterion = 0.30677  #>  #> Replicate 3: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.31474 #> bias-adjusted cross-validation criterion = 0.30862 #> 95% CI for bias-adjusted CV criterion = (0.27543, 0.34181) #> full-sample criterion = 0.30677  #>  #> Replicate 4: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31807 #> 95% CI for bias-adjusted CV criterion = (0.28462, 0.35152) #> full-sample criterion = 0.30677  #>  #> Replicate 5: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31926 #> 95% CI for bias-adjusted CV criterion = (0.28581, 0.35271) #> full-sample criterion = 0.30677  #>  #> Average: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.31983 (0.003887) #> bias-adjusted cross-validation criterion = 0.31394 (0.0040093) #> full-sample criterion = 0.30677 cv.auto.reps <- cv(   models(m.1, m.2, m.3, m.4, m.5,          m.6, m.7, m.8, m.9, m.10),   data = Auto,   seed = 8004,   reps = 5 ) plot(cv.auto.reps)"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"manipulating-cv-and-related-objects","dir":"Articles","previous_headings":"","what":"Manipulating “cv” and related objects","title":"Cross-validating regression models","text":"cv() functions returns object class \"cv\"—closely related object, example, class \"cvList\"—contains variety information results CV procedure. cv package provides .data.frame() methods put information form data frames examination analysis.5 also summary() method extracting summarizing information resulting data frames. ’ll illustrate replicated CV performed 10 polynomial-regression models fit Auto data: case, 5 replications 10-fold CV. Converting cv.auto.reps data frame produces, default: resulting data frame \\(\\mathsf{replications} \\times (\\mathsf{folds} + 1) \\times \\mathsf{models} = 5 \\times (10 + 1) \\times 10 = 550\\) rows, first rows pertain first model first replication, fold = 0 indicates overall results first replication first model. regression coefficients appear columns data frame. first model includes intercept linear polynomial term, coefficients NA. ’s possible suppress regression coefficients specifying argument columns=\"criteria\" .data.frame(): summary() method \"cvDataFrame\" related objects formula interface, may used, example, follows: See ?summary.cvDataFrame details.","code":"cv.auto.reps #>  #> Model model.1 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 24.185 (0.046346) #> bias-adjusted cross-validation criterion = 24.172 (0.043909) #> full-sample criterion = 23.944  #>  #> Model model.2 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.253 (0.035024) #> bias-adjusted cross-validation criterion = 19.239 (0.033274) #> full-sample criterion = 18.985  #>  #> Model model.3 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.349 (0.078429) #> bias-adjusted cross-validation criterion = 19.328 (0.074238) #> full-sample criterion = 18.945  #>  #> Model model.4 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.449 (0.1074) #> bias-adjusted cross-validation criterion = 19.418 (0.10153) #> full-sample criterion = 18.876  #>  #> Model model.5 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.095 (0.17148) #> bias-adjusted cross-validation criterion = 19.059 (0.16202) #> full-sample criterion = 18.427  #>  #> Model model.6 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.034 (0.20617) #> bias-adjusted cross-validation criterion = 18.99 (0.19465) #> full-sample criterion = 18.241  #>  #> Model model.7 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 18.897 (0.20121) #> bias-adjusted cross-validation criterion = 18.852 (0.19015) #> full-sample criterion = 18.078  #>  #> Model model.8 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.026 (0.21574) #> bias-adjusted cross-validation criterion = 18.973 (0.2035) #> full-sample criterion = 18.066  #>  #> Model model.9 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.115 (0.21158) #> bias-adjusted cross-validation criterion = 19.054 (0.20041) #> full-sample criterion = 18.027  #>  #> Model model.10 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.427 (0.24337) #> bias-adjusted cross-validation criterion = 19.339 (0.22818) #> full-sample criterion = 18.01 class(cv.auto.reps) #> [1] \"cvModList\" D <- as.data.frame(cv.auto.reps) dim(D) #> [1] 550  62 class(D) #> [1] \"cvModListDataFrame\" \"cvListDataFrame\"    \"cvDataFrame\"        #> [4] \"data.frame\" head(D) #>     model rep fold criterion adjusted.criterion full.criterion coef.Intercept #> 1 model.1   1    0      24.2               24.2           23.9           23.4 #> 2 model.1   1    1      34.3                 NA             NA           23.3 #> 3 model.1   1    2      24.5                 NA             NA           23.3 #> 4 model.1   1    3      13.9                 NA             NA           23.6 #> 5 model.1   1    4      15.5                 NA             NA           23.5 #> 6 model.1   1    5      28.6                 NA             NA           23.4 #>   coef.poly(horsepower, 1) coef.poly(horsepower, 2)1 coef.poly(horsepower, 2)2 #> 1                     -120                        NA                        NA #> 2                     -121                        NA                        NA #> 3                     -119                        NA                        NA #> 4                     -120                        NA                        NA #> 5                     -118                        NA                        NA #> 6                     -119                        NA                        NA #>   coef.poly(horsepower, 3)1 coef.poly(horsepower, 3)2 coef.poly(horsepower, 3)3 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 4)1 coef.poly(horsepower, 4)2 coef.poly(horsepower, 4)3 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 4)4 coef.poly(horsepower, 5)1 coef.poly(horsepower, 5)2 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 5)3 coef.poly(horsepower, 5)4 coef.poly(horsepower, 5)5 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 6)1 coef.poly(horsepower, 6)2 coef.poly(horsepower, 6)3 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 6)4 coef.poly(horsepower, 6)5 coef.poly(horsepower, 6)6 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 7)1 coef.poly(horsepower, 7)2 coef.poly(horsepower, 7)3 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 7)4 coef.poly(horsepower, 7)5 coef.poly(horsepower, 7)6 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 7)7 coef.poly(horsepower, 8)1 coef.poly(horsepower, 8)2 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 8)3 coef.poly(horsepower, 8)4 coef.poly(horsepower, 8)5 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 8)6 coef.poly(horsepower, 8)7 coef.poly(horsepower, 8)8 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 9)1 coef.poly(horsepower, 9)2 coef.poly(horsepower, 9)3 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 9)4 coef.poly(horsepower, 9)5 coef.poly(horsepower, 9)6 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 9)7 coef.poly(horsepower, 9)8 coef.poly(horsepower, 9)9 #> 1                        NA                        NA                        NA #> 2                        NA                        NA                        NA #> 3                        NA                        NA                        NA #> 4                        NA                        NA                        NA #> 5                        NA                        NA                        NA #> 6                        NA                        NA                        NA #>   coef.poly(horsepower, 10)1 coef.poly(horsepower, 10)2 #> 1                         NA                         NA #> 2                         NA                         NA #> 3                         NA                         NA #> 4                         NA                         NA #> 5                         NA                         NA #> 6                         NA                         NA #>   coef.poly(horsepower, 10)3 coef.poly(horsepower, 10)4 #> 1                         NA                         NA #> 2                         NA                         NA #> 3                         NA                         NA #> 4                         NA                         NA #> 5                         NA                         NA #> 6                         NA                         NA #>   coef.poly(horsepower, 10)5 coef.poly(horsepower, 10)6 #> 1                         NA                         NA #> 2                         NA                         NA #> 3                         NA                         NA #> 4                         NA                         NA #> 5                         NA                         NA #> 6                         NA                         NA #>   coef.poly(horsepower, 10)7 coef.poly(horsepower, 10)8 #> 1                         NA                         NA #> 2                         NA                         NA #> 3                         NA                         NA #> 4                         NA                         NA #> 5                         NA                         NA #> 6                         NA                         NA #>   coef.poly(horsepower, 10)9 coef.poly(horsepower, 10)10 #> 1                         NA                          NA #> 2                         NA                          NA #> 3                         NA                          NA #> 4                         NA                          NA #> 5                         NA                          NA #> 6                         NA                          NA D <- as.data.frame(cv.auto.reps, columns=\"criteria\") head(D) #>     model rep fold criterion adjusted.criterion full.criterion #> 1 model.1   1    0      24.2               24.2           23.9 #> 2 model.1   1    1      34.3                 NA             NA #> 3 model.1   1    2      24.5                 NA             NA #> 4 model.1   1    3      13.9                 NA             NA #> 5 model.1   1    4      15.5                 NA             NA #> 6 model.1   1    5      28.6                 NA             NA head(subset(D, fold == 0)) #>      model rep fold criterion adjusted.criterion full.criterion #> 1  model.1   1    0      24.2               24.2           23.9 #> 12 model.1   2    0      24.1               24.1           23.9 #> 23 model.1   3    0      24.2               24.2           23.9 #> 34 model.1   4    0      24.2               24.1           23.9 #> 45 model.1   5    0      24.2               24.2           23.9 #> 56 model.2   1    0      19.2               19.2           19.0 summary(D, adjusted.criterion ~ model + rep) # fold \"0\" only #>           rep #> model           1      2      3      4      5 #>   model.1  24.193 24.113 24.226 24.144 24.184 #>   model.2  19.209 19.285 19.240 19.205 19.256 #>   model.3  19.309 19.445 19.348 19.255 19.282 #>   model.4  19.521 19.524 19.402 19.343 19.301 #>   model.5  19.242 19.139 19.137 18.877 18.899 #>   model.6  19.157 19.145 19.082 18.730 18.838 #>   model.7  18.935 19.056 18.960 18.596 18.715 #>   model.8  19.043 19.174 19.111 18.675 18.861 #>   model.9  19.133 19.286 19.161 18.811 18.880 #>   model.10 19.524 19.586 19.345 19.027 19.214 summary(D, criterion ~ model + rep,          include=\"folds\") # mean over folds #>           rep #> model           1      2      3      4      5 #>   model.1  24.181 24.128 24.226 24.134 24.229 #>   model.2  19.202 19.310 19.236 19.193 19.306 #>   model.3  19.309 19.483 19.352 19.247 19.336 #>   model.4  19.536 19.571 19.415 19.342 19.360 #>   model.5  19.262 19.195 19.163 18.874 18.963 #>   model.6  19.182 19.217 19.116 18.731 18.910 #>   model.7  18.954 19.135 18.997 18.601 18.787 #>   model.8  19.071 19.259 19.162 18.686 18.944 #>   model.9  19.171 19.379 19.214 18.834 18.965 #>   model.10 19.603 19.710 19.416 19.068 19.329 summary(D, criterion ~ model + rep, fun=sd,          include=\"folds\") #>           rep #> model           1      2      3      4      5 #>   model.1  7.5627 5.2658 5.7947 4.7165 7.3591 #>   model.2  5.9324 5.2518 5.8166 5.1730 6.2126 #>   model.3  6.0754 5.4492 5.8168 5.2288 6.2387 #>   model.4  6.1737 5.2488 5.9952 5.4866 6.3122 #>   model.5  5.7598 5.2136 6.2658 5.3923 6.3016 #>   model.6  5.7094 5.0444 6.4010 5.2868 5.9796 #>   model.7  5.6307 5.1164 6.6751 5.1084 5.8796 #>   model.8  5.7009 5.1229 6.4827 5.1068 5.9460 #>   model.9  5.7979 5.2289 6.3960 5.3181 6.0902 #>   model.10 6.1825 4.8554 6.2557 5.7029 6.1985"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"parallel-computations","dir":"Articles","previous_headings":"","what":"Parallel computations","title":"Cross-validating regression models","text":"CV functions cv package capable performing parallel computations setting ncores argument (specifying number computer cores used) number > 1 (default). Parallel computation can advantageous large problems, reducing execution time program. illustrate, let’s time model selection Mroz’s logistic regression, repeating computation performed previously (LOO CV lengthen calculation) parallel using 2 cores: computer, parallel computation 2 cores nearly twice fast, produces result non-parallel computation.","code":"system.time(   m.mroz.sel.cv <- cv(     selectStepAIC,     Mroz,     k = \"loo\",     criterion = BayesRule,     working.model = m.mroz,     AIC = FALSE   ) ) #>    user  system elapsed  #>  25.241   0.957  26.302 system.time(   m.mroz.sel.cv.p <- cv(     selectStepAIC,     Mroz,     k = \"loo\",     criterion = BayesRule,     working.model = m.mroz,     AIC = FALSE,     ncores = 2   ) ) #>    user  system elapsed  #>   0.352   0.051  15.086 all.equal(m.mroz.sel.cv, m.mroz.sel.cv.p) #> [1] TRUE"},{"path":[]},{"path":"https://gmonette.github.io/cv/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"John Fox. Author. Georges Monette. Author, maintainer.","code":""},{"path":"https://gmonette.github.io/cv/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Fox J, Monette G (2024). cv: Cross-Validating Regression Models. R package version 2.0.1, https://CRAN.R-project.org/package=cv, https://gmonette.github.io/cv/.","code":"@Manual{,   title = {cv: Cross-Validating Regression Models},   author = {John Fox and Georges Monette},   year = {2024},   note = {R package version 2.0.1, https://CRAN.R-project.org/package=cv},   url = {https://gmonette.github.io/cv/}, }"},{"path":"https://gmonette.github.io/cv/index.html","id":"cv-package-for-r-cross-validating-regression-models","dir":"","previous_headings":"","what":"Cross-Validating Regression Models","title":"Cross-Validating Regression Models","text":"cv package R provides consistent extensible framework cross-validating standard R statistical models. functions supplied package: cv() generic function default method, computationally efficient \"lm\" \"glm\" methods, \"rlm\" method (robust linear models), method list competing models. also \"merMod\", \"lme\", \"glmmTMB\" methods mixed-effects models. cv() supports parallel computations. mse() (mean-squared error), rmse() (root-mean-squared error), medAbsErr() (median absolute error), BayesRule() cross-validation criteria (“cost functions”), suitable use cv(). cv() also can cross-validate selection procedure (following) regression model: cvModelList() employs CV select model among number candidates, cross-validates model-selection procedure. selectStepAIC() predictor-selection procedure based stepAIC() function MASS package. selectTrans() procedure selecting predictor response transformations regression, based powerTransform() function car package. selectTransStepAIC() procedure first selects predictor response transformations selects predictors. additional introductory information using cv package, see “Cross-validating regression models” vignette (vignette(\"cv\", package=\"cv\")). also vignettes cross-validating mixed-effects models (vignette(\"cv-mixed\", package=\"cv\")), cross-validating model selection (vignette(\"cv-selection\", package=\"cv\")), computational technical notes (vignette(\"cv-notes\", package=\"cv\")). cv package designed extensible classes regression models, CV criteria, model-selection procedures; details, see “Extending cv package” vignette (vignette(\"cv-extend\", package=\"cv\")).","code":""},{"path":"https://gmonette.github.io/cv/index.html","id":"installing-the-cv-package","dir":"","previous_headings":"","what":"Installing the cv package","title":"Cross-Validating Regression Models","text":"install current version cv package CRAN: install development version cv package GitHub:","code":"install.packages(\"cv\") if (!require(remotes)) install.packages(\"remotes\") remotes::install_github(\"gmonette/cv\", build_vignettes=TRUE,   dependencies=TRUE)"},{"path":"https://gmonette.github.io/cv/reference/Pigs.html","id":null,"dir":"Reference","previous_headings":"","what":"Body Weights of 48 Pigs in 9 Successive Weeks — Pigs","title":"Body Weights of 48 Pigs in 9 Successive Weeks — Pigs","text":"data set appears Table 3.1 Diggle, Liang, Zeger (1994).","code":""},{"path":"https://gmonette.github.io/cv/reference/Pigs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Body Weights of 48 Pigs in 9 Successive Weeks — Pigs","text":"","code":"data(\"Pigs\", package = \"cv\")"},{"path":"https://gmonette.github.io/cv/reference/Pigs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Body Weights of 48 Pigs in 9 Successive Weeks — Pigs","text":"data frame 432 rows 3 columns. id Pig id number, 1--48. week Week number, 1--9. weight Weight kg.","code":""},{"path":"https://gmonette.github.io/cv/reference/Pigs.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Body Weights of 48 Pigs in 9 Successive Weeks — Pigs","text":"P. J. Diggle, K.-Y. Liang, S. L. Zeger, Analysis Longitudinal Data (Oxford, 1994).","code":""},{"path":"https://gmonette.github.io/cv/reference/Pigs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Body Weights of 48 Pigs in 9 Successive Weeks — Pigs","text":"","code":"library(\"lme4\") #> Loading required package: Matrix m.p <- lmer(weight ~ week + (1 | id) + (1 | week),             data=Pigs, REML=FALSE,             control=lmerControl(optimizer=\"bobyqa\")) summary(m.p) #> Linear mixed model fit by maximum likelihood  ['lmerMod'] #> Formula: weight ~ week + (1 | id) + (1 | week) #>    Data: Pigs #> Control: lmerControl(optimizer = \"bobyqa\") #>  #>      AIC      BIC   logLik deviance df.resid  #>   2037.6   2058.0  -1013.8   2027.6      427  #>  #> Scaled residuals:  #>     Min      1Q  Median      3Q     Max  #> -3.7750 -0.5418  0.0054  0.4762  3.9816  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. #>  id       (Intercept) 14.83622 3.8518   #>  week     (Intercept)  0.08499 0.2915   #>  Residual              4.29733 2.0730   #> Number of obs: 432, groups:  id, 48; week, 9 #>  #> Fixed effects: #>             Estimate Std. Error t value #> (Intercept) 19.35561    0.63340   30.56 #> week         6.20990    0.05393  115.14 #>  #> Correlation of Fixed Effects: #>      (Intr) #> week -0.426 cv(m.p, clusterVariables=c(\"id\", \"week\"), k=10, seed=8469) #> R RNG seed set to 8469 #> 10-Fold Cross Validation based on 432 {id, week} clusters #> criterion: mse #> cross-validation criterion = 19.2352 #> bias-adjusted cross-validation criterion = 19.23294 #> 95% CI for bias-adjusted CV criterion = (16.49263, 21.97325) #> full-sample criterion = 19.20076"},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Cost Functions for Fitted Regression Models — mse","title":"Cost Functions for Fitted Regression Models — mse","text":"Compute cost functions (cross-validation criteria) fitted regression models.","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cost Functions for Fitted Regression Models — mse","text":"","code":"mse(y, yhat)  rmse(y, yhat)  medAbsErr(y, yhat)  BayesRule(y, yhat)  BayesRule2(y, yhat)"},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cost Functions for Fitted Regression Models — mse","text":"y response yhat fitted value","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cost Functions for Fitted Regression Models — mse","text":"general, cost functions return single numeric value measuring lack--fit. mse() returns mean-squared error; rmse() returns root-mean-squared error; medAbsErr() returns median absolute error; BayesRule() BayesRule2() return proportion misclassified cases.","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cost Functions for Fitted Regression Models — mse","text":"Cost functions (cross-validation criteria) meant measure lack--fit. Several cost functions provided: mse() returns mean-squared error prediction numeric response variable y predictions yhat; rmse() returns root-mean-squared error just square-root mse(). medAbsErr() returns median absolute error prediction numeric response y predictions yhat. BayesRule() BayesRule2() report proportion incorrect predictions dichotomous response variable y, assumed coded (coercible ) 0 1. yhat values predicted probabilities rounded 0 1. distinction BayesRule() BayesRule2() former checks y values either 0 1 yhat values 0 1, latter therefore faster.","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cost Functions for Fitted Regression Models — mse","text":"mse(): Mean-square error. rmse(): Root-mean-square error. medAbsErr(): Median absolute error. BayesRule(): Bayes Rule binary response. BayesRule2(): Bayes rule binary response (without bounds checking).","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cost Functions for Fitted Regression Models — mse","text":"","code":"data(\"Duncan\", package=\"carData\") m.lm <- lm(prestige ~ income + education, data=Duncan) mse(Duncan$prestige, fitted(m.lm)) #> [1] 166.8155 #> attr(,\"casewise loss\") #> [1] \"(y - yhat)^2\"  data(\"Mroz\", package=\"carData\") m.glm <- glm(lfp ~ ., data=Mroz, family=binomial) BayesRule(Mroz$lfp == \"yes\", fitted(m.glm)) #> [1] 0.3067729 #> attr(,\"casewise loss\") #> [1] \"y != round(yhat)\""},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate a Model-Selection Procedure — cv.function","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"cv() \"function\" method general function cross-validate model-selection procedure, following: selectStepAIC() procedure applies stepAIC() model-selection function MASS package; selectTrans() procedure selecting predictor response transformations regression, uses powerTransform() function car package; selectTransAndStepAIC() combines predictor response transformations predictor selection; selectModelList() uses cross-validation select model list models created models() employs (recursive) cross-validation assess predictive accuracy procedure.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"","code":"# S3 method for `function` cv(   model,   data,   criterion = mse,   k = 10L,   reps = 1L,   seed = NULL,   working.model = NULL,   y.expression = NULL,   confint = n >= 400L,   level = 0.95,   details = k <= 10L,   save.model = FALSE,   ncores = 1L,   ... )  selectStepAIC(   data,   indices,   model,   criterion = mse,   AIC = TRUE,   details = TRUE,   save.model = FALSE,   ... )  selectTrans(   data,   indices,   details = TRUE,   save.model = FALSE,   model,   criterion = mse,   predictors,   response,   family = c(\"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\"),   family.y = c(\"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\"),   rounded = TRUE,   ... )  selectTransStepAIC(   data,   indices,   details = TRUE,   save.model = FALSE,   model,   criterion = mse,   predictors,   response,   family = c(\"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\"),   family.y = c(\"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\"),   rounded = TRUE,   AIC = TRUE,   ... )  selectModelList(   data,   indices,   model,   criterion = mse,   k = 10L,   k.recurse = k,   details = k <= 10L,   save.model = FALSE,   seed = FALSE,   quietly = TRUE,   ... )  compareFolds(object, digits = 3, ...)  # S3 method for cvSelect coef(object, average, NAs = 0, ...)"},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"model regression model object fit data, cv() \"function\" method, model-selection procedure function (see Details). data full data frame model selection. criterion CV criterion (\"cost\" lack--fit) function. k perform k-fold cross-validation (default 10); k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation. reps number times replicate k-fold CV (default 1) seed R's random number generator; used n-fold cross-validation. explicitly set, seed randomly generated saved make results reproducible. cases, internal use , seed set FALSE suppress automatically setting seed. working.model regression model object fit data, typically begin model-selection process; use selectModelList(), list competing models created models(). y.expression normally response variable found model working.model argument; , particular selection procedure, model working.model argument absent, response inferred model, response can specified expression, expression(log(income)), evaluated within data set provided data argument. confint TRUE (default number cases 400 greater), compute confidence interval bias-corrected CV criterion, criterion average casewise components. level confidence level (default 0.95). details TRUE, save detailed information value CV criterion cases fold regression coefficients (possibly information) fold deleted; default TRUE k 10 smaller, FALSE otherwise. save.model save model selected using full data set (default, FALSE). ncores number cores use parallel computations (default 1, .e., computations done parallel) ... cvSelect() cv() \"function\" method, arguments passed procedure(); selectStepAIC() selectTransStepAIC(), arguments passed stepAIC(). indices indices cases data defining current fold. AIC TRUE (default) use AIC model-selection criterion; FALSE, use BIC. k argument stepAIC() set accordingly (note distinct number folds k). predictors character vector names predictors model transform; missing, predictors transformed. response name response variable; missing, response transformed. family transformation family predictors, one \"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\", \"bcPower\" default. names transformation functions car package; see bcPower(). family.y transformation family response, \"bcPower\" default. rounded TRUE (default) use nicely rounded versions estimated transformation parameters (see bcPower()). k.recurse number folds recursive CV; defaults value k; may specified \"loo\" \"n\" well integer. quietly TRUE (default), simple messages (example value random-number generator seed set), warnings errors, suppressed. object object class \"cvSelect\". digits significant digits printing coefficients (default 3). average supplied, function, mean median, use us averaging estimates across folds; missing, estimates fold returned. NAs values substitute NAs calculating averaged estimates; default, 0, appropriate, e.g., regression coefficients; value 1 might appropriate power-transformation estimates.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"object class \"cvSelect\", inheriting class \"cv\", CV criterion (\"CV crit\"), bias-adjusted CV criterion (\"adj CV crit\"), criterion model applied full data (\"full crit\"), confidence interval level bias-adjusted CV criterion (\"confint\"), number folds (\"k\"), seed R's random-number generator (\"seed\"), (optionally) list coefficients (, case selectTrans(), estimated transformation parameters, case selectTransAndStepAIC(), regression coefficients transformation parameters) selected models fold (\"coefficients\"). reps > 1, object class c(\"cvSelectList\", \"cvList\") returned, literally list c(\"cvSelect\", \"cv\") objects.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"model-selection function supplied procedure (cvSelect()) model (cv()) argument accept following arguments: data set data argument cvSelect() cv(). indices indices rows data defining current fold; missing, model-selection procedure applied full data. arguments passed via ... cvSelect() cv(). procedure() model() return list following named elements: fit., vector predicted values cases current fold computed model omitting cases; crit.., CV criterion computed cases using model omitting current fold; (optionally) coefficients, parameter estimates model computed omitting current fold. indices argument missing, procedure() returns cross-validation criterion cases based model fit cases. examples model-selection functions procedure argument, see code selectStepAIC(), selectTrans(), selectTransAndStepAIC(). additional information, see \"Cross-validating model selection\" vignette (vignette(\"cv-select\", package=\"cv\")) \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")).","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"cv(`function`): cv() method applying model model-selection (specification) procedure. selectStepAIC(): select regression model using stepAIC() function MASS package. selectTrans(): select transformations predictors response using powerTransform() car package. selectTransStepAIC(): select transformations predictors response, select predictors. selectModelList(): select model using (recursive) CV. compareFolds(): print coefficients selected models several folds. coef(cvSelect): extract coefficients selected models several folds possibly average .","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cv.function.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate a Model-Selection Procedure — cv.function","text":"","code":"data(\"Auto\", package=\"ISLR2\") m.auto <- lm(mpg ~ . - name - origin, data=Auto) cv(selectStepAIC, Auto, seed=123, working.model=m.auto) #> R RNG seed set to 123 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 11.87795 #> bias-adjusted cross-validation criterion = 11.8753 #> full-sample criterion = 11.65549  cv(selectStepAIC, Auto, seed=123, working.model=m.auto,          AIC=FALSE, k=5, reps=3) # via BIC #> R RNG seed set to 123 #> R RNG seed set to 319729 #> R RNG seed set to 889422 #>  #> Replicate 1: #> 5-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 11.91891 #> bias-adjusted cross-validation criterion = 11.8895 #> full-sample criterion = 11.65549  #>  #> Replicate 2: #> 5-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 11.80601 #> bias-adjusted cross-validation criterion = 11.78946 #> full-sample criterion = 11.65549  #>  #> Replicate 3: #> 5-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 11.81343 #> bias-adjusted cross-validation criterion = 11.79576 #> full-sample criterion = 11.65549  #>  #> Average: #> 5-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 11.86432 (0.06311657) #> bias-adjusted cross-validation criterion = 11.84105 (0.05600268) #> full-sample criterion = 11.65549  data(\"Prestige\", package=\"carData\") m.pres <- lm(prestige ~ income + education + women,              data=Prestige) cvt <- cv(selectTrans, data=Prestige, working.model=m.pres, seed=123,           predictors=c(\"income\", \"education\", \"women\"),           response=\"prestige\", family=\"yjPower\") #> R RNG seed set to 123 cvt #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 58.68193 #> bias-adjusted cross-validation criterion = 58.26258 #> full-sample criterion = 50.60016  compareFolds(cvt) #> CV criterion by folds: #>    fold.1    fold.2    fold.3    fold.4    fold.5    fold.6    fold.7    fold.8  #> 100.01805  27.79099  66.84054  60.97286  30.75663  17.98406 110.27446  66.35537  #>    fold.9   fold.10  #>  37.32475  67.45709  #>  #> Coefficients by folds: #>         lam.education lam.income lam.women lambda #> Fold 1          1.000      0.330     0.330      1 #> Fold 2          1.000      0.330     0.330      1 #> Fold 3          1.000      0.330     0.000      1 #> Fold 4          1.000      0.330     0.330      1 #> Fold 5          1.000      0.330     0.000      1 #> Fold 6          1.000      0.330     0.000      1 #> Fold 7          1.000      0.000     0.000      1 #> Fold 8          1.000      0.330     0.000      1 #> Fold 9          1.000      0.500     0.330      1 #> Fold 10         1.000      0.330     0.159      1 coef(cvt, average=median, NAs=1) # NAs not really needed here #> lam.education    lam.income     lam.women        lambda  #>    1.00000000    0.33000000    0.07948918    1.00000000  cv(m.pres, seed=123) #> R RNG seed set to 123 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 66.91929 #> bias-adjusted cross-validation criterion = 66.45514 #> full-sample criterion = 59.15265  Auto$year <- as.factor(Auto$year) Auto$origin <- factor(Auto$origin,                       labels=c(\"America\", \"Europe\", \"Japan\")) rownames(Auto) <- make.names(Auto$name, unique=TRUE) Auto$name <- NULL m.auto <- lm(mpg ~ . , data=Auto) cvs <- cv(selectTransStepAIC, data=Auto, seed=76692, working.model=m.auto,           criterion=medAbsErr,           predictors=c(\"cylinders\", \"displacement\", \"horsepower\",                        \"weight\", \"acceleration\"),           response=\"mpg\", AIC=FALSE) #> R RNG seed set to 76692 cvs #> 10-Fold Cross Validation #> criterion: medAbsErr #> cross-validation criterion = 1.476272 #> full-sample criterion = 1.339604  compareFolds(cvs) #> CV criterion by folds: #>   fold.1   fold.2   fold.3   fold.4   fold.5   fold.6   fold.7   fold.8  #> 1.563930 1.562867 1.369843 1.282772 1.246066 1.582635 1.340277 1.183112  #>   fold.9  fold.10  #> 1.138892 1.585400  #>  #> Coefficients by folds: #>         (Intercept) horsepower lam.acceleration lam.cylinders lam.displacement #> Fold 1      9.71384   -0.17408          0.50000      -0.50000          0.18555 #> Fold 2      9.09355   -0.31285          0.00000      -0.50000          0.19452 #> Fold 3      9.61824   -0.19248          0.00000      -0.50000          0.15461 #> Fold 4      9.49410   -0.25380          0.00000      -0.50000          0.12800 #> Fold 5      9.14403   -0.14934          0.00000      -0.50000          0.18388 #> Fold 6      9.63481   -0.16739          0.00000      -0.50000          0.17122 #> Fold 7      9.60487   -0.18119          0.00000       0.00000          0.19063 #> Fold 8      8.92286   -0.29237          0.00000      -0.50000          0.33000 #> Fold 9      8.71492   -0.22484          0.00000      -0.50000          0.16881 #> Fold 10     9.61727   -0.17086          0.00000       0.00000          0.16638 #>         lam.horsepower lam.weight   lambda   weight   year71   year72   year73 #> Fold 1         0.00000    0.00000  0.00000 -0.74636  0.03764 -0.00327 -0.02477 #> Fold 2         0.00000    0.00000  0.00000 -0.48573  0.02141 -0.01416 -0.03720 #> Fold 3         0.00000    0.00000  0.00000 -0.72085  0.01128 -0.02569 -0.03872 #> Fold 4         0.00000    0.00000  0.00000 -0.57844  0.02269 -0.02130 -0.05069 #> Fold 5         0.00000    0.00000  0.00000 -0.69081  0.02531 -0.01062 -0.04625 #> Fold 6         0.00000    0.00000  0.00000 -0.74049  0.02456  0.00759 -0.03412 #> Fold 7         0.00000    0.00000  0.00000 -0.72741  0.02341 -0.01438 -0.04241 #> Fold 8         0.00000    0.00000  0.00000 -0.48913  0.02720 -0.01844 -0.05477 #> Fold 9         0.00000    0.00000  0.00000 -0.48188  0.00896 -0.03530 -0.04815 #> Fold 10        0.00000    0.00000  0.00000 -0.73550  0.02937 -0.00899 -0.03814 #>           year74   year75   year76   year77   year78   year79   year80   year81 #> Fold 1   0.05606  0.07080  0.07250  0.14420  0.14281  0.23266  0.35127  0.25635 #> Fold 2   0.04374  0.04010  0.06713  0.13116  0.14857  0.21825  0.33253  0.26173 #> Fold 3   0.05187  0.03837  0.06399  0.11593  0.12601  0.20499  0.32821  0.24478 #> Fold 4   0.04488  0.04238  0.05907  0.12717  0.13902  0.22986  0.33253  0.25407 #> Fold 5   0.05039  0.05596  0.07044  0.13356  0.14724  0.24675  0.33331  0.26938 #> Fold 6   0.06266  0.06940  0.07769  0.14211  0.14647  0.23532  0.34761  0.26737 #> Fold 7   0.04368  0.03327  0.07175  0.12777  0.13816  0.23722  0.33822  0.27453 #> Fold 8   0.04670  0.06595  0.08192  0.13289  0.13899  0.23003  0.33022  0.25909 #> Fold 9   0.01986  0.02981  0.05843  0.10584  0.11625  0.20625  0.31601  0.23350 #> Fold 10  0.05408  0.04881  0.07862  0.14101  0.14313  0.23258  0.35649  0.26214 #>           year82 acceleration displacement cylinders originEurope originJapan #> Fold 1   0.30546                                                              #> Fold 2   0.30876     -0.19017     -0.03233                                    #> Fold 3   0.29204                                                              #> Fold 4   0.28048     -0.14790               -0.30064                          #> Fold 5   0.32594                                          0.06261        0.04 #> Fold 6   0.33062                                                              #> Fold 7   0.30436                                                              #> Fold 8   0.30500     -0.17392     -0.01683                                    #> Fold 9   0.29284     -0.14623     -0.05409                                    #> Fold 10  0.32421                                                              data(\"Duncan\", package=\"carData\") m1 <- lm(prestige ~ income + education, data=Duncan) m2 <- lm(prestige ~ income + education + type, data=Duncan) m3 <- lm(prestige ~ (income + education)*type, data=Duncan) cv(selectModelList, data=Duncan, seed=5962,    working.model=models(m1, m2, m3)) # recursive CV #> R RNG seed set to 5962 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 107.3751 #> bias-adjusted cross-validation criterion = 135.4726 #> full-sample criterion = 113.7905"},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate Regression Models — cv","title":"Cross-Validate Regression Models — cv","text":"cv() parallelized generic k-fold (including n-fold, .e., leave-one-) cross-validation function, default method, specific methods linear generalized-linear models can much computationally efficient, method robust linear models. also cv() methods mixed-effects models, model-selection procedures, several models fit data, documented separately.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate Regression Models — cv","text":"","code":"cv(model, data, criterion, k, reps = 1L, seed, ...)  # S3 method for default cv(   model,   data = insight::get_data(model),   criterion = mse,   k = 10L,   reps = 1L,   seed = NULL,   criterion.name = deparse(substitute(criterion)),   details = k <= 10L,   confint = n >= 400L,   level = 0.95,   ncores = 1L,   type = \"response\",   start = FALSE,   model.function,   ... )  # S3 method for lm cv(   model,   data = insight::get_data(model),   criterion = mse,   k = 10L,   reps = 1L,   seed = NULL,   details = k <= 10L,   confint = n >= 400L,   level = 0.95,   method = c(\"auto\", \"hatvalues\", \"Woodbury\", \"naive\"),   ncores = 1L,   ... )  # S3 method for glm cv(   model,   data = insight::get_data(model),   criterion = mse,   k = 10L,   reps = 1L,   seed = NULL,   details = k <= 10L,   confint = n >= 400L,   level = 0.95,   method = c(\"exact\", \"hatvalues\", \"Woodbury\"),   ncores = 1L,   start = FALSE,   ... )  # S3 method for rlm cv(model, data, criterion, k, reps = 1L, seed, ...)  # S3 method for cv print(x, digits = getOption(\"digits\"), ...)  # S3 method for cvList print(x, ...)  # S3 method for cv as.data.frame(   x,   row.names = NULL,   optional = TRUE,   rows = c(\"cv\", \"folds\"),   columns = c(\"criteria\", \"coefficients\"),   ... )  # S3 method for cvList as.data.frame(x, row.names = NULL, optional = TRUE, ...)  # S3 method for cvDataFrame print(x, digits = getOption(\"digits\") - 2L, ...)  # S3 method for cvDataFrame summary(   object,   formula,   subset = NULL,   fun = mean,   include = c(\"cv\", \"folds\", \"all\"),   ... )"},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate Regression Models — cv","text":"model regression model object (see Details). data data frame model fit (usually necessary). criterion cross-validation criterion (\"cost\" lack--fit) function form f(y, yhat) y observed values response yhat predicted values; default mse (mean-squared error). k perform k-fold cross-validation (default 10); k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation. reps number times replicate k-fold CV (default 1). seed R's random number generator; optional, supplied random seed selected saved; needed n-fold cross-validation. ... match generic; passed predict() default cv() method; passed Tapply() function car package summary.cvDataFrame(). criterion.name character string giving name CV criterion function returned \"cv\" object (usually needed). details TRUE (default number folds k <= 10), save detailed information value CV criterion cases fold regression coefficients fold deleted. confint TRUE (default number cases 400 greater), compute confidence interval bias-corrected CV criterion, criterion average casewise components. level confidence level (default 0.95). ncores number cores use parallel computations (default 1, .e., computations done parallel). type default method, value passed type argument predict(); default type=\"response\", appropriate, e.g., \"glm\" model may recognized ignored predict() methods model classes. start TRUE (default FALSE), start argument update() set vector regression coefficients model fit full data, possibly making CV updates faster, e.g., GLM. model.function regression function, typically new cv() method calls cv.default() via NextMethod(), residing package declared dependency cv package, e.g., nnet::multinom. usually necessary specify model.function make cv.default() work. method computational method apply linear (.e., \"lm\") model generalized linear (.e., \"glm\") model. See Details explanation available options. x \"cv\", \"cvList\", \"cvDataFrame\" object printed coerced data frame. digits significant digits printing, default taken \"digits\" option. row.names optional row names result, defaults NULL. optional match .data.frame() generic function; FALSE (default TRUE), names columns returned data frame, including names coefficients, coerced syntactically correct names. rows rows resulting data frame retain: setting rows=\"cv\" retains rows pertaining overall CV result (marked \"fold 0\" ); setting rows=\"folds\" retains rows pertaining individual folds 1 k; default rows = c(\"cv\", \"folds\"), retains rows. columns columns resulting data frame retain: setting columns=\"critera\" retains columns pertaining CV criteria; setting columns=\"coefficients\" retains columns pertaining model coefficients (broadly construed); default columns = c(\"criteria\", \"coefficients\"), retains ; columns \"model\", \"rep\", \"fold\", present, always retained. object object inheriting \"cvDataFrame\" summarize. formula form .criterion ~ classifying.variable(s) (see examples). subset subsetting expression; default (NULL) subset \"cvDataFrame\" object. fun summary function apply, defaulting mean. include rows \"cvDataFrame\" include summary. One \"cv\" (default), rows representing overall CV results; \"folds\", rows individual folds; \"\", rows (generally sensible).","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate Regression Models — cv","text":"cv() methods return object class \"cv\", CV criterion (\"CV crit\"), bias-adjusted CV criterion (\"adj CV crit\"), criterion model applied full data (\"full crit\"), confidence interval level bias-adjusted CV criterion (\"confint\"), number folds (\"k\"), seed R's random-number generator (\"seed\"). details=TRUE, returned object also include \"details\" component, list two elements: \"criterion\", containing CV criterion computed cases fold; \"coefficients\", regression coefficients computed model fold deleted.  methods may return subset components may add additional information. reps > 1, object class \"cvList\" returned, literally list \"cv\" objects.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validate Regression Models — cv","text":"default cv() method uses update() refit model fold, work appropriate update() predict() methods, default method GetResponse() works GetResponse() method supplied. \"lm\" \"glm\" methods can use much faster computational algorithms, selected method argument. linear-model method accommodates weighted linear models. classes models, leave-one-(n-fold) case, fitted values folds can computed hat-values via method=\"hatvalues\" without refitting model; GLMs, method approximate, LMs exact. classes models, one case omitted fold, fitted values may obtained without refitting model exploiting Woodbury matrix identity via method=\"Woodbury\". hatvalues, method exact LMs approximate GLMs. default linear models method=\"auto\", equivalent method=\"hatvalues\" n-fold cross-validation method=\"Woodbury\" otherwise; method=\"naive\" refits model via update() generally much slower. default generalized linear models method=\"exact\", employs update(). default conservative, usually safe use method=\"hatvalues\" n-fold CV method=\"Woodbury\" k-fold CV. also method robust linear models fit rlm() MASS package (avoid inheriting \"lm\" method default \"auto\" computational method inappropriate). additional details, see \"Cross-validating regression models\" vignette (vignette(\"cv\", package=\"cv\")). cv() designed extensible classes regression models; see \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")).","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"methods-by-class-","dir":"Reference","previous_headings":"","what":"Methods (by class)","title":"Cross-Validate Regression Models — cv","text":"cv(default): \"default\" method. cv(lm): \"lm\" method. cv(glm): \"glm\" method. cv(rlm): \"rlm\" method (avoid inheriting \"lm\" method).","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"methods-by-generic-","dir":"Reference","previous_headings":"","what":"Methods (by generic)","title":"Cross-Validate Regression Models — cv","text":"print(cv): print() method \"cv\" objects. .data.frame(cv): .data.frame() method \"cv\" objects.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate Regression Models — cv","text":"print(cvList): print() method \"cvList\" objects. .data.frame(cvList): .data.frame() method \"cvList\" objects. print(cvDataFrame): print() method \"cvDataFrame\" objects. summary(cvDataFrame): summary() method \"cvDataFrame\" objects.","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate Regression Models — cv","text":"","code":"data(\"Auto\", package=\"ISLR2\") m.auto <- lm(mpg ~ horsepower, data=Auto) cv(m.auto,  k=\"loo\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: mse #> cross-validation criterion = 24.23151 (cv.auto <- cv(m.auto, seed=1234)) #> R RNG seed set to 1234 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.3794 #> bias-adjusted cross-validation criterion = 24.35646 #> full-sample criterion = 23.94366  compareFolds(cv.auto) #> CV criterion by folds: #>   fold.1   fold.2   fold.3   fold.4   fold.5   fold.6   fold.7   fold.8  #> 24.31128 18.54939 33.52760 22.09623 25.09248 19.56235 29.05875 16.84553  #>   fold.9  fold.10  #> 19.62079 35.28078  #>  #> Coefficients by folds: #>         (Intercept) horsepower #> Fold 1         39.9      -0.16 #> Fold 2         40.3      -0.16 #> Fold 3         39.9      -0.16 #> Fold 4         40.1      -0.16 #> Fold 5         40.0      -0.16 #> Fold 6         39.4      -0.15 #> Fold 7         40.2      -0.16 #> Fold 8         40.1      -0.16 #> Fold 9         39.6      -0.16 #> Fold 10        39.9      -0.16 (cv.auto.reps <- cv(m.auto, seed=1234, reps=3)) #> R RNG seed set to 1234 #> R RNG seed set to 469908 #> R RNG seed set to 267 #>  #> Replicate 1: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.28572 #> bias-adjusted cross-validation criterion = 24.26775 #> full-sample criterion = 23.94366  #>  #> Replicate 2: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.30002 #> bias-adjusted cross-validation criterion = 24.28129 #> full-sample criterion = 23.94366  #>  #> Replicate 3: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.3794 #> bias-adjusted cross-validation criterion = 24.35646 #> full-sample criterion = 23.94366  #>  #> Average: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.31271 (0.04496328) #> bias-adjusted cross-validation criterion = 24.29331 (0.0425791) #> full-sample criterion = 23.94366  D.auto.reps <- as.data.frame(cv.auto.reps) head(D.auto.reps) #>   rep fold    mse adjusted.mse full.mse coef.Intercept coef.horsepower #> 1   1    0 24.286       24.268   23.944         39.936        -0.15784 #> 2   1    1 23.069           NA       NA         39.700        -0.15576 #> 3   1    2 15.671           NA       NA         40.165        -0.15871 #> 4   1    3 25.502           NA       NA         39.694        -0.15634 #> 5   1    4 24.518           NA       NA         40.507        -0.16337 #> 6   1    5 24.540           NA       NA         39.997        -0.15796 summary(D.auto.reps, mse ~ rep + fold, include=\"folds\") #>    fold #> rep        1        2        3        4        5        6        7        8 #>   1 23.06906 15.67122 25.50203 24.51830 24.54048 27.90666 19.74447 36.27542 #>   2 28.73251 21.92554 22.25392 23.22799 33.15260 24.33338 24.81161 16.17814 #>   3 24.31128 18.54939 33.52760 22.09623 25.09248 19.56235 29.05875 16.84553 #>    fold #> rep        9       10 #>   1 19.43045 26.45118 #>   2 20.69013 27.64165 #>   3 19.62079 35.28078 summary(D.auto.reps, mse ~ rep + fold, include = \"folds\",         subset = fold <= 5) # first 5 folds #>    fold #> rep        1        2        3        4        5 #>   1 23.06906 15.67122 25.50203 24.51830 24.54048 #>   2 28.73251 21.92554 22.25392 23.22799 33.15260 #>   3 24.31128 18.54939 33.52760 22.09623 25.09248 summary(D.auto.reps, mse ~ rep, include=\"folds\") #>        1        2        3  #> 24.31093 24.29475 24.39452  summary(D.auto.reps, mse ~ rep, fun=sd, include=\"folds\") #>        1        2        3  #> 5.607849 4.707138 6.383539   data(\"Mroz\", package=\"carData\") m.mroz <- glm(lfp ~ ., data=Mroz, family=binomial) cv(m.mroz, criterion=BayesRule, seed=123) #> R RNG seed set to 123 #> 10-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.3320053 #> bias-adjusted cross-validation criterion = 0.3260248 #> 95% CI for bias-adjusted CV criterion = (0.292366, 0.3596836) #> full-sample criterion = 0.3067729   data(\"Duncan\", package=\"carData\") m.lm <- lm(prestige ~ income + education, data=Duncan) m.rlm <- MASS::rlm(prestige ~ income + education,                    data=Duncan) cv(m.lm, k=\"loo\", method=\"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 198.5274 #> bias-adjusted cross-validation criterion = 198.1496 #> full-sample criterion = 166.8155  cv(m.rlm, k=\"loo\") #> n-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 193.3949 #> bias-adjusted cross-validation criterion = 192.9083 #> full-sample criterion = 169.7487"},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate Mixed-Effects Model — cv.merMod","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"cv() methods mixed-effect models class \"merMod\", fit lmer() glmer() functions lme4 package; models class \"lme\" fit lme() function nlme package; models class \"glmmTMB\" fit glmmTMB() function glmmTMB package.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"","code":"# S3 method for merMod cv(   model,   data = insight::get_data(model),   criterion = mse,   k = NULL,   reps = 1L,   seed,   details = NULL,   ncores = 1L,   clusterVariables,   blups = coef,   fixed.effects = lme4::fixef,   ... )  # S3 method for lme cv(   model,   data = insight::get_data(model),   criterion = mse,   k = NULL,   reps = 1L,   seed,   details = NULL,   ncores = 1L,   clusterVariables,   blups = coef,   fixed.effects = nlme::fixef,   ... )  # S3 method for glmmTMB cv(   model,   data = insight::get_data(model),   criterion = mse,   k = NULL,   reps = 1L,   seed,   details = NULL,   ncores = 1L,   clusterVariables,   blups = coef,   fixed.effects = glmmTMB::fixef,   ... )"},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"model mixed-effects model object cv() method available. data data frame model fit (usually necessary) criterion cross-validation (\"cost\" lack--fit) criterion function form f(y, yhat) y observed values response yhat predicted values; default mse (mean-squared error). k perform k-fold cross-validation; k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation; default 10 cross-validating individual cases \"loo\" cross-validating clusters. reps number times replicate k-fold CV (default 1), greater), compute confidence interval bias-corrected CV criterion, criterion average casewise components. seed R's random number generator; optional, supplied random seed selected saved; needed n-fold cross-validation details TRUE (default number folds k <= 10), save detailed information value CV criterion cases fold regression coefficients fold deleted. ncores number cores use parallel computations (default 1, .e., computations done parallel) clusterVariables character vector names variables defining clusters mixed model nested crossed random effects; missing, cross-validation performed individual cases rather clusters blups function used compute BLUPs case-based CV details = TRUE. fixed.effects function used compute fixed-effect coefficients cluster-based CV details = TRUE. ... cv() methods, match generic, cvMixed(), arguments passed update().","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"methods cv.merMod(), cv.lme(), cv.glmmTMB(), return objects class \"cv\", , reps > 1, class \"cvList\" (see cv()).","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"mixed-effects models, cross-validation can done \"clusters\" individual observations. former, predictions based fixed effects; latter, predictions include random effects (.e., best linear unbiased predictors \"BLUPS\").","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"cv(merMod): cv() method lmer() glmer() models lme4 package. cv(lme): cv() method lme() models nlme package. cv(glmmTMB): cv() method glmmTMB() models glmmTMB package.","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cv.merMod.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate Mixed-Effects Model — cv.merMod","text":"","code":"library(\"lme4\") # from ?lmer: (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: Reaction ~ Days + (Days | Subject) #>    Data: sleepstudy #> REML criterion at convergence: 1743.628 #> Random effects: #>  Groups   Name        Std.Dev. Corr #>  Subject  (Intercept) 24.741        #>           Days         5.922   0.07 #>  Residual             25.592        #> Number of obs: 180, groups:  Subject, 18 #> Fixed Effects: #> (Intercept)         Days   #>      251.41        10.47   cv(fm1, clusterVariables=\"Subject\") # LOO CV of clusters #> n-Fold Cross Validation based on 18 {Subject} clusters #> criterion: mse #> cross-validation criterion = 2460.604 #> bias-adjusted cross-validation criterion = 2454.627 #> full-sample criterion = 2251.398  cv(fm1, seed=447) # 10-fold CV of cases #> R RNG seed set to 447 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 869.533 #> bias-adjusted cross-validation criterion = 847.5586 #> full-sample criterion = 549.342  cv(fm1, clusterVariables=\"Subject\", k=5,    seed=834, reps=3) # 5-fold CV of clusters, repeated 3 times #> R RNG seed set to 834 #> R RNG seed set to 448690 #> R RNG seed set to 916534 #>  #> Replicate 1: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> criterion: mse #> cross-validation criterion = 2458.192 #> bias-adjusted cross-validation criterion = 2434.117 #> full-sample criterion = 2251.398  #>  #> Replicate 2: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> criterion: mse #> cross-validation criterion = 2511.651 #> bias-adjusted cross-validation criterion = 2479.989 #> full-sample criterion = 2251.398  #>  #> Replicate 3: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> criterion: mse #> cross-validation criterion = 2403.755 #> bias-adjusted cross-validation criterion = 2387.049 #> full-sample criterion = 2251.398  #>  #> Average: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> criterion: mse #> cross-validation criterion = 2457.947 (44.04918) #> bias-adjusted cross-validation criterion = 2433.818 (37.94423) #> full-sample criterion = 2251.398   library(\"nlme\") #>  #> Attaching package: ‘nlme’ #> The following object is masked from ‘package:lme4’: #>  #>     lmList # from ?lme (fm2 <- lme(distance ~ age + Sex, data = Orthodont,             random = ~ 1)) #> Linear mixed-effects model fit by REML #>   Data: Orthodont  #>   Log-restricted-likelihood: -218.7563 #>   Fixed: distance ~ age + Sex  #> (Intercept)         age   SexFemale  #>  17.7067130   0.6601852  -2.3210227  #>  #> Random effects: #>  Formula: ~1 | Subject #>         (Intercept) Residual #> StdDev:    1.807425 1.431592 #>  #> Number of Observations: 108 #> Number of Groups: 27  cv(fm2) # LOO CV of cases #> R RNG seed set to 765199 #> 10-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 2.666103 #> bias-adjusted cross-validation criterion = 2.589541 #> full-sample criterion = 1.582435  cv(fm2, clusterVariables=\"Subject\", k=5, seed=321) # 5-fold CV of clusters #> R RNG seed set to 321 #> 5-Fold Cross Validation based on 27 {Subject} clusters #> criterion: mse #> cross-validation criterion = 5.875411 #> bias-adjusted cross-validation criterion = 5.780695 #> full-sample criterion = 5.017326   library(\"glmmTMB\") # from ?glmmTMB (m1 <- glmmTMB(count ~ mined + (1|site),                zi=~mined,                family=poisson, data=Salamanders)) #> Formula:          count ~ mined + (1 | site) #> Zero inflation:         ~mined #> Data: Salamanders #>       AIC       BIC    logLik  df.resid  #> 1908.4695 1930.8080 -949.2348       639  #> Random-effects (co)variances: #>  #> Conditional model: #>  Groups Name        Std.Dev. #>  site   (Intercept) 0.28     #>  #> Number of obs: 644 / Conditional model: site, 23 #>  #> Fixed Effects: #>  #> Conditional model: #> (Intercept)      minedno   #>      0.0879       1.1419   #>  #> Zero-inflation model: #> (Intercept)      minedno   #>       1.139       -1.736   cv(m1, seed=97816, k=5, clusterVariables=\"site\") # 5-fold CV of clusters #> R RNG seed set to 97816 #> 5-Fold Cross Validation based on 23 {site} clusters #> criterion: mse #> cross-validation criterion = 6.006117 #> bias-adjusted cross-validation criterion = 6.002191 #> 95% CI for bias-adjusted CV criterion = (2.385452, 9.61893) #> full-sample criterion = 5.970489  cv(m1, seed=34506, k=5) # 5-fold CV of cases #> R RNG seed set to 34506 #> 5-Fold Cross Validation #> criterion: mse #> cross-validation criterion = 6.058988 #> bias-adjusted cross-validation criterion = 6.032337 #> 95% CI for bias-adjusted CV criterion = (2.483579, 9.581094) #> full-sample criterion = 5.79783"},{"path":"https://gmonette.github.io/cv/reference/cv.modList.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate Several Models Fit to the Same Data — cv.modList","title":"Cross-Validate Several Models Fit to the Same Data — cv.modList","text":"cv() method object class  \"modlist\", created models() function. cv() method simplifies process cross-validating several models set CV folds may also used recursive CV, CV used select one among several models. models() performs \"sanity\" checks, warning models different classes, reporting error fit apparently different data sets different response variables.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.modList.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate Several Models Fit to the Same Data — cv.modList","text":"","code":"# S3 method for modList cv(   model,   data,   criterion = mse,   k,   reps = 1L,   seed,   quietly = TRUE,   recursive = FALSE,   ... )  models(...)  # S3 method for cvModList print(x, ...)  # S3 method for cvModList plot(   x,   y,   spread = c(\"range\", \"sd\"),   confint = TRUE,   xlab = \"\",   ylab,   main,   axis.args = list(labels = names(x), las = 3L),   col = palette()[2L],   lwd = 2L,   grid = TRUE,   ... )  # S3 method for cvModList as.data.frame(x, row.names = NULL, optional = TRUE, ...)"},{"path":"https://gmonette.github.io/cv/reference/cv.modList.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate Several Models Fit to the Same Data — cv.modList","text":"model list regression model objects, created models(). data (required) data set models fit. criterion CV criterion (\"cost\" lack--fit) function, defaults mse. k number CV folds; may omitted, case value depend default cv() method invoked individual models. reps number replications CV model (default 1). seed (optional) seed R's pseudo-random-number generator, used create set CV folds models; omitted, seed randomly generated saved. used leave-one-CV. quietly TRUE (default), simple messages (example value random-number generator seed set), warnings errors, suppressed. recursive TRUE (default FALSE), cross-validation performed recursively select \"best\" model deleting fold turn calculating CV estimate criterion remaining folds; equivalent employing selectModelList() model-selection procedure. ... cv.modList(), additional arguments passed cv() method applied model. models(), two competing models fit data; several models may named. print() method, arguments passed print() method individual model cross-validations. plot() method, arguments passed base plot() function. x object class \"cvModList\" printed plotted. y name element \"cv\" object plotted; defaults \"adj CV crit\", exists, \"CV crit\". spread \"range\", default, show range CV criteria model along average; \"sd\", show average plus minus 1 standard deviation. confint TRUE (default) confidence intervals \"cv\" objects, plot confidence intervals around CV criteria. xlab label x-axis (defaults blank). ylab label y-axis (missing, label constructed). main main title graph (missing, label constructed). axis.args list arguments axis() function, used draw horizontal axis. addition axis arguments given explicitly, side=1 (horizontal axis) =seq(along=x) (.e., 1 number models) used modified. col color line points, defaults second element color palette; see palette(). lwd line width line (defaults 2). grid TRUE (default), include grid lines graph. row.names optional row names result, defaults NULL. optional match .data.frame() generic function; FALSE (default TRUE), names columns returned data frame, including names coefficients, coerced syntactically correct names.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.modList.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate Several Models Fit to the Same Data — cv.modList","text":"models() returns \"modList\" object, cv() method returns \"cvModList\" object, , recursive=TRUE, object class c(\"cvSelect\", \"cv\").","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.modList.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate Several Models Fit to the Same Data — cv.modList","text":"cv(modList): cv() method \"modList\" objects. models(): create list models. print(cvModList): print() method \"cvModList\" objects. plot(cvModList): plot() method \"cvModList\" objects. .data.frame(cvModList): .data.frame() method \"cvModList\" objects.","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cv.modList.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate Several Models Fit to the Same Data — cv.modList","text":"","code":"data(\"Duncan\", package=\"carData\") m1 <- lm(prestige ~ income + education, data=Duncan) m2 <- lm(prestige ~ income + education + type, data=Duncan) m3 <- lm(prestige ~ (income + education)*type, data=Duncan) (cv.models <- cv(models(m1=m1, m2=m2, m3=m3),                  data=Duncan, seed=7949, reps=5)) #>  #> Model m1 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 195.0866 (2.307011) #> bias-adjusted cross-validation criterion = 193.5434 (2.195082) #> full-sample criterion = 166.8155  #>  #> Model m2 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 111.7281 (4.908514) #> bias-adjusted cross-validation criterion = 110.1813 (4.623137) #> full-sample criterion = 84.39899  #>  #> Model m3 averaged across 5 replications (with SDs): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 142.3619 (5.682756) #> bias-adjusted cross-validation criterion = 137.932 (5.264929) #> full-sample criterion = 74.45878  D.cv.models <- as.data.frame(cv.models) head(D.cv.models) #>   model rep fold criterion adjusted.criterion full.criterion coef.Intercept #> 1    m1   1    0   198.534             196.89         166.82        -6.0647 #> 2    m1   1    1   139.227                 NA             NA        -5.3264 #> 3    m1   1    2   328.311                 NA             NA        -7.4825 #> 4    m1   1    3    87.649                 NA             NA        -5.1543 #> 5    m1   1    4   133.564                 NA             NA        -5.9934 #> 6    m1   1    5    55.317                 NA             NA        -5.4389 #>   coef.income coef.education coef.typeprof coef.typewc coef.income:typeprof #> 1     0.59873        0.54583            NA          NA                   NA #> 2     0.57458        0.55107            NA          NA                   NA #> 3     0.53963        0.59419            NA          NA                   NA #> 4     0.60090        0.53586            NA          NA                   NA #> 5     0.54033        0.59515            NA          NA                   NA #> 6     0.57504        0.54903            NA          NA                   NA #>   coef.income:typewc coef.education:typeprof coef.education:typewc #> 1                 NA                      NA                    NA #> 2                 NA                      NA                    NA #> 3                 NA                      NA                    NA #> 4                 NA                      NA                    NA #> 5                 NA                      NA                    NA #> 6                 NA                      NA                    NA summary(D.cv.models, criterion ~ model + rep, include=\"folds\") #>      rep #> model        1        2        3        4        5 #>    m1 204.7489 197.8415 192.9138 199.5759 200.2259 #>    m2 110.5324 109.8870 121.7147 111.1913 120.0575 #>    m3 141.9428 145.7092 143.0177 143.1847 155.0245 plot(cv.models)  (cv.models.ci <- cv(models(m1=m1, m2=m2, m3=m3),                     data=Duncan, seed=5962, confint=TRUE, level=0.50)) #>  #> Model m1: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 196.953 #> bias-adjusted cross-validation criterion = 195.2606 #> 50% CI for bias-adjusted CV criterion = (162.0119, 228.5093) #> full-sample criterion = 166.8155  #>  #> Model m2: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 107.3751 #> bias-adjusted cross-validation criterion = 106.081 #> 50% CI for bias-adjusted CV criterion = (84.96181, 127.2003) #> full-sample criterion = 84.39899  #>  #> Model m3: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 141.2562 #> bias-adjusted cross-validation criterion = 137.0925 #> 50% CI for bias-adjusted CV criterion = (108.4551, 165.7299) #> full-sample criterion = 74.45878                   # nb: n too small for accurate CIs plot(cv.models.ci)  (cv.models.recursive <- cv(models(m1=m1, m2=m2, m3=m3),                            data=Duncan, seed=5962,                            recursive=TRUE, save.model=TRUE)) #> R RNG seed set to 5962 #> 10-Fold Cross Validation #> cross-validation criterion = 107.3751 #> bias-adjusted cross-validation criterion = 135.4726 #> full-sample criterion = 113.7905  cv.models.recursive$selected.model #>  #> Call: #> lm(formula = prestige ~ income + education + type, data = Duncan) #>  #> Coefficients: #> (Intercept)       income    education     typeprof       typewc   #>     -0.1850       0.5975       0.3453      16.6575     -14.6611   #>"},{"path":"https://gmonette.github.io/cv/reference/cvCompute.html","id":null,"dir":"Reference","previous_headings":"","what":"Utility Functions for the cv Package — cvCompute","title":"Utility Functions for the cv Package — cvCompute","text":"functions primarily useful writing methods cv() generic function. used internally package can also used extensions (see vignette \"Extending cv package, vignette(\"cv-extend\", package=\"cv\")).","code":""},{"path":"https://gmonette.github.io/cv/reference/cvCompute.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Utility Functions for the cv Package — cvCompute","text":"","code":"cvCompute(   model,   data = insight::get_data(model),   criterion = mse,   criterion.name,   k = 10L,   reps = 1L,   seed,   details = k <= 10L,   confint,   level = 0.95,   method = NULL,   ncores = 1L,   type = \"response\",   start = FALSE,   f,   fPara = f,   locals = list(),   model.function = NULL,   model.function.name = NULL,   ... )  cvMixed(   model,   package,   data = insight::get_data(model),   criterion = mse,   criterion.name,   k,   reps = 1L,   confint,   level = 0.95,   seed,   details,   ncores = 1L,   clusterVariables,   predict.clusters.args = list(object = model, newdata = data),   predict.cases.args = list(object = model, newdata = data),   blups,   fixed.effects,   ... )  cvSelect(   procedure,   data,   criterion = mse,   criterion.name,   model,   y.expression,   k = 10L,   confint = n >= 400,   level = 0.95,   reps = 1L,   save.coef,   details = k <= 10L,   save.model = FALSE,   seed,   ncores = 1L,   ... )  folds(n, k)  fold(folds, i, ...)  # S3 method for folds fold(folds, i, ...)  # S3 method for folds print(x, ...)  GetResponse(model, ...)  # S3 method for default GetResponse(model, ...)  # S3 method for merMod GetResponse(model, ...)  # S3 method for lme GetResponse(model, ...)  # S3 method for glmmTMB GetResponse(model, ...)  # S3 method for modList GetResponse(model, ...)"},{"path":"https://gmonette.github.io/cv/reference/cvCompute.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Utility Functions for the cv Package — cvCompute","text":"model regression model object. data data frame model fit (usually necessary, except cvSelect()). criterion cross-validation criterion (\"cost\" lack--fit) function form f(y, yhat) y observed values response yhat predicted values; default mse (mean-squared error). criterion.name character string giving name CV criterion function returned \"cv\" object). k perform k-fold cross-validation (default 10); k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation; folds(), k must number. reps number times replicate k-fold CV (default 1). seed R's random number generator; optional, supplied random seed selected saved; needed n-fold cross-validation. details TRUE (default number folds k <= 10), save detailed information value CV criterion cases fold regression coefficients fold deleted. confint TRUE (default number cases 400 greater), compute confidence interval bias-corrected CV criterion, criterion average casewise components. level confidence level (default 0.95). method computational method apply; use cv() methods. ncores number cores use parallel computations (default 1, .e., computations done parallel). type used cv() methods, default method, type passed type argument predict(); default type=\"response\", appropriate, e.g., \"glm\" model may recognized ignored predict() methods model classes. start used cv() methods; TRUE (default FALSE), start argument, set vector regression coefficients model fit full data, passed update(), possibly making CV updates faster, e.g. GLM. f function called cvCompute() fold. fPara function called cvCompute() fold using parallel computation. locals named list objects required local environment cvCompute() f() fPara(). model.function regression function, typically new cv() method, residing package declared dependency cv package, e.g., nnet::multinom. model.function.name quoted name regression function, e.g., \"multinom\". ... match generic; passed predict() default method, fPara() (parallel computations) cvCompute(). package name package mixed-modeling function (functions) employed resides; used get namespace package. clusterVariables character vector names variables defining clusters mixed model nested crossed random effects; missing, cross-validation performed individual cases rather clusters predict.clusters.args list arguments used predict whole data set mixed model performing CV clusters; first two elements model newdata; see \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")). predict.cases.args list arguments used predict whole data set mixed model performing CV cases; first two elements model newdata; see \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")). blups function used compute BLUPs case-based CV details = TRUE. fixed.effects function used compute fixed-effect coefficients cluster-based CV details = TRUE. procedure model-selection procedure function (see Details). y.expression normally response variable found model argument; , particular selection procedure, model argument absent, response inferred model, response can specified expression, expression(log(income)), evaluated within data set provided data argument. save.coef save coefficients selected models? Deprecated favor details argument; specified, details set set value save.coef. save.model save model selected using full data set. n number cases, constructed folds. folds object class \"folds\". fold number object class \"folds\". x \"cv\", \"cvList\", \"folds\" object printed","code":""},{"path":"https://gmonette.github.io/cv/reference/cvCompute.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Utility Functions for the cv Package — cvCompute","text":"utility functions return various kinds objects: cvCompute() returns object class \"cv\", CV criterion (\"CV crit\"), bias-adjusted CV criterion (\"adj CV crit\"), criterion model applied full data (\"full crit\"), confidence interval level bias-adjusted CV criterion (\"confint\"), number folds (\"k\"), seed R's random-number generator (\"seed\"). details=TRUE, returned object also include \"details\" component, list two elements: \"criterion\", containing CV criterion computed cases fold; \"coefficients\", regression coefficients computed model fold deleted.  cv() methods calling cvCompute() may return subset components may add additional information. reps > 1, object class \"cvList\" returned, literally list \"cv\" objects. cvMixed() also returns object class \"cv\" \"cvList\". cvSelect returns object class \"cvSelect\" inheriting \"cv\", object class \"cvSelectList\" inheriting \"cvList\". folds() returns object class folds, fold() print() methods. GetResponse() returns (numeric) response variable model. supplied default method returns model$y component model object, , model S4 object, result returned get_response() function insight package. result NULL, result model.response(model.frame(model)) returned, checking case whether result numeric vector. also  \"lme\", \"merMod\" \"glmmTMB\" methods convert factor responses numeric 0/1 responses, appropriate generalized linear mixed model binary response.","code":""},{"path":"https://gmonette.github.io/cv/reference/cvCompute.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Utility Functions for the cv Package — cvCompute","text":"cvCompute(): used internally cv() methods (direct use); exported support new cv() methods. cvMixed(): used internally cv() methods mixed-effect models (direct use); exported support new cv() methods. cvSelect(): used internally cv() methods cross-validating model-selection procedure; may also called directly purpose, use via cv() preferred. cvSelect() exported primarily support new model-selection procedures. folds(): used internally cv() methods (direct use). fold(): extract fold \"folds\" object. fold(folds): fold() method \"folds\" objects. print(folds): print() method \"folds\" objects. GetResponse(): function return response variable regression model. GetResponse(default): default method. GetResponse(merMod): \"merMod\" method. GetResponse(lme): \"lme\" method. GetResponse(glmmTMB): \"glmmTMB\" method. GetResponse(modList): \"modList\" method.","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cvCompute.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Utility Functions for the cv Package — cvCompute","text":"","code":"fit <- lm(mpg ~ gear, mtcars) GetResponse(fit) #>           Mazda RX4       Mazda RX4 Wag          Datsun 710      Hornet 4 Drive  #>                21.0                21.0                22.8                21.4  #>   Hornet Sportabout             Valiant          Duster 360           Merc 240D  #>                18.7                18.1                14.3                24.4  #>            Merc 230            Merc 280           Merc 280C          Merc 450SE  #>                22.8                19.2                17.8                16.4  #>          Merc 450SL         Merc 450SLC  Cadillac Fleetwood Lincoln Continental  #>                17.3                15.2                10.4                10.4  #>   Chrysler Imperial            Fiat 128         Honda Civic      Toyota Corolla  #>                14.7                32.4                30.4                33.9  #>       Toyota Corona    Dodge Challenger         AMC Javelin          Camaro Z28  #>                21.5                15.5                15.2                13.3  #>    Pontiac Firebird           Fiat X1-9       Porsche 914-2        Lotus Europa  #>                19.2                27.3                26.0                30.4  #>      Ford Pantera L        Ferrari Dino       Maserati Bora          Volvo 142E  #>                15.8                19.7                15.0                21.4   set.seed(123) (ffs <- folds(n=22, k=5)) #> 5 folds of approximately 4 cases each #>  fold 1: 15 19 14 3 10 #>  fold 2: 11 5 4 20 6 #>  fold 3: 9 18 16 21 #>  fold 4: 12 1 22 7 #>  fold 5: 17 13 8 2 fold(ffs, 2) #> [1] 11  5  4 20  6"},{"path":"https://gmonette.github.io/cv/news/index.html","id":"cv-201","dir":"Changelog","previous_headings":"","what":"cv 2.0.1","title":"cv 2.0.1","text":"Small fix docs.","code":""},{"path":"https://gmonette.github.io/cv/news/index.html","id":"cv-200","dir":"Changelog","previous_headings":"","what":"cv 2.0.0","title":"cv 2.0.0","text":"CRAN release: 2024-04-29 New cv.function() method meant replace cvSelect(), direct use now discouraged. New selectModelList() used cv.function() (cvSelect()). selectModelList() implements recursive cross-validation, fit model selected CV assessed CV. procedure also available setting recursive=TRUE call cv.modList(). cv.default() cv() methods acquire details argument, TRUE includes information folds returned object. New .data.frame.cv() related methods turning detailed results returned cv() methods data frame, new print() summary() methods objects produced. Improvements code, introducing folds(), fold(), related functions. Refactoring code; cv() methods now call cvCompute() (new), cvMixed(), cvSelect(). Reorganization package file structure documentation. Make cv.default() method robust, particularly parallel computations. Reorganize package vignettes (now 5). small improvements.","code":""},{"path":"https://gmonette.github.io/cv/news/index.html","id":"cv-110","dir":"Changelog","previous_headings":"","what":"cv 1.1.0","title":"cv 1.1.0","text":"CRAN release: 2024-01-27 cv() et al. now work properly “non-casewise average” CV criteria new rmse() medAbsErr(), just “casewise-average” fit criteria mse() BayesRule(). Bias adjustment confidence intervals (new) computed casewise-average CV criteria. Demonstrate 1 - AUC isn’t casewise-average criterion. Generally suppress spurious messages setting seed cv.modList() LOO CV. Fix bugs selectTrans() caused errors one response predictors arguments specified. Fix bug cvMixed() prevented parallel computations (reported Craig See). Fix small bug cvSelect(), returning properly named “coefficients” element save.coef TRUE. Fix bug cv.lm() cv.glm() method=“hatvalues” cost criteria mse(). Add selectTransStepAIC() procedure use cvSelect(). Add medAbsErr() rmse() cost criteria. Add coef.cvSelect() method. Add cv.rlm() method. plot.cvModList() can show averages +/- SDs, averages CIs, well averages ranges. Add Pigs data set. change getResponse() methods GetResponse() avoid name clash nlme. Improvements updates documentation, expanded cv.Rmd vignette. Mixed-models methods longer flagged “experimental.” Mixed-models CV functions longer limited nested random effects.","code":""},{"path":"https://gmonette.github.io/cv/news/index.html","id":"cv-101","dir":"Changelog","previous_headings":"","what":"cv 1.0.1","title":"cv 1.0.1","text":"CRAN release: 2023-10-31 Initial CRAN version.","code":""},{"path":"https://gmonette.github.io/cv/news/index.html","id":"cv-010","dir":"Changelog","previous_headings":"","what":"cv 0.1.0","title":"cv 0.1.0","text":"Initial version.","code":""}]
