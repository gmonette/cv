<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Cross-validating model selection • cv</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Cross-validating model selection">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">cv</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">2.0.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/cv.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/cv-extend.html">Extending the cv package</a></li>
    <li><a class="dropdown-item" href="../articles/cv-mixed.html">Cross-validating mixed-effects models</a></li>
    <li><a class="dropdown-item" href="../articles/cv-notes.html">Computational and technical notes on cross-validating regression models</a></li>
    <li><a class="dropdown-item" href="../articles/cv-selection.html">Cross-validating model selection</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/gmonette/cv/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Cross-validating model selection</h1>
                        <h4 data-toc-skip class="author">John Fox and
Georges Monette</h4>
            
            <h4 data-toc-skip class="date">2024-07-22</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/gmonette/cv/blob/main/vignettes/cv-selection.Rmd" class="external-link"><code>vignettes/cv-selection.Rmd</code></a></small>
      <div class="d-none name"><code>cv-selection.Rmd</code></div>
    </div>

    
    
<p>As <span class="citation">Hastie, Tibshirani, &amp; Friedman (2009,
sec. 7.10.2: “The Wrong and Right Way to Do Cross-validation”)</span>
explain, if the whole data are used to select or fine-tune a statistical
model, subsequent cross-validation of the model is intrinsically
misleading, because the model is selected to fit the whole data,
including the part of the data that remains when each fold is
removed.</p>
<div class="section level2">
<h2 id="a-preliminary-example">A preliminary example<a class="anchor" aria-label="anchor" href="#a-preliminary-example"></a>
</h2>
<p>The following example is similar in spirit to one employed by <span class="citation">Hastie et al. (2009)</span>. Suppose that we randomly
generate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">n = 1000</annotation></semantics></math>
independent observations for a response variable variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>=</mo><mn>10</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y \sim N(\mu = 10, \sigma^2 = 0)</annotation></semantics></math>,
and independently sample
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math>
observations for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">p = 100</annotation></semantics></math>
“predictors,”
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>x</mi><mn>100</mn></msub></mrow><annotation encoding="application/x-tex">x_1, \ldots, x_{100}</annotation></semantics></math>,
each from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub><mo>∼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x_j \sim N(0, 1)</annotation></semantics></math>.
The response has nothing to do with the predictors and so the population
linear-regression model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>α</mi><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><msub><mi>β</mi><mn>100</mn></msub><msub><mi>x</mi><mrow><mi>i</mi><mo>,</mo><mn>100</mn></mrow></msub><mo>+</mo><msub><mi>ε</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_{100} x_{i,100} + \varepsilon_i</annotation></semantics></math>
has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\alpha = 10</annotation></semantics></math>
and all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_j = 0</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">24361</span><span class="op">)</span> <span class="co"># for reproducibility</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean <span class="op">=</span> <span class="fl">10</span><span class="op">)</span>,</span>
<span>                X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span> <span class="op">*</span> <span class="fl">100</span><span class="op">)</span>, <span class="fl">1000</span>, <span class="fl">100</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">D</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;         y      X.1      X.2      X.3       X.4       X.5</span></span>
<span><span class="co">#&gt; 1 10.0316 -1.23886 -0.26487 -0.03539 -2.576973  0.811048</span></span>
<span><span class="co">#&gt; 2  9.6650  0.12287 -0.17744  0.37290 -0.935138  0.628673</span></span>
<span><span class="co">#&gt; 3 10.0232 -0.95052 -0.73487 -1.05978  0.882944  0.023918</span></span>
<span><span class="co">#&gt; 4  8.9910  1.13571  0.32411  0.11037  1.376303 -0.422114</span></span>
<span><span class="co">#&gt; 5  9.0712  1.49474  1.87538  0.10575  0.292140 -0.184568</span></span>
<span><span class="co">#&gt; 6 11.3493 -0.18453 -0.78037 -1.23804 -0.010949  0.691034</span></span></code></pre></div>
<p>Least-squares provides accurate estimates of the regression constant
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\alpha = 10</annotation></semantics></math>
and the error variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma^2 = 1</annotation></semantics></math>
for the “null model” including only the regression constant; moreover,
the omnibus
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>-test
of the correct null hypothesis that all of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>s
are 0 for the “full model” with all 100
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>s
is associated with a large
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">D</span><span class="op">)</span></span>
<span><span class="va">m.null</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fl">1</span>, data <span class="op">=</span> <span class="va">D</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova</a></span><span class="op">(</span><span class="va">m.null</span>, <span class="va">m.full</span><span class="op">)</span></span>
<span><span class="co">#&gt; Analysis of Variance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1: y ~ 1</span></span>
<span><span class="co">#&gt; Model 2: y ~ X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + </span></span>
<span><span class="co">#&gt;     X.11 + X.12 + X.13 + X.14 + X.15 + X.16 + X.17 + X.18 + X.19 + </span></span>
<span><span class="co">#&gt;     X.20 + X.21 + X.22 + X.23 + X.24 + X.25 + X.26 + X.27 + X.28 + </span></span>
<span><span class="co">#&gt;     X.29 + X.30 + X.31 + X.32 + X.33 + X.34 + X.35 + X.36 + X.37 + </span></span>
<span><span class="co">#&gt;     X.38 + X.39 + X.40 + X.41 + X.42 + X.43 + X.44 + X.45 + X.46 + </span></span>
<span><span class="co">#&gt;     X.47 + X.48 + X.49 + X.50 + X.51 + X.52 + X.53 + X.54 + X.55 + </span></span>
<span><span class="co">#&gt;     X.56 + X.57 + X.58 + X.59 + X.60 + X.61 + X.62 + X.63 + X.64 + </span></span>
<span><span class="co">#&gt;     X.65 + X.66 + X.67 + X.68 + X.69 + X.70 + X.71 + X.72 + X.73 + </span></span>
<span><span class="co">#&gt;     X.74 + X.75 + X.76 + X.77 + X.78 + X.79 + X.80 + X.81 + X.82 + </span></span>
<span><span class="co">#&gt;     X.83 + X.84 + X.85 + X.86 + X.87 + X.88 + X.89 + X.90 + X.91 + </span></span>
<span><span class="co">#&gt;     X.92 + X.93 + X.94 + X.95 + X.96 + X.97 + X.98 + X.99 + X.100</span></span>
<span><span class="co">#&gt;   Res.Df RSS  Df Sum of Sq    F Pr(&gt;F)</span></span>
<span><span class="co">#&gt; 1    999 974                          </span></span>
<span><span class="co">#&gt; 2    899 888 100      85.2 0.86   0.82</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.null</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ 1, data = D)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -3.458 -0.681  0.019  0.636  2.935 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   9.9370     0.0312     318   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.987 on 999 degrees of freedom</span></span></code></pre></div>
<p>Next, using the <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> function in the
<strong>MASS</strong> package <span class="citation">(Venables &amp;
Ripley, 2002)</span>, let us perform a forward stepwise regression to
select a “best” model, starting with the null model, and using AIC as
the model-selection criterion (see the help page for
<code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> for details):<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;It’s generally advantageous to start with the largest
model, here the one with 100 predictors, and proceed by backward
elimination. In this demonstration, however, where all of the
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;annotation encoding="application/x-tex"&gt;\beta&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;s
are really 0, the selected model will be small, and so we proceed by
forward selection from the null model to save computing time.&lt;/p&gt;'><sup>1</sup></a></p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="http://www.stats.ox.ac.uk/pub/MASS4/" class="external-link">"MASS"</a></span><span class="op">)</span>  <span class="co"># for stepAIC()</span></span>
<span><span class="va">m.select</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC</a></span><span class="op">(</span></span>
<span>  <span class="va">m.null</span>,</span>
<span>  direction <span class="op">=</span> <span class="st">"forward"</span>,</span>
<span>  trace <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lower <span class="op">=</span>  <span class="op">~</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">formula</a></span><span class="op">(</span><span class="va">m.full</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.select</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ X.99 + X.90 + X.87 + X.40 + X.65 + X.91 + X.53 + </span></span>
<span><span class="co">#&gt;     X.45 + X.31 + X.56 + X.61 + X.60 + X.46 + X.35 + X.92, data = D)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -3.262 -0.645  0.024  0.641  3.118 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   9.9372     0.0310  320.80   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; X.99         -0.0910     0.0308   -2.95   0.0032 ** </span></span>
<span><span class="co">#&gt; X.90         -0.0820     0.0314   -2.62   0.0090 ** </span></span>
<span><span class="co">#&gt; X.87         -0.0694     0.0311   -2.24   0.0256 *  </span></span>
<span><span class="co">#&gt; X.40         -0.0476     0.0308   -1.55   0.1221    </span></span>
<span><span class="co">#&gt; X.65         -0.0552     0.0315   -1.76   0.0795 .  </span></span>
<span><span class="co">#&gt; X.91          0.0524     0.0308    1.70   0.0894 .  </span></span>
<span><span class="co">#&gt; X.53         -0.0492     0.0305   -1.61   0.1067    </span></span>
<span><span class="co">#&gt; X.45          0.0554     0.0318    1.74   0.0818 .  </span></span>
<span><span class="co">#&gt; X.31          0.0452     0.0311    1.46   0.1457    </span></span>
<span><span class="co">#&gt; X.56          0.0543     0.0327    1.66   0.0972 .  </span></span>
<span><span class="co">#&gt; X.61         -0.0508     0.0317   -1.60   0.1091    </span></span>
<span><span class="co">#&gt; X.60         -0.0513     0.0319   -1.61   0.1083    </span></span>
<span><span class="co">#&gt; X.46          0.0516     0.0327    1.58   0.1153    </span></span>
<span><span class="co">#&gt; X.35          0.0470     0.0315    1.49   0.1358    </span></span>
<span><span class="co">#&gt; X.92          0.0443     0.0310    1.43   0.1533    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.973 on 984 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.0442, Adjusted R-squared:  0.0296 </span></span>
<span><span class="co">#&gt; F-statistic: 3.03 on 15 and 984 DF,  p-value: 8.34e-05</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://gmonette.github.io/cv/">"cv"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: doParallel</span></span>
<span><span class="co">#&gt; Loading required package: foreach</span></span>
<span><span class="co">#&gt; Loading required package: iterators</span></span>
<span><span class="co">#&gt; Loading required package: parallel</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">D</span><span class="op">$</span><span class="va">y</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.select</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.93063</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>The resulting model has 15 predictors, a very modest
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup><mo>=</mo><mn>.044</mn></mrow><annotation encoding="application/x-tex">R^2 = .044</annotation></semantics></math>,
but a small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value
for its omnibus
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>-test
(which, of course, is entirely spurious because the same data were used
to select and test the model). The MSE for the selected model is smaller
than the true error variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma^2 = 1</annotation></semantics></math>,
as is the estimated error variance for the selected model,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mn>2</mn></msup><mo>=</mo><msup><mn>0.973</mn><mn>2</mn></msup><mo>=</mo><mn>0.947</mn></mrow><annotation encoding="application/x-tex">\widehat{\sigma}^2 = 0.973^2 = 0.947</annotation></semantics></math>.</p>
<p>If we cross-validate the selected model, we also obtain an optimistic
estimate of its predictive power (although the confidence interval for
the bias-adjusted MSE includes 1):</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://gmonette.github.io/cv/">"cv"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.select</span>, seed <span class="op">=</span> <span class="fl">2529</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 2529</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.95937</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.95785</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.87661, 1.0391)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.93063</span></span></code></pre></div>
<p>The <code>"function"</code> method of <code><a href="../reference/cv.html">cv()</a></code> allows us to
cross-validate the whole model-selection procedure, where first argument
to <code><a href="../reference/cv.html">cv()</a></code> is a model-selection function capable of refitting
the model with a fold omitted and returning a CV criterion. The
<code><a href="../reference/cv.function.html">selectStepAIC()</a></code> function, in the <strong>cv</strong>
package and based on <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code>, is suitable for use with
<code><a href="../reference/cv.html">cv()</a></code>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.select</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="va">selectStepAIC</span>,</span>
<span>  data <span class="op">=</span> <span class="va">D</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">3791</span>,</span>
<span>  working.model <span class="op">=</span> <span class="va">m.null</span>,</span>
<span>  direction <span class="op">=</span> <span class="st">"forward"</span>,</span>
<span>  scope <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lower <span class="op">=</span>  <span class="op">~</span> <span class="fl">1</span>, upper <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">formula</a></span><span class="op">(</span><span class="va">m.full</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 3791</span></span>
<span><span class="va">cv.select</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 1.0687</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 1.0612</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.97172, 1.1506)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.93063</span></span></code></pre></div>
<p>The other arguments to <code><a href="../reference/cv.html">cv()</a></code> are:</p>
<ul>
<li>
<code>data</code>, the data set to which the model is fit;</li>
<li>
<code>seed</code>, an optional seed for R’s pseudo-random-number
generator; as for <code><a href="../reference/cv.html">cv()</a></code>, if the seed isn’t supplied by the
user, a seed is randomly selected and saved;</li>
<li>additional arguments required by the model-selection function, here
the starting <code>working.model</code> argument, the
<code>direction</code> of model selection, and the <code>scope</code> of
models considered (from the model with only a regression constant to the
model with all 100 predictors).</li>
</ul>
<p>By default, <code><a href="../reference/cv.html">cv()</a></code> performs 10-fold CV, and produces an
estimate of MSE for the model-selection procedure even <em>larger</em>
than the true error variance,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sigma^2 = 1</annotation></semantics></math>.</p>
<p>Also by default, when the number of folds is 10 or fewer,
<code><a href="../reference/cv.html">cv()</a></code> saves details data about the folds. In this example,
the <code><a href="../reference/cv.function.html">compareFolds()</a></code> function reveals that the variables
retained by the model-selection process in the several folds are quite
different:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.function.html">compareFolds</a></span><span class="op">(</span><span class="va">cv.select</span><span class="op">)</span></span>
<span><span class="co">#&gt; CV criterion by folds:</span></span>
<span><span class="co">#&gt;  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10 </span></span>
<span><span class="co">#&gt; 1.26782 1.12837 1.04682 1.31007 1.06899 0.87916 0.88380 0.95026 1.21070 0.94130 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients by folds:</span></span>
<span><span class="co">#&gt;         (Intercept)    X.87    X.90    X.99    X.91    X.54    X.53    X.56</span></span>
<span><span class="co">#&gt; Fold 1       9.9187 -0.0615 -0.0994 -0.0942  0.0512  0.0516                </span></span>
<span><span class="co">#&gt; Fold 2       9.9451 -0.0745 -0.0899 -0.0614          0.0587          0.0673</span></span>
<span><span class="co">#&gt; Fold 3       9.9423 -0.0783 -0.0718 -0.0987  0.0601                  0.0512</span></span>
<span><span class="co">#&gt; Fold 4       9.9410 -0.0860 -0.0831 -0.0867  0.0570         -0.0508        </span></span>
<span><span class="co">#&gt; Fold 5       9.9421 -0.0659 -0.0849 -0.1004  0.0701  0.0511 -0.0487  0.0537</span></span>
<span><span class="co">#&gt; Fold 6       9.9633 -0.0733 -0.0874 -0.0960  0.0555  0.0629 -0.0478        </span></span>
<span><span class="co">#&gt; Fold 7       9.9279 -0.0618 -0.0960 -0.0838  0.0533         -0.0464        </span></span>
<span><span class="co">#&gt; Fold 8       9.9453 -0.0610 -0.0811 -0.0818          0.0497 -0.0612  0.0560</span></span>
<span><span class="co">#&gt; Fold 9       9.9173 -0.0663 -0.0894 -0.1100  0.0504  0.0524          0.0747</span></span>
<span><span class="co">#&gt; Fold 10      9.9449 -0.0745 -0.0906 -0.0891  0.0535  0.0482 -0.0583  0.0642</span></span>
<span><span class="co">#&gt;            X.40    X.45    X.65    X.68    X.92    X.15    X.26    X.46    X.60</span></span>
<span><span class="co">#&gt; Fold 1                  -0.0590                 -0.0456  0.0658  0.0608        </span></span>
<span><span class="co">#&gt; Fold 2                                   0.0607          0.0487                </span></span>
<span><span class="co">#&gt; Fold 3  -0.0496         -0.0664          0.0494                                </span></span>
<span><span class="co">#&gt; Fold 4  -0.0597  0.0579 -0.0531          0.0519 -0.0566                 -0.0519</span></span>
<span><span class="co">#&gt; Fold 5                           0.0587                          0.0527 -0.0603</span></span>
<span><span class="co">#&gt; Fold 6  -0.0596  0.0552          0.0474                                        </span></span>
<span><span class="co">#&gt; Fold 7           0.0572          0.0595                                        </span></span>
<span><span class="co">#&gt; Fold 8           0.0547 -0.0617  0.0453  0.0493 -0.0613  0.0591  0.0703 -0.0588</span></span>
<span><span class="co">#&gt; Fold 9  -0.0552  0.0573 -0.0635  0.0492         -0.0513  0.0484         -0.0507</span></span>
<span><span class="co">#&gt; Fold 10 -0.0558                          0.0529                  0.0710        </span></span>
<span><span class="co">#&gt;            X.61     X.8    X.28    X.29    X.31    X.35    X.70    X.89    X.17</span></span>
<span><span class="co">#&gt; Fold 1  -0.0490          0.0616 -0.0537                  0.0638                </span></span>
<span><span class="co">#&gt; Fold 2           0.0671                  0.0568                  0.0523        </span></span>
<span><span class="co">#&gt; Fold 3  -0.0631          0.0616                                                </span></span>
<span><span class="co">#&gt; Fold 4           0.0659         -0.0549          0.0527                  0.0527</span></span>
<span><span class="co">#&gt; Fold 5           0.0425                  0.0672  0.0613          0.0493        </span></span>
<span><span class="co">#&gt; Fold 6           0.0559         -0.0629  0.0498          0.0487                </span></span>
<span><span class="co">#&gt; Fold 7                                                           0.0611  0.0472</span></span>
<span><span class="co">#&gt; Fold 8  -0.0719                                          0.0586                </span></span>
<span><span class="co">#&gt; Fold 9                   0.0525                                                </span></span>
<span><span class="co">#&gt; Fold 10 -0.0580                                  0.0603                        </span></span>
<span><span class="co">#&gt;            X.25     X.4    X.64    X.81    X.97    X.11     X.2    X.33    X.47</span></span>
<span><span class="co">#&gt; Fold 1                                   0.0604          0.0575                </span></span>
<span><span class="co">#&gt; Fold 2   0.0478          0.0532  0.0518                                        </span></span>
<span><span class="co">#&gt; Fold 3                           0.0574                          0.0473        </span></span>
<span><span class="co">#&gt; Fold 4                   0.0628                                                </span></span>
<span><span class="co">#&gt; Fold 5   0.0518                                                                </span></span>
<span><span class="co">#&gt; Fold 6                                           0.0521                        </span></span>
<span><span class="co">#&gt; Fold 7           0.0550                                                        </span></span>
<span><span class="co">#&gt; Fold 8                                                                         </span></span>
<span><span class="co">#&gt; Fold 9                                   0.0556                          0.0447</span></span>
<span><span class="co">#&gt; Fold 10          0.0516                                                        </span></span>
<span><span class="co">#&gt;             X.6    X.72    X.73    X.77    X.79 X.88</span></span>
<span><span class="co">#&gt; Fold 1   0.0476                                     </span></span>
<span><span class="co">#&gt; Fold 2                   0.0514                     </span></span>
<span><span class="co">#&gt; Fold 3                                              </span></span>
<span><span class="co">#&gt; Fold 4                                  -0.0473     </span></span>
<span><span class="co">#&gt; Fold 5           0.0586                         0.07</span></span>
<span><span class="co">#&gt; Fold 6                          -0.0489             </span></span>
<span><span class="co">#&gt; Fold 7                                              </span></span>
<span><span class="co">#&gt; Fold 8                                              </span></span>
<span><span class="co">#&gt; Fold 9                                              </span></span>
<span><span class="co">#&gt; Fold 10</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="polynomial-regression-for-the-auto-data-revisited-recursive-cross-validation">Polynomial regression for the Auto data revisited: recursive
cross-validation<a class="anchor" aria-label="anchor" href="#polynomial-regression-for-the-auto-data-revisited-recursive-cross-validation"></a>
</h2>
In the introductory vignette on cross-validating regression models,
following <span class="citation">James, Witten, Hastie, &amp; Tibshirani
(2021, secs. 5.1, 5.3)</span>, we fit polynomial regressions up to
degree 10 to the relationship of <code>mpg</code> to
<code>horsepower</code> for the <code>Auto</code> data, saving the
results in <code>m.1</code> through <code>m.10</code>. We then used
<code><a href="../reference/cv.html">cv()</a></code> to compare the cross-validated MSE for the 10 models,
discovering that the 7th degree polynomial had the smallest MSE (by a
small margin); repeating the relevant graph:
<div class="figure" style="text-align: center">
<img src="fig/polynomial-regression-CV-graph-duplicated-1.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$ (repeated)" width="100%"><p class="caption">
Cross-validated 10-fold and LOO MSE as a function of polynomial degree,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
(repeated)
</p>
</div>
<p>If we then select the 7th degree polynomial model, intending to use
it for prediction, the CV estimate of the MSE for this model will be
optimistic. One solution is to cross-validate the process of using CV to
select the “best” model—that is, to apply CV to CV recursively. The
function <code><a href="../reference/cv.function.html">selectModelList()</a></code>, which is suitable for use with
<code><a href="../reference/cv.html">cv()</a></code>, implements this idea.</p>
<p>Applying <code><a href="../reference/cv.function.html">selectModelList()</a></code> to the <code>Auto</code>
polynomial-regression models, and using 10-fold CV, we obtain:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">recursiveCV.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="va">selectModelList</span>,</span>
<span>  <span class="va">Auto</span>,</span>
<span>  working.model <span class="op">=</span> <span class="fu"><a href="../reference/cv.modList.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>                         <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>  save.model <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">2120</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 2120</span></span>
<span><span class="va">recursiveCV.auto</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.105</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.68</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.746</span></span>
<span><span class="va">recursiveCV.auto</span><span class="op">$</span><span class="va">selected.model</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 7), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;          (Intercept)  poly(horsepower, 7)1  poly(horsepower, 7)2  </span></span>
<span><span class="co">#&gt;                23.45               -120.14                 44.09  </span></span>
<span><span class="co">#&gt; poly(horsepower, 7)3  poly(horsepower, 7)4  poly(horsepower, 7)5  </span></span>
<span><span class="co">#&gt;                -3.95                 -5.19                 13.27  </span></span>
<span><span class="co">#&gt; poly(horsepower, 7)6  poly(horsepower, 7)7  </span></span>
<span><span class="co">#&gt;                -8.55                  7.98</span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.7</span>, seed <span class="op">=</span> <span class="fl">2120</span><span class="op">)</span> <span class="co"># same seed for same folds</span></span>
<span><span class="co">#&gt; R RNG seed set to 2120</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.898</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 18.854</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.078</span></span></code></pre></div>
<p>As expected, recursive CV produces a larger estimate of MSE for the
selected 7th degree polynomial model than CV applied directly to this
model.</p>
<p>We can equivalently call <code><a href="../reference/cv.html">cv()</a></code> with the list of models as
the first argument and set <code>recursive=TRUE</code>:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">recursiveCV.auto.alt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/cv.modList.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>         <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Auto</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">2120</span>,</span>
<span>  recursive <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  save.model <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 2120</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/all.equal-methods.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">recursiveCV.auto</span>, <span class="va">recursiveCV.auto.alt</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Component \"criterion\": 1 string mismatch"</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="mrozs-logistic-regression-revisited">Mroz’s logistic regression revisited<a class="anchor" aria-label="anchor" href="#mrozs-logistic-regression-revisited"></a>
</h2>
<p>Next, let’s apply model selection to Mroz’s logistic regression for
married women’s labor-force participation, also discussed in the
introductory vignette on cross-validating regression models. First,
recall the logistic regression model that we fit to the
<code>Mroz</code> data:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Mroz"</span>, package <span class="op">=</span> <span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="va">m.mroz</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Mroz</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 921.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
<p>Applying stepwise model selection Mroz’s logistic regression, using
BIC as the model-selection criterion (via the argument
<code>k=log(nrow(Mroz))</code> to <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code>) selects 5 of
the 7 original predictors:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz.sel</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC</a></span><span class="op">(</span><span class="va">m.mroz</span>, k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                      trace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz.sel</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ k5 + age + wc + lwg + inc, family = binomial, </span></span>
<span><span class="co">#&gt;     data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)   2.9019     0.5429    5.35  9.0e-08 ***</span></span>
<span><span class="co">#&gt; k5           -1.4318     0.1932   -7.41  1.3e-13 ***</span></span>
<span><span class="co">#&gt; age          -0.0585     0.0114   -5.13  2.9e-07 ***</span></span>
<span><span class="co">#&gt; wcyes         0.8724     0.2064    4.23  2.4e-05 ***</span></span>
<span><span class="co">#&gt; lwg           0.6157     0.1501    4.10  4.1e-05 ***</span></span>
<span><span class="co">#&gt; inc          -0.0337     0.0078   -4.32  1.6e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  906.46  on 747  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 918.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 3</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">BayesRule</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">$</span><span class="va">lfp</span> <span class="op">==</span> <span class="st">"yes"</span>,</span>
<span>          <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">m.mroz.sel</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.31873</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "y != round(yhat)"</span></span></code></pre></div>
<p>Bayes rule applied to the selected model misclassifies 32% of the
cases in the <code>Mroz</code> data.</p>
<p>Cross-validating the selected model produces a similar, slightly
larger, estimate of misclassification, about 33%:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz.sel</span>, criterion <span class="op">=</span> <span class="va">BayesRule</span>, seed <span class="op">=</span> <span class="fl">345266</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 345266</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: exact</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.33068</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.33332</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.2997, 0.36695)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.31873</span></span></code></pre></div>
<p>Is this estimate of predictive performance optimistic?</p>
<p>We proceed to apply the model-selection procedure by
cross-validation, producing more or less the same result:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz.sel.cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="va">selectStepAIC</span>,</span>
<span>  <span class="va">Mroz</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">6681</span>,</span>
<span>  criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>  working.model <span class="op">=</span> <span class="va">m.mroz</span>,</span>
<span>  AIC <span class="op">=</span> <span class="cn">FALSE</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 6681</span></span>
<span><span class="va">m.mroz.sel.cv</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.33068</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.33452</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.3009, 0.36815)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.31873</span></span></code></pre></div>
<p>Setting <code>AIC=FALSE</code> in the call to <code><a href="../reference/cv.html">cv()</a></code> uses
the BIC rather than the AIC as the model-selection criterion. As it
turns out, exactly the same predictors are selected when each of the 10
folds are omitted, and the several coefficient estimates are very
similar, as we show using <code><a href="../reference/cv.function.html">compareFolds()</a></code>:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.function.html">compareFolds</a></span><span class="op">(</span><span class="va">m.mroz.sel.cv</span><span class="op">)</span></span>
<span><span class="co">#&gt; CV criterion by folds:</span></span>
<span><span class="co">#&gt;  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10 </span></span>
<span><span class="co">#&gt; 0.27632 0.40789 0.34211 0.36000 0.28000 0.37333 0.32000 0.29333 0.33333 0.32000 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients by folds:</span></span>
<span><span class="co">#&gt;         (Intercept)     age     inc      k5     lwg wcyes</span></span>
<span><span class="co">#&gt; Fold 1       2.5014 -0.0454 -0.0388 -1.3613  0.5653  0.85</span></span>
<span><span class="co">#&gt; Fold 2       3.0789 -0.0659 -0.0306 -1.5335  0.6923  0.79</span></span>
<span><span class="co">#&gt; Fold 3       3.0141 -0.0595 -0.0305 -1.3994  0.5428  0.86</span></span>
<span><span class="co">#&gt; Fold 4       2.7251 -0.0543 -0.0354 -1.4474  0.6298  1.09</span></span>
<span><span class="co">#&gt; Fold 5       2.7617 -0.0566 -0.0320 -1.4752  0.6324  0.74</span></span>
<span><span class="co">#&gt; Fold 6       3.0234 -0.0621 -0.0348 -1.4537  0.6618  0.94</span></span>
<span><span class="co">#&gt; Fold 7       2.9615 -0.0600 -0.0351 -1.4127  0.5835  0.97</span></span>
<span><span class="co">#&gt; Fold 8       2.9598 -0.0603 -0.0329 -1.3865  0.6210  0.69</span></span>
<span><span class="co">#&gt; Fold 9       3.2481 -0.0650 -0.0381 -1.4138  0.6093  0.94</span></span>
<span><span class="co">#&gt; Fold 10      2.7724 -0.0569 -0.0295 -1.4503  0.6347  0.85</span></span></code></pre></div>
<p>In this example, therefore, we appear to obtain a realistic estimate
of model performance directly from the selected model, because there is
little added uncertainty induced by model selection.</p>
</div>
<div class="section level2">
<h2 id="cross-validating-choice-of-transformations-in-regression">Cross-validating choice of transformations in regression<a class="anchor" aria-label="anchor" href="#cross-validating-choice-of-transformations-in-regression"></a>
</h2>
<p>The <strong>cv</strong> package also provides a <code><a href="../reference/cv.html">cv()</a></code>
procedure, <code><a href="../reference/cv.function.html">selectTrans()</a></code>, for choosing transformations of
the predictors and the response in regression.</p>
<p>Some background: As <span class="citation">Weisberg (2014, sec.
8.2)</span> explains, there are technical advantages to having (numeric)
predictors in linear regression analysis that are themselves linearly
related. If the predictors <em>aren’t</em> linearly related, then the
relationships between them can often be straightened by power
transformations. Transformations can be selected after graphical
examination of the data, or by analytic methods. Once the relationships
between the predictors are linearized, it can be advantageous similarly
to transform the response variable towards normality.</p>
<p>Selecting transformations analytically raises the possibility of
automating the process, as would be required for cross-validation. One
could, in principle, apply graphical methods to select transformations
for each fold, but because a data analyst couldn’t forget the choices
made for previous folds, the process wouldn’t really be applied
independently to the folds.</p>
<p>To illustrate, we adapt an example appearing in several places in
<span class="citation">Fox &amp; Weisberg (2019)</span> (for example in
Chapter 3 on transforming data), using data on the prestige and other
characteristics of 102 Canadian occupations circa 1970. The data are in
the <code>Prestige</code> data frame in the <strong>carData</strong>
package:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Prestige"</span>, package <span class="op">=</span> <span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">)</span></span>
<span><span class="co">#&gt;                     education income women prestige census type</span></span>
<span><span class="co">#&gt; gov.administrators      13.11  12351 11.16     68.8   1113 prof</span></span>
<span><span class="co">#&gt; general.managers        12.26  25879  4.02     69.1   1130 prof</span></span>
<span><span class="co">#&gt; accountants             12.77   9271 15.70     63.4   1171 prof</span></span>
<span><span class="co">#&gt; purchasing.officers     11.42   8865  9.11     56.8   1175 prof</span></span>
<span><span class="co">#&gt; chemists                14.62   8403 11.68     73.5   2111 prof</span></span>
<span><span class="co">#&gt; physicists              15.64  11030  5.13     77.6   2113 prof</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">)</span></span>
<span><span class="co">#&gt;    education         income          women          prestige        census    </span></span>
<span><span class="co">#&gt;  Min.   : 6.38   Min.   :  611   Min.   : 0.00   Min.   :14.8   Min.   :1113  </span></span>
<span><span class="co">#&gt;  1st Qu.: 8.45   1st Qu.: 4106   1st Qu.: 3.59   1st Qu.:35.2   1st Qu.:3120  </span></span>
<span><span class="co">#&gt;  Median :10.54   Median : 5930   Median :13.60   Median :43.6   Median :5135  </span></span>
<span><span class="co">#&gt;  Mean   :10.74   Mean   : 6798   Mean   :28.98   Mean   :46.8   Mean   :5402  </span></span>
<span><span class="co">#&gt;  3rd Qu.:12.65   3rd Qu.: 8187   3rd Qu.:52.20   3rd Qu.:59.3   3rd Qu.:8312  </span></span>
<span><span class="co">#&gt;  Max.   :15.97   Max.   :25879   Max.   :97.51   Max.   :87.2   Max.   :9517  </span></span>
<span><span class="co">#&gt;    type   </span></span>
<span><span class="co">#&gt;  bc  :44  </span></span>
<span><span class="co">#&gt;  prof:31  </span></span>
<span><span class="co">#&gt;  wc  :23  </span></span>
<span><span class="co">#&gt;  NA's: 4  </span></span>
<span><span class="co">#&gt;           </span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
<p>The variables in the <code>Prestige</code> data set are:</p>
<ul>
<li>
<code>education</code>: average years of education for incumbents in
the occupation, from the 1971 Canadian Census.</li>
<li>
<code>income</code>: average dollars of annual income for the
occupation, from the Census.</li>
<li>
<code>women</code>: percentage of occupational incumbents who were
women, also from the Census.</li>
<li>
<code>prestige</code>: the average prestige rating of the occupation
on a 0–100 “thermometer” scale, in a Canadian social survey conducted
around the same time.</li>
<li>
<code>type</code>, type of occupation, and <code>census</code>, the
Census occupational code, which are not used in our example.</li>
</ul>
<p>The object of a regression analysis for the <code>Prestige</code>
data (and their original purpose) is to predict occupational prestige
from the other variables in the data set.</p>
<p>A scatterplot matrix (using the <code><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix()</a></code>
function in the <strong>car</strong> package) of the numeric variables
in the data reveals that the distributions of <code>income</code> and
<code>women</code> are positively skewed, and that some of the
relationships among the three predictors, and between the predictors and
the response (i.e., <code>prestige</code>), are nonlinear:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://r-forge.r-project.org/projects/car/" class="external-link">"car"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: carData</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span></span>
<span>  <span class="op">~</span> <span class="va">prestige</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Prestige</span>,</span>
<span>  smooth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/scatterplot-matrix-1.png" alt="Scatterplot matrix for the `Prestige` data." width="100%"><p class="caption">
Scatterplot matrix for the <code>Prestige</code> data.
</p>
</div>
<p>The <code><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform()</a></code> function in the
<strong>car</strong> package transforms variables towards multivariate
normality by a generalization of Box and Cox’s maximum-likelihood-like
approach <span class="citation">(Box &amp; Cox, 1964)</span>. Several
“families” of power transformations can be used, including the original
Box-Cox family, simple powers (and roots), and two adaptations of the
Box-Cox family to data that may include negative values and zeros: the
Box-Cox-with-negatives family and the Yeo-Johnson family; see <span class="citation">Weisberg (2014, Chapter 8)</span>, and <span class="citation">Fox &amp; Weisberg (2019, Chapter 3)</span> for
details. Because <code>women</code> has some zero values, we use the
Yeo-Johnson family:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">trans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">income</span>, <span class="va">education</span>, <span class="va">women</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>,</span>
<span>                        data <span class="op">=</span> <span class="va">Prestige</span>,</span>
<span>                        family <span class="op">=</span> <span class="st">"yjPower"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">trans</span><span class="op">)</span></span>
<span><span class="co">#&gt; yjPower Transformations to Multinormality </span></span>
<span><span class="co">#&gt;           Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; income       0.2678        0.33       0.1051       0.4304</span></span>
<span><span class="co">#&gt; education    0.5162        1.00      -0.2822       1.3145</span></span>
<span><span class="co">#&gt; women        0.1630        0.16       0.0112       0.3149</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Likelihood ratio test that all transformation parameters are equal to 0</span></span>
<span><span class="co">#&gt;                              LRT df    pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0 0 0) 15.739  3 0.00128</span></span></code></pre></div>
<p>We thus have evidence of the desirability of transforming
<code>income</code> (by the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">1/3</annotation></semantics></math>
power) and <code>women</code> (by the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.16</mn><annotation encoding="application/x-tex">0.16</annotation></semantics></math>
power—which is close to the “0” power, i.e., the log transformation),
but not <code>education</code>. Applying the “rounded” power
transformations makes the predictors better-behaved:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P</span> <span class="op">&lt;-</span> <span class="va">Prestige</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"prestige"</span>, <span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="op">(</span><span class="va">lambdas</span> <span class="op">&lt;-</span> <span class="va">trans</span><span class="op">$</span><span class="va">roundlam</span><span class="op">)</span></span>
<span><span class="co">#&gt;    income education     women </span></span>
<span><span class="co">#&gt;   0.33000   1.00000   0.16302</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">lambdas</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">var</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">P</span><span class="op">[</span>, <span class="va">var</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/bcPower.html" class="external-link">yjPower</a></span><span class="op">(</span><span class="va">P</span><span class="op">[</span>, <span class="va">var</span><span class="op">]</span>, lambda <span class="op">=</span> <span class="va">lambdas</span><span class="op">[</span><span class="va">var</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">P</span><span class="op">)</span></span>
<span><span class="co">#&gt;     prestige        income       education         women     </span></span>
<span><span class="co">#&gt;  Min.   :14.8   Min.   :22.2   Min.   : 6.38   Min.   :0.00  </span></span>
<span><span class="co">#&gt;  1st Qu.:35.2   1st Qu.:44.2   1st Qu.: 8.45   1st Qu.:1.73  </span></span>
<span><span class="co">#&gt;  Median :43.6   Median :50.3   Median :10.54   Median :3.36  </span></span>
<span><span class="co">#&gt;  Mean   :46.8   Mean   :50.8   Mean   :10.74   Mean   :3.50  </span></span>
<span><span class="co">#&gt;  3rd Qu.:59.3   3rd Qu.:56.2   3rd Qu.:12.65   3rd Qu.:5.59  </span></span>
<span><span class="co">#&gt;  Max.   :87.2   Max.   :83.6   Max.   :15.97   Max.   :6.83</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span></span>
<span>  <span class="op">~</span> <span class="va">prestige</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>,</span>
<span>  data <span class="op">=</span> <span class="va">P</span>,</span>
<span>  smooth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/transformed-predictors-1.png" alt="Scatterplot matrix for the `Prestige` data with the predictors transformed." width="100%"><p class="caption">
Scatterplot matrix for the <code>Prestige</code> data with the
predictors transformed.
</p>
</div>
<p>Comparing the MSE for the regressions with the original and
transformed predictors shows a advantage to the latter:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.pres</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>, data <span class="op">=</span> <span class="va">Prestige</span><span class="op">)</span></span>
<span><span class="va">m.pres.trans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>, data <span class="op">=</span> <span class="va">P</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">$</span><span class="va">prestige</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.pres</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 59.153</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">P</span><span class="op">$</span><span class="va">prestige</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.pres.trans</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 50.6</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>Similarly, component+residual plots for the two regressions, produced
by the <code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots()</a></code> function in the <strong>car</strong>
package, suggest that the partial relationship of <code>prestige</code>
to <code>income</code> is more nearly linear in the transformed data,
but the transformation of <code>women</code> fails to capture what
appears to be a slight quadratic partial relationship; the partial
relationship of <code>prestige</code> to <code>education</code> is close
to linear in both regressions:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m.pres</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/CR-plots-untransformed-1.png" alt="Component+residual plots for the `Prestige` regression with the original predictors." width="672"><p class="caption">
Component+residual plots for the <code>Prestige</code> regression with
the original predictors.
</p>
</div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m.pres.trans</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/CR-plots-transformed-1.png" alt="Component+residual plots for the `Prestige` regression with transformed predictors." width="672"><p class="caption">
Component+residual plots for the <code>Prestige</code> regression with
transformed predictors.
</p>
</div>
<p>Having transformed the predictors towards multinormality, we now
consider whether there’s evidence for transforming the response (using
<code><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform()</a></code> for Box and Cox’s original method), and we
discover that there’s not:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="va">m.pres.trans</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; bcPower Transformation to Normality </span></span>
<span><span class="co">#&gt;    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; Y1    1.0194           1       0.6773       1.3615</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that transformation parameter is equal to 0</span></span>
<span><span class="co">#&gt;  (log transformation)</span></span>
<span><span class="co">#&gt;                          LRT df     pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0) 32.217  1 1.38e-08</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that no transformation is needed</span></span>
<span><span class="co">#&gt;                            LRT df  pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (1) 0.012384  1 0.911</span></span></code></pre></div>
<p>The <code><a href="../reference/cv.function.html">selectTrans()</a></code> function in the <strong>cv</strong>
package automates the process of selecting predictor and response
transformations. The function takes a <code>data</code> set and
“working” <code>model</code> as arguments, along with the candidate
<code>predictors</code> and <code>response</code> for transformation,
and the transformation <code>family</code> to employ. If the
<code>predictors</code> argument is missing then only the response is
transformed, and if the <code>response</code> argument is missing, only
the supplied predictors are transformed. The default <code>family</code>
for transforming the predictors is <code>"bcPower"</code>—the original
Box-Cox family—as is the default <code>family.y</code> for transforming
the response; here we specify <code>family="yjPower</code> because of
the zeros in <code>women</code>. <code><a href="../reference/cv.function.html">selectTrans()</a></code> returns the
result of applying a lack-of-fit criterion to the model after the
selected transformation is applied, with the default
<code>criterion=mse</code>:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.function.html">selectTrans</a></span><span class="op">(</span></span>
<span>  data <span class="op">=</span> <span class="va">Prestige</span>,</span>
<span>  model <span class="op">=</span> <span class="va">m.pres</span>,</span>
<span>  predictors <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span>,</span>
<span>  response <span class="op">=</span> <span class="st">"prestige"</span>,</span>
<span>  family <span class="op">=</span> <span class="st">"yjPower"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; $criterion</span></span>
<span><span class="co">#&gt; [1] 50.6</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $model</span></span>
<span><span class="co">#&gt; NULL</span></span></code></pre></div>
<p><code><a href="../reference/cv.function.html">selectTrans()</a></code> also takes an optional
<code>indices</code> argument, making it suitable for doing computations
on a subset of the data (i.e., a CV fold), and hence for use with
<code><a href="../reference/cv.html">cv()</a></code> (see <code><a href="../reference/cv.function.html">?selectTrans</a></code> for details):</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="va">selectTrans</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Prestige</span>,</span>
<span>  working.model <span class="op">=</span> <span class="va">m.pres</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">1463</span>,</span>
<span>  predictors <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span>,</span>
<span>  response <span class="op">=</span> <span class="st">"prestige"</span>,</span>
<span>  family <span class="op">=</span> <span class="st">"yjPower"</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 1463</span></span>
<span><span class="va">cvs</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 54.487</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 54.308</span></span>
<span><span class="co">#&gt; full-sample criterion = 50.6</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.pres</span>, seed <span class="op">=</span> <span class="fl">1463</span><span class="op">)</span> <span class="co"># untransformed model with same folds</span></span>
<span><span class="co">#&gt; R RNG seed set to 1463</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 63.293</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 63.073</span></span>
<span><span class="co">#&gt; full-sample criterion = 59.153</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.function.html">compareFolds</a></span><span class="op">(</span><span class="va">cvs</span><span class="op">)</span></span>
<span><span class="co">#&gt; CV criterion by folds:</span></span>
<span><span class="co">#&gt;  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10 </span></span>
<span><span class="co">#&gt;  63.453  79.257  20.634  94.569  19.902  55.821  26.555  75.389  55.702  50.215 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients by folds:</span></span>
<span><span class="co">#&gt;         lam.education lam.income lam.women lambda</span></span>
<span><span class="co">#&gt; Fold 1          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 2          1.000      0.330     0.169      1</span></span>
<span><span class="co">#&gt; Fold 3          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 4          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 5          1.000      0.330     0.000      1</span></span>
<span><span class="co">#&gt; Fold 6          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 7          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 8          1.000      0.330     0.000      1</span></span>
<span><span class="co">#&gt; Fold 9          1.000      0.330     0.000      1</span></span>
<span><span class="co">#&gt; Fold 10         1.000      0.330     0.000      1</span></span></code></pre></div>
<p>The results suggest that the predictive power of the transformed
regression is reliably greater than that of the untransformed regression
(though in both case, the cross-validated MSE is considerably higher
than the MSE computed for the whole data). Examining the selected
transformations for each fold reveals that the predictor
<code>education</code> and the response <code>prestige</code> are never
transformed; that the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">1/3</annotation></semantics></math>
power is selected for <code>income</code> in all of the folds; and that
the transformation selected for <code>women</code> varies narrowly
across the folds between the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>th
power (i.e., log) and the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">1/3</annotation></semantics></math>
power.</p>
</div>
<div class="section level2">
<h2 id="selecting-both-transformations-and-predictorsvenables">Selecting both transformations and predictors<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The presentation in the section benefits from an email
conversation with Bill Venables, who of course isn’t responsible for the
use to which we’ve put his insightful remarks.&lt;/p&gt;"><sup>2</sup></a><a class="anchor" aria-label="anchor" href="#selecting-both-transformations-and-predictorsvenables"></a>
</h2>
<p>As we mentioned, <span class="citation">Hastie et al. (2009, sec.
7.10.2: “The Wrong and Right Way to Do Cross-validation”)</span> explain
that honest cross-validation has to take account of model specification
and selection. Statistical modeling is at least partly a craft, and one
could imagine applying that craft to successive partial data sets, each
with a fold removed. The resulting procedure would be tedious, though
possibly worth the effort, but it would also be difficult to realize in
practice: After all, we can hardly erase our memory of statistical
modeling choices between analyzing partial data sets.</p>
<p>Alternatively, if we’re able to automate the process of model
selection, then we can more realistically apply CV mechanically. That’s
what we did in the preceding two sections, first for predictor selection
and then for selection of transformations in regression. In this
section, we consider the case where we both select variable
transformations and then proceed to select predictors. It’s insufficient
to apply these steps sequentially, first, for example, using
<code><a href="../reference/cv.html">cv()</a></code> with <code><a href="../reference/cv.function.html">selectTrans()</a></code> and then with
<code><a href="../reference/cv.function.html">selectStepAIC()</a></code>; rather we should apply the whole
model-selection procedure with each fold omitted. The
<code>selectTransAndStepAIC()</code> function, also supplied by the
<strong>cv</strong> package, does exactly that.</p>
<p>To illustrate this process, we return to the <code>Auto</code> data
set:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mpg         cylinders     displacement   horsepower        weight    </span></span>
<span><span class="co">#&gt;  Min.   : 9.0   Min.   :3.00   Min.   : 68   Min.   : 46.0   Min.   :1613  </span></span>
<span><span class="co">#&gt;  1st Qu.:17.0   1st Qu.:4.00   1st Qu.:105   1st Qu.: 75.0   1st Qu.:2225  </span></span>
<span><span class="co">#&gt;  Median :22.8   Median :4.00   Median :151   Median : 93.5   Median :2804  </span></span>
<span><span class="co">#&gt;  Mean   :23.4   Mean   :5.47   Mean   :194   Mean   :104.5   Mean   :2978  </span></span>
<span><span class="co">#&gt;  3rd Qu.:29.0   3rd Qu.:8.00   3rd Qu.:276   3rd Qu.:126.0   3rd Qu.:3615  </span></span>
<span><span class="co">#&gt;  Max.   :46.6   Max.   :8.00   Max.   :455   Max.   :230.0   Max.   :5140  </span></span>
<span><span class="co">#&gt;                                                                            </span></span>
<span><span class="co">#&gt;   acceleration       year        origin                     name    </span></span>
<span><span class="co">#&gt;  Min.   : 8.0   Min.   :70   Min.   :1.00   amc matador       :  5  </span></span>
<span><span class="co">#&gt;  1st Qu.:13.8   1st Qu.:73   1st Qu.:1.00   ford pinto        :  5  </span></span>
<span><span class="co">#&gt;  Median :15.5   Median :76   Median :1.00   toyota corolla    :  5  </span></span>
<span><span class="co">#&gt;  Mean   :15.5   Mean   :76   Mean   :1.58   amc gremlin       :  4  </span></span>
<span><span class="co">#&gt;  3rd Qu.:17.0   3rd Qu.:79   3rd Qu.:2.00   amc hornet        :  4  </span></span>
<span><span class="co">#&gt;  Max.   :24.8   Max.   :82   Max.   :3.00   chevrolet chevette:  4  </span></span>
<span><span class="co">#&gt;                                             (Other)           :365</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span> <span class="op">~</span> <span class="va">year</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; year</span></span>
<span><span class="co">#&gt; 70 71 72 73 74 75 76 77 78 79 80 81 82 </span></span>
<span><span class="co">#&gt; 29 27 28 40 26 30 34 28 36 29 27 28 30</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span> <span class="op">~</span> <span class="va">origin</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; origin</span></span>
<span><span class="co">#&gt;   1   2   3 </span></span>
<span><span class="co">#&gt; 245  68  79</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span> <span class="op">~</span> <span class="va">cylinders</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; cylinders</span></span>
<span><span class="co">#&gt;   3   4   5   6   8 </span></span>
<span><span class="co">#&gt;   4 199   3  83 103</span></span></code></pre></div>
<p>We previously used the <code>Auto</code> here in a preliminary
example where we employed CV to inform the selection of the order of a
polynomial regression of <code>mpg</code> on <code>horsepower</code>.
Here, we consider more generally the problem of predicting
<code>mpg</code> from the other variables in the <code>Auto</code> data.
We begin with a bit of data management, and then examine the pairwise
relationships among the numeric variables in the data set:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Auto</span><span class="op">$</span><span class="va">cylinders</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">cylinders</span>,</span>
<span>                         labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"3.4"</span>, <span class="st">"3.4"</span>, <span class="st">"5.6"</span>, <span class="st">"5.6"</span>, <span class="st">"8"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Auto</span><span class="op">$</span><span class="va">year</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">year</span><span class="op">)</span></span>
<span><span class="va">Auto</span><span class="op">$</span><span class="va">origin</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">origin</span>,</span>
<span>                      labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"America"</span>, <span class="st">"Europe"</span>, <span class="st">"Japan"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/make.names.html" class="external-link">make.names</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">name</span>, unique <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">Auto</span><span class="op">$</span><span class="va">name</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span></span>
<span>  <span class="op">~</span> <span class="va">mpg</span> <span class="op">+</span> <span class="va">displacement</span> <span class="op">+</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="va">weight</span> <span class="op">+</span> <span class="va">acceleration</span>,</span>
<span>  smooth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Auto</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-explore-1.png" alt="Scatterplot matrix for the numeric variables in the `Auto` data" width="100%"><p class="caption">
Scatterplot matrix for the numeric variables in the <code>Auto</code>
data
</p>
</div>
<p>A comment before we proceed: <code>origin</code> is clearly
categorical and so converting it to a factor is natural, but we could
imagine treating <code>cylinders</code> and <code>year</code> as numeric
predictors. There are, however, only 5 distinct values of
<code>cylinders</code> (ranging from 3 to 8), but cars with 3 or 5
cylinders are rare. and none of the cars has 7 cylinders. There are
similarly only 13 distinct years between 1970 and 1982 in the data, and
the relationship between <code>mpg</code> and <code>year</code> is
difficult to characterize.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Of course, making the decision to treat
&lt;code&gt;year&lt;/code&gt; as a factor on this basis could be construed as
cheating in the current context, which illustrates the difficulty of
automating the whole model-selection process. It’s rarely desirable, in
our opinion, to forgo exploration of the data to ensure the purity of
model validation. We believe, however, that it’s still useful to
automate as much of the process as we can to obtain a more realistic, if
still biased, estimate of the predictive power of a model.&lt;/p&gt;"><sup>3</sup></a> It’s apparent that most these variables are
positively skewed and that many of the pairwise relationships among them
are nonlinear.</p>
<p>We begin with a “working model” that specifies linear partial
relationships of the response to the numeric predictors:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ ., data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -9.006 -1.745 -0.092  1.525 10.950 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  37.034132   1.969393   18.80  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; cylinders5.6 -2.602941   0.655200   -3.97  8.5e-05 ***</span></span>
<span><span class="co">#&gt; cylinders8   -0.582458   1.171452   -0.50  0.61934    </span></span>
<span><span class="co">#&gt; displacement  0.017425   0.006734    2.59  0.01004 *  </span></span>
<span><span class="co">#&gt; horsepower   -0.041353   0.013379   -3.09  0.00215 ** </span></span>
<span><span class="co">#&gt; weight       -0.005548   0.000632   -8.77  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration  0.061527   0.088313    0.70  0.48643    </span></span>
<span><span class="co">#&gt; year71        0.968058   0.837390    1.16  0.24841    </span></span>
<span><span class="co">#&gt; year72       -0.601435   0.825115   -0.73  0.46652    </span></span>
<span><span class="co">#&gt; year73       -0.687689   0.740272   -0.93  0.35351    </span></span>
<span><span class="co">#&gt; year74        1.375576   0.876500    1.57  0.11741    </span></span>
<span><span class="co">#&gt; year75        0.929929   0.859072    1.08  0.27974    </span></span>
<span><span class="co">#&gt; year76        1.559893   0.822505    1.90  0.05867 .  </span></span>
<span><span class="co">#&gt; year77        2.909416   0.841729    3.46  0.00061 ***</span></span>
<span><span class="co">#&gt; year78        3.175198   0.798940    3.97  8.5e-05 ***</span></span>
<span><span class="co">#&gt; year79        5.019299   0.845759    5.93  6.8e-09 ***</span></span>
<span><span class="co">#&gt; year80        9.099763   0.897293   10.14  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year81        6.688660   0.885218    7.56  3.3e-13 ***</span></span>
<span><span class="co">#&gt; year82        8.071125   0.870668    9.27  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; originEurope  2.046664   0.517124    3.96  9.1e-05 ***</span></span>
<span><span class="co">#&gt; originJapan   2.144887   0.507717    4.22  3.0e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.92 on 371 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.867,  Adjusted R-squared:  0.86 </span></span>
<span><span class="co">#&gt; F-statistic:  121 on 20 and 371 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: mpg</span></span>
<span><span class="co">#&gt;              Sum Sq  Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; cylinders       292   2   17.09 7.9e-08 ***</span></span>
<span><span class="co">#&gt; displacement     57   1    6.70  0.0100 *  </span></span>
<span><span class="co">#&gt; horsepower       82   1    9.55  0.0021 ** </span></span>
<span><span class="co">#&gt; weight          658   1   76.98 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration      4   1    0.49  0.4864    </span></span>
<span><span class="co">#&gt; year           3017  12   29.40 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; origin          190   2   11.13 2.0e-05 ***</span></span>
<span><span class="co">#&gt; Residuals      3173 371                    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-working-model-1.png" alt="Component+residual plots for the working model fit to the `Auto` data" width="100%"><p class="caption">
Component+residual plots for the working model fit to the
<code>Auto</code> data
</p>
</div>
<p>The component+residual plots, created with the <code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots()</a></code>
function in the previously loaded <strong>car</strong> package, clearly
reveal the inadequacy of the model.</p>
<p>We proceed to transform the numeric predictors towards
multi-normality:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">num.predictors</span> <span class="op">&lt;-</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"displacement"</span>, <span class="st">"horsepower"</span>, <span class="st">"weight"</span>, <span class="st">"acceleration"</span><span class="op">)</span></span>
<span><span class="va">tr.x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">[</span>, <span class="va">num.predictors</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">tr.x</span><span class="op">)</span></span>
<span><span class="co">#&gt; bcPower Transformations to Multinormality </span></span>
<span><span class="co">#&gt;              Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; displacement   -0.0509           0      -0.2082       0.1065</span></span>
<span><span class="co">#&gt; horsepower     -0.1249           0      -0.2693       0.0194</span></span>
<span><span class="co">#&gt; weight         -0.0870           0      -0.2948       0.1208</span></span>
<span><span class="co">#&gt; acceleration    0.3061           0      -0.0255       0.6376</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that transformation parameters are equal to 0</span></span>
<span><span class="co">#&gt;  (all log transformations)</span></span>
<span><span class="co">#&gt;                                LRT df  pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0 0 0 0) 4.8729  4 0.301</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that no transformations are needed</span></span>
<span><span class="co">#&gt;                                LRT df   pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (1 1 1 1) 390.08  4 &lt;2e-16</span></span></code></pre></div>
<p>We then apply the (rounded) transformations—all, as it turns out,
logs—to the data and re-estimate the model:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">A</span> <span class="op">&lt;-</span> <span class="va">Auto</span></span>
<span><span class="va">powers</span> <span class="op">&lt;-</span> <span class="va">tr.x</span><span class="op">$</span><span class="va">roundlam</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">pred</span> <span class="kw">in</span> <span class="va">num.predictors</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">A</span><span class="op">[</span>, <span class="va">pred</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/bcPower.html" class="external-link">bcPower</a></span><span class="op">(</span><span class="va">A</span><span class="op">[</span>, <span class="va">pred</span><span class="op">]</span>, lambda <span class="op">=</span> <span class="va">powers</span><span class="op">[</span><span class="va">pred</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">A</span><span class="op">)</span></span>
<span><span class="co">#&gt;                           mpg cylinders displacement horsepower weight</span></span>
<span><span class="co">#&gt; chevrolet.chevelle.malibu  18         8       5.7268     4.8675 8.1617</span></span>
<span><span class="co">#&gt; buick.skylark.320          15         8       5.8579     5.1059 8.2142</span></span>
<span><span class="co">#&gt; plymouth.satellite         18         8       5.7621     5.0106 8.1421</span></span>
<span><span class="co">#&gt; amc.rebel.sst              16         8       5.7170     5.0106 8.1412</span></span>
<span><span class="co">#&gt; ford.torino                17         8       5.7104     4.9416 8.1458</span></span>
<span><span class="co">#&gt; ford.galaxie.500           15         8       6.0615     5.2883 8.3759</span></span>
<span><span class="co">#&gt;                           acceleration year  origin</span></span>
<span><span class="co">#&gt; chevrolet.chevelle.malibu       2.4849   70 America</span></span>
<span><span class="co">#&gt; buick.skylark.320               2.4423   70 America</span></span>
<span><span class="co">#&gt; plymouth.satellite              2.3979   70 America</span></span>
<span><span class="co">#&gt; amc.rebel.sst                   2.4849   70 America</span></span>
<span><span class="co">#&gt; ford.torino                     2.3514   70 America</span></span>
<span><span class="co">#&gt; ford.galaxie.500                2.3026   70 America</span></span>
<span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">m.auto</span>, data <span class="op">=</span> <span class="va">A</span><span class="op">)</span></span></code></pre></div>
<p>Finally, we perform Box-Cox regression to transform the response
(also obtaining a log transformation):</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; bcPower Transformation to Normality </span></span>
<span><span class="co">#&gt;    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; Y1    0.0024           0      -0.1607       0.1654</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that transformation parameter is equal to 0</span></span>
<span><span class="co">#&gt;  (log transformation)</span></span>
<span><span class="co">#&gt;                              LRT df  pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0) 0.00080154  1 0.977</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that no transformation is needed</span></span>
<span><span class="co">#&gt;                          LRT df   pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (1) 124.13  1 &lt;2e-16</span></span>
<span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">m</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(mpg) ~ cylinders + displacement + horsepower + </span></span>
<span><span class="co">#&gt;     weight + acceleration + year + origin, data = A)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -0.3341 -0.0577  0.0041  0.0607  0.3808 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)    8.8965     0.3582   24.84  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; cylinders5.6  -0.0636     0.0257   -2.47    0.014 *  </span></span>
<span><span class="co">#&gt; cylinders8    -0.0769     0.0390   -1.97    0.049 *  </span></span>
<span><span class="co">#&gt; displacement   0.0280     0.0515    0.54    0.587    </span></span>
<span><span class="co">#&gt; horsepower    -0.2901     0.0563   -5.15  4.2e-07 ***</span></span>
<span><span class="co">#&gt; weight        -0.5427     0.0819   -6.62  1.2e-10 ***</span></span>
<span><span class="co">#&gt; acceleration  -0.1421     0.0563   -2.52    0.012 *  </span></span>
<span><span class="co">#&gt; year71         0.0250     0.0289    0.87    0.387    </span></span>
<span><span class="co">#&gt; year72        -0.0168     0.0289   -0.58    0.562    </span></span>
<span><span class="co">#&gt; year73        -0.0426     0.0260   -1.64    0.103    </span></span>
<span><span class="co">#&gt; year74         0.0493     0.0304    1.62    0.106    </span></span>
<span><span class="co">#&gt; year75         0.0472     0.0296    1.59    0.112    </span></span>
<span><span class="co">#&gt; year76         0.0709     0.0284    2.49    0.013 *  </span></span>
<span><span class="co">#&gt; year77         0.1324     0.0293    4.52  8.2e-06 ***</span></span>
<span><span class="co">#&gt; year78         0.1447     0.0278    5.21  3.1e-07 ***</span></span>
<span><span class="co">#&gt; year79         0.2335     0.0292    7.99  1.7e-14 ***</span></span>
<span><span class="co">#&gt; year80         0.3238     0.0317   10.22  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year81         0.2565     0.0309    8.29  2.1e-15 ***</span></span>
<span><span class="co">#&gt; year82         0.3076     0.0304   10.13  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; originEurope   0.0492     0.0195    2.52    0.012 *  </span></span>
<span><span class="co">#&gt; originJapan    0.0441     0.0195    2.26    0.024 *  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.104 on 371 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.911,  Adjusted R-squared:  0.906 </span></span>
<span><span class="co">#&gt; F-statistic:  189 on 20 and 371 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: log(mpg)</span></span>
<span><span class="co">#&gt;              Sum Sq  Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; cylinders      0.07   2    3.05   0.048 *  </span></span>
<span><span class="co">#&gt; displacement   0.00   1    0.30   0.587    </span></span>
<span><span class="co">#&gt; horsepower     0.29   1   26.54 4.2e-07 ***</span></span>
<span><span class="co">#&gt; weight         0.48   1   43.88 1.2e-10 ***</span></span>
<span><span class="co">#&gt; acceleration   0.07   1    6.37   0.012 *  </span></span>
<span><span class="co">#&gt; year           4.45  12   34.13 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; origin         0.08   2    3.71   0.025 *  </span></span>
<span><span class="co">#&gt; Residuals      4.03 371                    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>The transformed numeric variables are much better-behaved:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span></span>
<span>  <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span> <span class="op">+</span> <span class="va">displacement</span> <span class="op">+</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="va">weight</span></span>
<span>  <span class="op">+</span> <span class="va">acceleration</span>,</span>
<span>  smooth <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">A</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-transformed-scatterplot-matrix-1.png" alt="Scatterplot matrix for the transformed numeric variables in the `Auto` data" width="100%"><p class="caption">
Scatterplot matrix for the transformed numeric variables in the
<code>Auto</code> data
</p>
</div>
<p>And the partial relationships in the model fit to the transformed
data are much more nearly linear:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-CR-plots-transformed-1.png" alt="Component+residual plots for the model fit to the transformed `Auto` data" width="100%"><p class="caption">
Component+residual plots for the model fit to the transformed
<code>Auto</code> data
</p>
</div>
<p>Having transformed both the numeric predictors and the response, we
proceed to use the <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> function in the
<strong>MASS</strong> package to perform predictor selection, employing
the BIC model-selection criterion (by setting the <code>k</code>
argument of <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\log(n)</annotation></semantics></math>):</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.step</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC</a></span><span class="op">(</span><span class="va">m</span>, k<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">A</span><span class="op">)</span><span class="op">)</span>, trace<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(mpg) ~ horsepower + weight + acceleration + </span></span>
<span><span class="co">#&gt;     year + origin, data = A)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -0.3523 -0.0568  0.0068  0.0674  0.3586 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   9.43459    0.26153   36.07  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; horsepower   -0.27625    0.05614   -4.92  1.3e-06 ***</span></span>
<span><span class="co">#&gt; weight       -0.60907    0.05600  -10.88  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration -0.13138    0.05319   -2.47  0.01397 *  </span></span>
<span><span class="co">#&gt; year71        0.02798    0.02894    0.97  0.33412    </span></span>
<span><span class="co">#&gt; year72       -0.00711    0.02845   -0.25  0.80274    </span></span>
<span><span class="co">#&gt; year73       -0.03953    0.02601   -1.52  0.12947    </span></span>
<span><span class="co">#&gt; year74        0.05275    0.02999    1.76  0.07936 .  </span></span>
<span><span class="co">#&gt; year75        0.05320    0.02928    1.82  0.07004 .  </span></span>
<span><span class="co">#&gt; year76        0.07432    0.02821    2.63  0.00878 ** </span></span>
<span><span class="co">#&gt; year77        0.13793    0.02888    4.78  2.6e-06 ***</span></span>
<span><span class="co">#&gt; year78        0.14588    0.02753    5.30  2.0e-07 ***</span></span>
<span><span class="co">#&gt; year79        0.23604    0.02908    8.12  7.0e-15 ***</span></span>
<span><span class="co">#&gt; year80        0.33527    0.03115   10.76  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year81        0.26287    0.03056    8.60  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year82        0.32339    0.02961   10.92  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; originEurope  0.05582    0.01678    3.33  0.00097 ***</span></span>
<span><span class="co">#&gt; originJapan   0.04355    0.01748    2.49  0.01314 *  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.105 on 374 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.909,  Adjusted R-squared:  0.905 </span></span>
<span><span class="co">#&gt; F-statistic:  220 on 17 and 374 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: log(mpg)</span></span>
<span><span class="co">#&gt;              Sum Sq  Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; horsepower     0.27   1   24.21 1.3e-06 ***</span></span>
<span><span class="co">#&gt; weight         1.30   1  118.28 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration   0.07   1    6.10  0.0140 *  </span></span>
<span><span class="co">#&gt; year           4.76  12   36.05 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; origin         0.14   2    6.21  0.0022 ** </span></span>
<span><span class="co">#&gt; Residuals      4.11 374                    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>The selected model includes three of the numeric predictors,
<code>horsepower</code>, <code>weight</code>, and
<code>acceleration</code>, along with the factors <code>year</code> and
<code>origin</code>. We can calculate the MSE for this model, but we
expect that the result will be optimistic because we used the whole data
to help specify the model</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6.5121</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>This is considerably smaller than the MSE for the original working
model:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 8.0932</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>A perhaps subtle point is that we compute the MSE for the selected
model on the original <code>mpg</code> response scale rather than the
log scale, so as to make the selected model comparable to the working
model. That’s slightly uncomfortable given the skewed distribution of
<code>mpg</code>. An alternative is to use the median absolute error
instead of the mean-squared error, employing the
<code><a href="../reference/cost-functions.html">medAbsErr()</a></code> function from the <strong>cv</strong>
package:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cost-functions.html">medAbsErr</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.3396</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">medAbsErr</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.6661</span></span></code></pre></div>
<p>Now let’s use <code><a href="../reference/cv.html">cv()</a></code> with
<code>selectTransAndStepAIC()</code> to automate and cross-validate the
whole model-specification process:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">num.predictors</span></span>
<span><span class="co">#&gt; [1] "displacement" "horsepower"   "weight"       "acceleration"</span></span>
<span><span class="va">cvs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="va">selectTransStepAIC</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Auto</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">76692</span>,</span>
<span>  working.model <span class="op">=</span> <span class="va">m.auto</span>,</span>
<span>  predictors <span class="op">=</span> <span class="va">num.predictors</span>,</span>
<span>  response <span class="op">=</span> <span class="st">"mpg"</span>,</span>
<span>  AIC <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  criterion <span class="op">=</span> <span class="va">medAbsErr</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 76692</span></span>
<span><span class="va">cvs</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: medAbsErr</span></span>
<span><span class="co">#&gt; cross-validation criterion = 1.4951</span></span>
<span><span class="co">#&gt; full-sample criterion = 1.3396</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.function.html">compareFolds</a></span><span class="op">(</span><span class="va">cvs</span><span class="op">)</span></span>
<span><span class="co">#&gt; CV criterion by folds:</span></span>
<span><span class="co">#&gt;  fold.1  fold.2  fold.3  fold.4  fold.5  fold.6  fold.7  fold.8  fold.9 fold.10 </span></span>
<span><span class="co">#&gt;  1.5639  1.5766  1.3698  1.2381  1.2461  1.5826  1.3426  1.1905  1.1355  1.5854 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients by folds:</span></span>
<span><span class="co">#&gt;         (Intercept) horsepower lam.acceleration lam.displacement lam.horsepower</span></span>
<span><span class="co">#&gt; Fold 1      9.71384   -0.17408          0.50000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 2      9.21713   -0.31480          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 3      9.61824   -0.19248          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 4      8.69910   -0.25523          0.50000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 5      9.14403   -0.14934          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 6      9.63481   -0.16739          0.50000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 7      9.98933   -0.36847          0.00000          0.00000       -0.15447</span></span>
<span><span class="co">#&gt; Fold 8      9.06301   -0.29721          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 9      8.88315   -0.22684          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 10     9.61727   -0.17086          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt;         lam.weight   lambda   weight   year71   year72   year73   year74</span></span>
<span><span class="co">#&gt; Fold 1     0.00000  0.00000 -0.74636  0.03764 -0.00327 -0.02477  0.05606</span></span>
<span><span class="co">#&gt; Fold 2     0.00000  0.00000 -0.47728  0.02173 -0.01488 -0.03770  0.04312</span></span>
<span><span class="co">#&gt; Fold 3     0.00000  0.00000 -0.72085  0.01128 -0.02569 -0.03872  0.05187</span></span>
<span><span class="co">#&gt; Fold 4     0.00000  0.00000 -0.53846  0.02153 -0.02922 -0.05181  0.04136</span></span>
<span><span class="co">#&gt; Fold 5     0.00000  0.00000 -0.69081  0.02531 -0.01062 -0.04625  0.05039</span></span>
<span><span class="co">#&gt; Fold 6     0.00000  0.00000 -0.74049  0.02456  0.00759 -0.03412  0.06266</span></span>
<span><span class="co">#&gt; Fold 7     0.00000  0.00000 -0.72843  0.02532 -0.01271 -0.04144  0.04568</span></span>
<span><span class="co">#&gt; Fold 8     0.00000  0.00000 -0.46392  0.02702 -0.02041 -0.05605  0.04437</span></span>
<span><span class="co">#&gt; Fold 9     0.00000  0.00000 -0.47136  0.00860 -0.03620 -0.04835  0.01906</span></span>
<span><span class="co">#&gt; Fold 10    0.00000  0.00000 -0.73550  0.02937 -0.00899 -0.03814  0.05408</span></span>
<span><span class="co">#&gt;           year75   year76   year77   year78   year79   year80   year81   year82</span></span>
<span><span class="co">#&gt; Fold 1   0.07080  0.07250  0.14420  0.14281  0.23266  0.35127  0.25635  0.30546</span></span>
<span><span class="co">#&gt; Fold 2   0.04031  0.06718  0.13094  0.14917  0.21871  0.33192  0.26196  0.30943</span></span>
<span><span class="co">#&gt; Fold 3   0.03837  0.06399  0.11593  0.12601  0.20499  0.32821  0.24478  0.29204</span></span>
<span><span class="co">#&gt; Fold 4   0.04072  0.05537  0.12292  0.14083  0.22878  0.32947  0.25140  0.27248</span></span>
<span><span class="co">#&gt; Fold 5   0.05596  0.07044  0.13356  0.14724  0.24675  0.33331  0.26938  0.32594</span></span>
<span><span class="co">#&gt; Fold 6   0.06940  0.07769  0.14211  0.14647  0.23532  0.34761  0.26737  0.33062</span></span>
<span><span class="co">#&gt; Fold 7   0.03614  0.07385  0.12976  0.14040  0.23976  0.33998  0.27652  0.30659</span></span>
<span><span class="co">#&gt; Fold 8   0.06573  0.08135  0.13158  0.13987  0.23011  0.32880  0.25886  0.30538</span></span>
<span><span class="co">#&gt; Fold 9   0.03018  0.05846  0.10536  0.11722  0.20665  0.31533  0.23352  0.29375</span></span>
<span><span class="co">#&gt; Fold 10  0.04881  0.07862  0.14101  0.14313  0.23258  0.35649  0.26214  0.32421</span></span>
<span><span class="co">#&gt;         acceleration displacement cylinders5.6 cylinders8 originEurope</span></span>
<span><span class="co">#&gt; Fold 1                                                                </span></span>
<span><span class="co">#&gt; Fold 2      -0.18909     -0.09197                                     </span></span>
<span><span class="co">#&gt; Fold 3                                                                </span></span>
<span><span class="co">#&gt; Fold 4      -0.03484                  -0.09080   -0.10909             </span></span>
<span><span class="co">#&gt; Fold 5                                                         0.06261</span></span>
<span><span class="co">#&gt; Fold 6                                                                </span></span>
<span><span class="co">#&gt; Fold 7                                                                </span></span>
<span><span class="co">#&gt; Fold 8      -0.17676     -0.10542                                     </span></span>
<span><span class="co">#&gt; Fold 9      -0.14514     -0.13452                                     </span></span>
<span><span class="co">#&gt; Fold 10                                                               </span></span>
<span><span class="co">#&gt;         originJapan</span></span>
<span><span class="co">#&gt; Fold 1             </span></span>
<span><span class="co">#&gt; Fold 2             </span></span>
<span><span class="co">#&gt; Fold 3             </span></span>
<span><span class="co">#&gt; Fold 4             </span></span>
<span><span class="co">#&gt; Fold 5         0.04</span></span>
<span><span class="co">#&gt; Fold 6             </span></span>
<span><span class="co">#&gt; Fold 7             </span></span>
<span><span class="co">#&gt; Fold 8             </span></span>
<span><span class="co">#&gt; Fold 9             </span></span>
<span><span class="co">#&gt; Fold 10</span></span></code></pre></div>
<p>Here, as for <code><a href="../reference/cv.function.html">selectTrans()</a></code>, the <code>predictors</code>
and <code>response</code> arguments specify candidate variables for
transformation, and <code>AIC=FALSE</code> uses the BIC for model
selection. The starting model, <code>m.auto</code>, is the working model
fit to the <code>Auto</code> data. The CV criterion isn’t bias-adjusted
because median absolute error isn’t a mean of casewise error
components.</p>
<p>Some noteworthy points:</p>
<ul>
<li>
<code><a href="../reference/cv.function.html">selectTransStepAIC()</a></code> automatically computes CV cost
criteria, here the median absolute error, on the untransformed response
scale.</li>
<li>The estimate of the median absolute error that we obtain by
cross-validating the whole model-specification process is a little
larger than the median absolute error computed for the model we fit to
the <code>Auto</code> data separately selecting transformations of the
predictors and the response and then selecting predictors for the whole
data set.</li>
<li>When we look at the transformations and predictors selected with
each of the 10 folds omitted (i.e., the output of
<code><a href="../reference/cv.function.html">compareFolds()</a></code>), we see that there is little uncertainty in
choosing variable transformations (the <code>lam.*</code>s for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>s
and <code>lambda</code> for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
in the output), but considerably more uncertainty in subsequently
selecting predictors: <code>horsepower</code>, <code>weight</code>, and
<code>year</code> are always included among the selected predictors;
<code>acceleration</code> and <code>displacement</code> are each
included respectively in 4 and 3 of 10 selected models; and
<code>cylinders</code> and <code>origin</code> are each included in only
1 of 10 models. Recall that when we selected predictors for the full
data, we obtained a model with <code>horsepower</code>,
<code>weight</code>, <code>acceleration</code>, <code>year</code>, and
<code>origin</code>.</li>
</ul>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-BoxCox:1964" class="csl-entry">
Box, G. E. P., &amp; Cox, D. R. (1964). An analysis of transformations.
<em>Journal of the Royal Statistical Society, Series
<span>B</span></em>, <em>26</em>, 211–252.
</div>
<div id="ref-FoxWeisberg:2019" class="csl-entry">
Fox, J., &amp; Weisberg, S. (2019). <em>An <span>R</span> companion to
applied regression</em> (Third edition). Thousand Oaks <span>CA</span>:
Sage.
</div>
<div id="ref-HastieTibshiraniFriedman:2009" class="csl-entry">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements
of statistical learning: Data mining, inference, and prediction</em>
(Second edition). New York: Springer. Retrieved from <a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf" class="external-link">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</div>
<div id="ref-JamesEtAl:2021" class="csl-entry">
James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). <em>An
introduction to statistical learning with applications in
<span>R</span></em> (Second edition). New York: Springer.
</div>
<div id="ref-VenablesRipley:2002" class="csl-entry">
Venables, W. N., &amp; Ripley, B. D. (2002). <em>Modern applied
statistics with <span>S</span></em> (Fourth edition). New York:
Springer.
</div>
<div id="ref-Weisberg:2014" class="csl-entry">
Weisberg, S. (2014). <em>Applied linear regression</em> (Second
edition). Hoboken <span>NJ</span>: Wiley.
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by John Fox, Georges Monette.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
