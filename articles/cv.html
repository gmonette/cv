<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="cv">
<title>Cross-validation of regression models • cv</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Cross-validation of regression models">
<meta property="og:description" content="cv">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cv</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">1.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item">
  <a class="nav-link" href="../articles/cv.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/cv-extend.html">Extending the cv package</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/gmonette/cv/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Cross-validation of regression models</h1>
                        <h4 data-toc-skip class="author">John Fox and
Georges Monette</h4>
            
            <h4 data-toc-skip class="date">2024-02-18</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/gmonette/cv/blob/HEAD/vignettes/cv.Rmd" class="external-link"><code>vignettes/cv.Rmd</code></a></small>
      <div class="d-none name"><code>cv.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="cross-validation">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h2>
<p>Cross-validation (CV) is an essentially simple and intuitively
reasonable approach to estimating the predictive accuracy of regression
models. CV is developed in many standard sources on regression modeling
and “machine learning”—we particularly recommend <span class="citation">James, Witten, Hastie, &amp; Tibshirani (2021, secs.
5.1, 5.3)</span>—and so we will describe the method only briefly here
before taking up computational issues and some examples. See <span class="citation">Arlot &amp; Celisse (2010)</span> for a wide-ranging,
if technical, survey of cross-validation and related methods that
emphasizes the statistical properties of CV.</p>
<p>Validating research by replication on independently collected data is
a common scientific norm. Emulating this process in a single study by
data-division is less common: The data are randomly divided into two,
possibly equal-size, parts; the first part is used to develop and fit a
statistical model; and then the second part is used to assess the
adequacy of the model fit to the first part of the data. Data-division,
however, suffers from two problems: (1) Dividing the data decreases the
sample size and thus increases sampling error; and (2), even more
disconcertingly, particularly in smaller samples, the results can vary
substantially based on the random division of the data: See <span class="citation">Harrell (2015, sec. 5.3)</span> for this and other
remarks about data-division and cross-validation.</p>
<p>Cross-validation speaks to both of these issues. In CV, the data are
randomly divided as equally as possible into several, say <span class="math inline">\(k\)</span>, parts, called “folds.” The statistical
model is fit <span class="math inline">\(k\)</span> times, leaving each
fold out in turn. Each fitted model is then used to predict the response
variable for the cases in the omitted fold. A CV criterion or “cost”
measure, such as the mean-squared error (“MSE”) of prediction, is then
computed using these predicted values. In the extreme <span class="math inline">\(k = n\)</span>, the number of cases in the data,
thus omitting individual cases and refitting the model <span class="math inline">\(n\)</span> times—a procedure termed “leave-one-out
(LOO) cross-validation.”</p>
<p>Because the <span class="math inline">\(n\)</span> models are each
fit to <span class="math inline">\(n - 1\)</span> cases, LOO CV produces
a nearly unbiased estimate of prediction error. The <span class="math inline">\(n\)</span> regression models are highly
statistical dependent, however, based as they are on nearly the same
data, and so the resulting estimate of prediction error has relatively
large variance. In contrast, estimated prediction error for <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(10\)</span> (commonly employed choices) are
somewhat biased but have smaller variance. It is also possible to
correct <span class="math inline">\(k\)</span>-fold CV for bias (see
below).</p>
</div>
<div class="section level2">
<h2 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h2>
<div class="section level3">
<h3 id="polynomial-regression-for-the-auto-data">Polynomial regression for the <code>Auto</code> data<a class="anchor" aria-label="anchor" href="#polynomial-regression-for-the-auto-data"></a>
</h3>
<p>The data for this example are drawn from the <strong>ISLR2</strong>
package for R, associated with <span class="citation">James et al.
(2021)</span>. The presentation here is close (though not identical) to
that in the original source <span class="citation">(James et al., 2021,
secs. 5.1, 5.3)</span>, and it demonstrates the use of the
<code><a href="../reference/cv.html">cv()</a></code> function in the <strong>cv</strong> package.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;span class="citation"&gt;James et al. (2021)&lt;/span&gt; use
the &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv.glm()&lt;/a&gt;&lt;/code&gt; function in the &lt;strong&gt;boot&lt;/strong&gt; package
&lt;span class="citation"&gt;(Canty &amp;amp; Ripley, 2022; Davison &amp;amp; Hinkley,
1997)&lt;/span&gt;. Despite its name, &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv.glm()&lt;/a&gt;&lt;/code&gt; is an independent
function and not a method of a &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; generic function.&lt;/p&gt;'><sup>1</sup></a></p>
<p>The <code>Auto</code> dataset contains information about 392
cars:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Auto"</span>, package<span class="op">=</span><span class="st">"ISLR2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mpg cylinders displacement horsepower weight acceleration year origin</span></span>
<span><span class="co">#&gt; 1  18         8          307        130   3504         12.0   70      1</span></span>
<span><span class="co">#&gt; 2  15         8          350        165   3693         11.5   70      1</span></span>
<span><span class="co">#&gt; 3  18         8          318        150   3436         11.0   70      1</span></span>
<span><span class="co">#&gt; 4  16         8          304        150   3433         12.0   70      1</span></span>
<span><span class="co">#&gt; 5  17         8          302        140   3449         10.5   70      1</span></span>
<span><span class="co">#&gt; 6  15         8          429        198   4341         10.0   70      1</span></span>
<span><span class="co">#&gt;                        name</span></span>
<span><span class="co">#&gt; 1 chevrolet chevelle malibu</span></span>
<span><span class="co">#&gt; 2         buick skylark 320</span></span>
<span><span class="co">#&gt; 3        plymouth satellite</span></span>
<span><span class="co">#&gt; 4             amc rebel sst</span></span>
<span><span class="co">#&gt; 5               ford torino</span></span>
<span><span class="co">#&gt; 6          ford galaxie 500</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 392   9</span></span></code></pre></div>
<p>With the exception of <code>origin</code> (which we don’t use here),
these variables are largely self-explanatory, except possibly for units
of measurement: for details see
<code><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html" class="external-link">help("Auto", package="ISLR2")</a></code>.</p>
<p>We’ll focus here on the relationship of <code>mpg</code> (miles per
gallon) to <code>horsepower</code>, as displayed in the following
scatterplot:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/mpg-horsepower-scatterplot-1.png" alt="`mpg` vs `horsepower` for the `Auto` data" width="100%"><p class="caption">
<code>mpg</code> vs <code>horsepower</code> for the <code>Auto</code>
data
</p>
</div>
<p>The relationship between the two variables is monotone, decreasing,
and nonlinear. Following <span class="citation">James et al.
(2021)</span>, we’ll consider approximating the relationship by a
polynomial regression, with the degree of the polynomial <span class="math inline">\(p\)</span> ranging from 1 (a linear regression) to
10.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Although it serves to illustrate the use of CV, a
polynomial is probably not the best choice here. Consider, for example
the scatterplot for log-transformed &lt;code&gt;mpg&lt;/code&gt; and
&lt;code&gt;horsepower&lt;/code&gt;, produced by
&lt;code&gt;plot(mpg ~ horsepower, data=Auto, log="xy")&lt;/code&gt; (execution of
which is left to the reader).&lt;/p&gt;'><sup>2</sup></a>
Polynomial fits for <span class="math inline">\(p = 1\)</span> to <span class="math inline">\(5\)</span> are shown in the following figure:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="va">horsepower</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Auto</span>, </span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span>, </span>
<span>                       length<span class="op">=</span><span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>,<span class="va">p</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span>  <span class="va">mpg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">m</span>, newdata<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>horsepower<span class="op">=</span><span class="va">horsepower</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">mpg</span>, col<span class="op">=</span><span class="va">p</span> <span class="op">+</span> <span class="fl">1</span>, lty<span class="op">=</span><span class="va">p</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, legend<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, col<span class="op">=</span><span class="fl">2</span><span class="op">:</span><span class="fl">6</span>, lty<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>       title<span class="op">=</span><span class="st">"Degree"</span>, inset<span class="op">=</span><span class="fl">0.02</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/mpg-horsepower-scatterplot-polynomials-1.png" alt="`mpg` vs `horsepower` for the `Auto` data" width="100%"><p class="caption">
<code>mpg</code> vs <code>horsepower</code> for the <code>Auto</code>
data
</p>
</div>
<p>The linear fit is clearly inappropriate; the fits for <span class="math inline">\(p = 2\)</span> (quadratic) through <span class="math inline">\(4\)</span> are very similar; and the fit for <span class="math inline">\(p = 5\)</span> may over-fit the data by chasing
one or two relatively high <code>mpg</code> values at the right (but see
the CV results reported below).</p>
<p>The following graph shows two measures of estimated (squared) error
as a function of polynomial-regression degree: The mean-squared error
(“MSE”), defined as <span class="math inline">\(\mathsf{MSE} =
\frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_i)^2\)</span>, and the usual
residual variance, defined as <span class="math inline">\(\widehat{\sigma}^2 = \frac{1}{n - p - 1}
\sum_{i=1}^n (y_i - \widehat{y}_i)^2\)</span>. The former necessarily
declines with <span class="math inline">\(p\)</span> (or, more strictly,
can’t increase with <span class="math inline">\(p\)</span>), while the
latter gets slightly larger for the largest values of <span class="math inline">\(p\)</span>, with the “best” value, by a small
margin, for <span class="math inline">\(p = 7\)</span>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://gmonette.github.io/cv/">"cv"</a></span><span class="op">)</span> <span class="co"># for mse() and other functions</span></span>
<span><span class="co">#&gt; Loading required package: doParallel</span></span>
<span><span class="co">#&gt; Loading required package: foreach</span></span>
<span><span class="co">#&gt; Loading required package: iterators</span></span>
<span><span class="co">#&gt; Loading required package: parallel</span></span>
<span></span>
<span><span class="va">var</span> <span class="op">&lt;-</span> <span class="va">mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">p</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span>  <span class="va">mse</span><span class="op">[</span><span class="va">p</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">var</span><span class="op">[</span><span class="va">p</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span><span class="op">^</span><span class="fl">2</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/range.html" class="external-link">range</a></span><span class="op">(</span><span class="va">mse</span>, <span class="va">var</span><span class="op">)</span>, type<span class="op">=</span><span class="st">"n"</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"Degree of polynomial, p"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Estimated Squared Error"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">mse</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">1</span>, col<span class="op">=</span><span class="fl">2</span>, pch<span class="op">=</span><span class="fl">16</span>, type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">var</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">2</span>, col<span class="op">=</span><span class="fl">3</span>, pch<span class="op">=</span><span class="fl">17</span>, type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, inset<span class="op">=</span><span class="fl">0.02</span>,</span>
<span>       legend<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hat</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, <span class="st">"MSE"</span><span class="op">)</span>,</span>
<span>       lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">2</span><span class="op">:</span><span class="fl">1</span>, col<span class="op">=</span><span class="fl">3</span><span class="op">:</span><span class="fl">2</span>, pch<span class="op">=</span><span class="fl">17</span><span class="op">:</span><span class="fl">16</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/mpg-horsepower-MSE-se-1.png" alt="Estimated squared error as a function of polynomial degree, $p$" width="100%"><p class="caption">
Estimated squared error as a function of polynomial degree, <span class="math inline">\(p\)</span>
</p>
</div>
<p>The code for this graph uses the <code><a href="../reference/cost-functions.html">mse()</a></code> function from the
<strong>cv</strong> package to compute the MSE for each fit.</p>
<div class="section level4">
<h4 id="using-cv">Using <code>cv()</code><a class="anchor" aria-label="anchor" href="#using-cv"></a>
</h4>
<p>The generic <code><a href="../reference/cv.html">cv()</a></code> function has an <code>"lm"</code>
method, which by default performs <span class="math inline">\(k =
10\)</span>-fold CV:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 2), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 563306</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.211</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.199</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>The <code>"lm"</code> method by default uses <code><a href="../reference/cost-functions.html">mse()</a></code> as
the CV criterion and the Woodbury matrix identity to update the
regression with each fold deleted without having literally to refit the
model. Computational details are discussed in the final section of this
vignette. The function reports the CV estimate of MSE, a biased-adjusted
estimate of the MSE (the bias adjustment is explained in the final
section), and the MSE is also computed for the original, full-sample
regression. Because the division of the data into 10 folds is random,
<code><a href="../reference/cv.html">cv()</a></code> explicitly (randomly) generates and saves a seed for
R’s pseudo-random number generator, to make the results replicable. The
user can also specify the seed directly via the <code>seed</code>
argument to <code><a href="../reference/cv.html">cv()</a></code>.</p>
<p>To perform LOO CV, we can set the <code>k</code> argument to
<code><a href="../reference/cv.html">cv()</a></code> to the number of cases in the data, here
<code>k=392</code>, or, more conveniently, to <code>k="loo"</code> or
<code>k="n"</code>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span></code></pre></div>
<p>For LOO CV of a linear model, <code><a href="../reference/cv.html">cv()</a></code> by default uses the
hatvalues from the model fit to the full data for the LOO updates, and
reports only the CV estimate of MSE. Alternative methods are to use the
Woodbury matrix identity or the “naive” approach of literally refitting
the model with each case omitted. All three methods produce exact
results for a linear model (within the precision of floating-point
computations):</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"naive"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>The <code>"naive"</code> and <code>"Woodbury"</code> methods also
return the bias-adjusted estimate of MSE and the full-sample MSE, but
bias isn’t an issue for LOO CV.</p>
<p>This is a small regression problem and all three computational
approaches are essentially instantaneous, but it is still of interest to
investigate their relative speed. In this comparison, we include the
<code><a href="../reference/cv.html">cv.glm()</a></code> function from the <strong>boot</strong> package,
which takes the naive approach, and for which we have to fit the linear
model as an equivalent Gaussian GLM. We use the
<code>microbenchmark()</code> function from the package of the same name
for the timings <span class="citation">(Mersmann, 2023)</span>:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto.glm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Auto</span>, <span class="va">m.auto.glm</span><span class="op">)</span><span class="op">$</span><span class="va">delta</span></span>
<span><span class="co">#&gt; [1] 19.248 19.248</span></span>
<span></span>
<span><span class="fu">microbenchmark</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>  hatvalues <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span><span class="op">)</span>,</span>
<span>  Woodbury <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span>,</span>
<span>  naive <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"naive"</span><span class="op">)</span>,</span>
<span>  cv.glm <span class="op">=</span> <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Auto</span>, <span class="va">m.auto.glm</span><span class="op">)</span>,</span>
<span>  times<span class="op">=</span><span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Unit: milliseconds</span></span>
<span><span class="co">#&gt;       expr      min       lq     mean   median       uq      max neval</span></span>
<span><span class="co">#&gt;  hatvalues   2.1278   2.1845   2.4001   2.4037   2.5287   2.8708    10</span></span>
<span><span class="co">#&gt;   Woodbury  22.0136  22.3063  23.5176  22.5724  24.5964  26.9542    10</span></span>
<span><span class="co">#&gt;      naive 474.1098 475.4085 505.5228 478.4832 493.9655 606.4865    10</span></span>
<span><span class="co">#&gt;     cv.glm 819.9689 823.1973 839.3771 827.4482 833.3517 949.2143    10</span></span></code></pre></div>
<p>On our computer, using the hatvalues is about an order of magnitude
faster than employing Woodbury matrix updates, and more than two orders
of magnitude faster than refitting the model.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Out of impatience, we asked
&lt;code&gt;microbenchmark()&lt;/code&gt; to execute each command only 10 times
rather than the default 100. With the exception of the last columns, the
output is self-explanatory. The last column shows which methods have
average timings that are statistically distinguishable. Because of the
small number of repetitions (i.e., 10), the &lt;code&gt;"hatvalues"&lt;/code&gt; and
&lt;code&gt;"Woodbury"&lt;/code&gt; methods aren’t distinguishable, but the
difference between these methods persists when we perform more
repetitions—we invite the reader to redo this computation with the
default &lt;code&gt;times=100&lt;/code&gt; repetitions.&lt;/p&gt;'><sup>3</sup></a></p>
</div>
<div class="section level4">
<h4 id="comparing-competing-models">Comparing competing models<a class="anchor" aria-label="anchor" href="#comparing-competing-models"></a>
</h4>
<p>The <code><a href="../reference/cv.html">cv()</a></code> function also has a method that can be applied
to a list of regression models for the same data, composed using the
<code><a href="../reference/models.html">models()</a></code> function. For <span class="math inline">\(k\)</span>-fold CV, the same folds are used for
the competing models, which reduces random error in their comparison.
This result can also be obtained by specifying a common seed for R’s
random-number generator while applying <code><a href="../reference/cv.html">cv()</a></code> separately to
each model, but employing a list of models is more convenient for both
<span class="math inline">\(k\)</span>-fold and LOO CV (where there is
no random component to the composition of the <span class="math inline">\(n\)</span> folds).</p>
<p>We illustrate with the polynomial regression models of varying degree
for the <code>Auto</code> data (discussed previously), beginning by
fitting and saving the 10 models:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/assign.html" class="external-link">assign</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"m."</span>, <span class="va">p</span><span class="op">)</span>,</span>
<span>         <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">p</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/ls.html" class="external-link">objects</a></span><span class="op">(</span>pattern<span class="op">=</span><span class="st">"m\\.[0-9]"</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] "m.1"  "m.10" "m.2"  "m.3"  "m.4"  "m.5"  "m.6"  "m.7"  "m.8"  "m.9"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.2</span><span class="op">)</span> <span class="co"># for example, the quadratic fit</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, p), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, p)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, p)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span></code></pre></div>
<p>We then apply <code><a href="../reference/cv.html">cv()</a></code> to the list of 10 models (the
<code>data</code> argument is required):</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 10-fold CV</span></span>
<span><span class="va">cv.auto.10</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="fu"><a href="../reference/models.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>                     <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>              data<span class="op">=</span><span class="va">Auto</span>, seed<span class="op">=</span><span class="fl">2120</span><span class="op">)</span></span>
<span><span class="va">cv.auto.10</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span> <span class="co"># for the linear and quadratic models</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.1:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.246</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 24.23</span></span>
<span><span class="co">#&gt; full-sample criterion = 23.944 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.2:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.346</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.327</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span>
<span></span>
<span><span class="co"># LOO CV</span></span>
<span><span class="va">cv.auto.loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="fu"><a href="../reference/models.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>                        <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>                 data<span class="op">=</span><span class="va">Auto</span>, k<span class="op">=</span><span class="st">"loo"</span><span class="op">)</span></span>
<span><span class="va">cv.auto.loo</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span> <span class="co"># linear and quadratic models</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.1:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.232</span></span>
<span><span class="co">#&gt; Model model.2:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span></code></pre></div>
<p>Because we didn’t supply names for the models in the calls to the
<code><a href="../reference/models.html">models()</a></code> function, the names <code>model.1</code>,
<code>model.2</code>, etc., are generated by the function.</p>
<p>Finally, we extract and graph the adjusted MSEs for <span class="math inline">\(10\)</span>-fold CV and the MSEs for LOO CV:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.mse.10</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">cv.auto.10</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[[</span><span class="st">"adj CV crit"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="va">cv.mse.loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="va">cv.auto.loo</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[[</span><span class="st">"CV crit"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/range.html" class="external-link">range</a></span><span class="op">(</span><span class="va">cv.mse.10</span>, <span class="va">cv.mse.loo</span><span class="op">)</span>, type<span class="op">=</span><span class="st">"n"</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"Degree of polynomial, p"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Cross-Validated MSE"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">cv.mse.10</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">1</span>, col<span class="op">=</span><span class="fl">2</span>, pch<span class="op">=</span><span class="fl">16</span>, type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">cv.mse.loo</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">2</span>, col<span class="op">=</span><span class="fl">3</span>, pch<span class="op">=</span><span class="fl">17</span>, type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, inset<span class="op">=</span><span class="fl">0.02</span>,</span>
<span>       legend<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"10-Fold CV"</span>, <span class="st">"LOO CV"</span><span class="op">)</span>,</span>
<span>       lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">2</span><span class="op">:</span><span class="fl">1</span>, col<span class="op">=</span><span class="fl">3</span><span class="op">:</span><span class="fl">2</span>, pch<span class="op">=</span><span class="fl">17</span><span class="op">:</span><span class="fl">16</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/polynomial-regression-CV-graph-1.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$" width="100%"><p class="caption">
Cross-validated 10-fold and LOO MSE as a function of polynomial degree,
<span class="math inline">\(p\)</span>
</p>
</div>
<p>Alternatively, we can use the <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method for
<code>"cvModList"</code> objects to compare the models, though with
separate graphs for 10-fold and LOO CV:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.auto.10</span>, main<span class="op">=</span><span class="st">"Polynomial Regressions, 10-Fold CV"</span>,</span>
<span>     axis.args<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>labels<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, xlab<span class="op">=</span><span class="st">"Degree of Polynomial, p"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.auto.loo</span>, main<span class="op">=</span><span class="st">"Polynomial Regressions, LOO CV"</span>,</span>
<span>     axis.args<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>labels<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, xlab<span class="op">=</span><span class="st">"Degree of Polynomial, p"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/polynomial-regression-CV-graph-2-1.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$" width="45%"><img src="fig/polynomial-regression-CV-graph-2-2.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$" width="45%"><p class="caption">
Cross-validated 10-fold and LOO MSE as a function of polynomial degree,
<span class="math inline">\(p\)</span>
</p>
</div>
<p>In this example, 10-fold and LOO CV produce generally similar
results, and also results that are similar to those produced by the
estimated error variance <span class="math inline">\(\widehat{\sigma}^2\)</span> for each model,
reported above (except for the highest-degree polynomials, where the CV
results more clearly suggest over-fitting).</p>
</div>
</div>
<div class="section level3">
<h3 id="logistic-regression-for-the-mroz-data">Logistic regression for the <code>Mroz</code> data<a class="anchor" aria-label="anchor" href="#logistic-regression-for-the-mroz-data"></a>
</h3>
<p>The <code>Mroz</code> data set from the <strong>carData</strong>
package <span class="citation">(associated with Fox &amp; Weisberg,
2019)</span> has been used by several authors to illustrate binary
logistic regression; see, in particular <span class="citation">Fox &amp;
Weisberg (2019)</span>. The data were originally drawn from the U.S.
Panel Study of Income Dynamics and pertain to married women. Here are a
few cases in the data set:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Mroz"</span>, package<span class="op">=</span><span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   lfp k5 k618 age wc hc    lwg   inc</span></span>
<span><span class="co">#&gt; 1 yes  1    0  32 no no 1.2102 10.91</span></span>
<span><span class="co">#&gt; 2 yes  0    2  30 no no 0.3285 19.50</span></span>
<span><span class="co">#&gt; 3 yes  1    3  35 no no 1.5141 12.04</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;     lfp k5 k618 age wc hc     lwg    inc</span></span>
<span><span class="co">#&gt; 751  no  0    0  43 no no 0.88814  9.952</span></span>
<span><span class="co">#&gt; 752  no  0    0  60 no no 1.22497 24.984</span></span>
<span><span class="co">#&gt; 753  no  0    3  39 no no 0.85321 28.363</span></span></code></pre></div>
<p>The response variable in the logistic regression is <code>lfp</code>,
labor-force participation, a factor coded <code>"yes"</code> or
<code>"no"</code>. The remaining variables are predictors:</p>
<ul>
<li>
<code>k5</code>, number of children 5 years old of younger in the
woman’s household;</li>
<li>
<code>k618</code>, number of children between 6 and 18 years
old;</li>
<li>
<code>age</code>, in years;</li>
<li>
<code>wc</code>, wife’s college attendance, <code>"yes"</code> or
<code>"no"</code>;</li>
<li>
<code>hc</code>, husband’s college attendance;</li>
<li>
<code>lwg</code>, the woman’s log wage rate if she is employed, or
her <em>imputed</em> wage rate, if she is not <span class="citation">(a
variable that Fox &amp; Weisberg, 2019 show is problematically
defined)</span>; and</li>
<li>
<code>inc</code>, family income, in $1000s, exclusive of wife’s
income.</li>
</ul>
<p>We use the <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code> function to fit a binary logistic
regression to the <code>Mroz</code> data:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="va">Mroz</span>, family<span class="op">=</span><span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 921.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">BayesRule</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">$</span><span class="va">lfp</span> <span class="op">==</span> <span class="st">"yes"</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, </span>
<span>          <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.mroz</span>, type<span class="op">=</span><span class="st">"response"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.30677</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "y != round(yhat)"</span></span></code></pre></div>
<p>In addition to the usually summary output for a GLM, we show the
result of applying the <code><a href="../reference/cost-functions.html">BayesRule()</a></code> function from the
<strong>cv</strong> package to predictions derived from the fitted
model. Bayes rule, which predicts a “success” in a binary regression
model when the fitted probability of success [i.e., <span class="math inline">\(\phi = \Pr(y = 1)\)</span>] is <span class="math inline">\(\widehat{\phi} \ge .5\)</span> and a “failure” if
<span class="math inline">\(\widehat{\phi} \lt .5\)</span>.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;code&gt;&lt;a href="../reference/cost-functions.html"&gt;BayesRule()&lt;/a&gt;&lt;/code&gt; does some error checking;
&lt;code&gt;&lt;a href="../reference/cost-functions.html"&gt;BayesRule2()&lt;/a&gt;&lt;/code&gt; is similar, but omits the error checking, and
so can be faster for large problems.&lt;/p&gt;'><sup>4</sup></a> The first
argument to <code><a href="../reference/cost-functions.html">BayesRule()</a></code> is the binary {0, 1} response, and
the second argument is the predicted probability of success.
<code><a href="../reference/cost-functions.html">BayesRule()</a></code> returns the proportion of predictions that are
<em>in error</em>, as appropriate for a “cost” function.</p>
<p>The value returned by <code><a href="../reference/cost-functions.html">BayesRule()</a></code> is associated with an
“attribute” named <code>"casewise loss"</code> and set to
<code>"y != round(yhat)"</code>, signifying that the Bayes rule CV
criterion is computed as the mean of casewise values, here 0 if the
prediction for a case matches the observed value and 1 if it does not
(signifying a prediction error). The <code><a href="../reference/cost-functions.html">mse()</a></code> function for
numeric responses is also calculated as a casewise average. Some other
criteria, such as the median absolute error, computed by the
<code><a href="../reference/cost-functions.html">medAbsErr()</a></code> function in the <strong>cv</strong> package,
aren’t averages of casewise components. The distinction is important
because, to our knowledge, the statistical theory of cross-validation,
for example, in <span class="citation">Davison &amp; Hinkley
(1997)</span>, <span class="citation">S. Bates, Hastie, &amp; Tibshirani
(2023)</span>, and <span class="citation">Arlot &amp; Celisse
(2010)</span>, is developed for CV criteria like MSE that are means of
casewise components. As a consequence, we limit computation of bias
adjustment and confidence intervals (see below) to criteria that are
casewise averages.</p>
<p>In this example, the fitted logistic regression incorrectly predicts
31% of the responses; we expect this estimate to be optimistic given
that the model is used to “predict” the data to which it is fit.</p>
<p>The <code>"glm"</code> method for <code><a href="../reference/cv.html">cv()</a></code> is largely
similar to the <code>"lm"</code> method, although the default algorithm,
selected explicitly by <code>method="exact"</code>, refits the model
with each fold removed (and is thus equivalent to
<code>method="naive"</code> for <code>"lm"</code> models). For
generalized linear models, <code>method="Woodbury"</code> or (for LOO
CV) <code>method="hatvalues"</code> provide approximate results (see the
last section of the vignette for details):</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, seed<span class="op">=</span><span class="fl">248</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 248</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31952</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28607, 0.35297)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, seed<span class="op">=</span><span class="fl">248</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 248</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31926</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28581, 0.35271)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<p>To ensure that the two methods use the same 10 folds, we specify the
seed for R’s random-number generator explicitly; here, and as is common
in our experience, the <code>"exact"</code> and <code>"Woodbury"</code>
algorithms produce nearly identical results. The CV estimates of
prediction error are slightly higher than the estimate based on all of
the cases.</p>
<p>The printed output includes a 95% confidence interval for the
bias-adjusted Bayes rule CV criterion. <span class="citation">S. Bates
et al. (2023)</span> show that these confidence intervals are unreliable
for models fit to small samples, and by default <code><a href="../reference/cv.html">cv()</a></code>
computes them only when the sample size is 400 or larger and when the CV
criterion employed is an average of casewise components, as is the case
for Bayes rule. See the final section of the vignette for details of the
computation of confidence intervals for bias-adjusted CV criteria.</p>
<p>Here are results of applying LOO CV to the Mroz model, using both the
exact and the approximate methods:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"hatvalues"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span></code></pre></div>
<p>To the number of decimal digits shown, the three methods produce
identical results for this example.</p>
<p>As for linear models, we report some timings for the various
<code><a href="../reference/cv.html">cv()</a></code> methods of computation in LOO CV as well as for the
<code><a href="../reference/cv.html">cv.glm()</a></code> function from the <strong>boot</strong> package
(which, recall, refits the model with each case removed, and thus is
comparable to <code><a href="../reference/cv.html">cv()</a></code> with <code>method="exact"</code>):</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">microbenchmark</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>  hatvalues<span class="op">=</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"hatvalues"</span><span class="op">)</span>,</span>
<span>  Woodbury<span class="op">=</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span>,</span>
<span>  exact<span class="op">=</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span><span class="op">)</span>,</span>
<span>  cv.glm<span class="op">=</span><span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="va">m.mroz</span>,</span>
<span>               cost<span class="op">=</span><span class="va">BayesRule</span><span class="op">)</span>,</span>
<span>  times<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt; Unit: milliseconds</span></span>
<span><span class="co">#&gt;       expr       min        lq      mean    median        uq       max neval</span></span>
<span><span class="co">#&gt;  hatvalues    2.5094    2.5287    2.5758    2.5675    2.6183    2.6581    10</span></span>
<span><span class="co">#&gt;   Woodbury   70.6619   71.0422   75.7558   72.4104   74.0124  105.0607    10</span></span>
<span><span class="co">#&gt;      exact 2959.5798 3119.3383 3139.6205 3175.0152 3195.0768 3252.3593    10</span></span>
<span><span class="co">#&gt;     cv.glm 3638.6686 3653.8956 3726.7315 3743.2608 3780.9826 3827.9383    10</span></span></code></pre></div>
<p>There is a substantial time penalty associated with exact
computations.</p>
</div>
</div>
<div class="section level2">
<h2 id="cross-validating-mixed-effects-models">Cross-validating mixed-effects models<a class="anchor" aria-label="anchor" href="#cross-validating-mixed-effects-models"></a>
</h2>
<p>The fundamental analogy for cross-validation is to the collection of
new data. That is, predicting the response in each fold from the model
fit to data in the other folds is like using the model fit to all of the
data to predict the response for new cases from the values of the
predictors for those new cases. As we explained, the application of this
idea to independently sampled cases is straightforward—simply partition
the data into random folds of equal size and leave each fold out in
turn, or, in the case of LOO CV, simply omit each case in turn.</p>
<p>In contrast, mixed-effects models are fit to <em>dependent</em> data,
in which cases as clustered, such as hierarchical data, where the
clusters comprise higher-level units (e.g., students clustered in
schools), or longitudinal data, where the clusters are individuals and
the cases repeated observations on the individuals over time.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;There are, however, more complex situations that give
rise to so-called &lt;em&gt;crossed&lt;/em&gt; (rather than &lt;em&gt;nested&lt;/em&gt;) random
effects. For example, consider students within classes within schools.
In primary schools, students typically are in a single class, and so
classes are nested within schools. In secondary schools, however,
students typically take several classes and students who are together in
a particular class may not be together in other classes; consequently,
random effects based on classes within schools are crossed. The
&lt;code&gt;&lt;a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link"&gt;lmer()&lt;/a&gt;&lt;/code&gt; function in the &lt;strong&gt;lme4&lt;/strong&gt; package is
capable of modeling both nested and crossed random effects, and the
&lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; methods for mixed models in the &lt;strong&gt;cv&lt;/strong&gt;
package pertain to both nested and crossed random effects. We present an
example of the latter later in the vignette.&lt;/p&gt;'><sup>5</sup></a></p>
<p>We can think of two approaches to applying cross-validation to
clustered data:<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;We subsequently discovered that &lt;span class="citation"&gt;Vehtari (2023, sec. 8)&lt;/span&gt; makes similar points.&lt;/p&gt;'><sup>6</sup></a></p>
<ol style="list-style-type: decimal">
<li><p>Treat CV as analogous to predicting the response for one or more
cases in a <em>newly observed cluster</em>. In this instance, the folds
comprise one or more whole clusters; we refit the model with all of the
cases in clusters in the current fold removed; and then we predict the
response for the cases in clusters in the current fold. These
predictions are based only on fixed effects because the random effects
for the omitted clusters are presumably unknown, as they would be for
data on cases in newly observed clusters.</p></li>
<li><p>Treat CV as analogous to predicting the response for a newly
observed case in an <em>existing cluster</em>. In this instance, the
folds comprise one or more individual cases, and the predictions can use
both the fixed and random effects.</p></li>
</ol>
<div class="section level3">
<h3 id="example-the-high-school-and-beyond-data">Example: The High-School and Beyond data<a class="anchor" aria-label="anchor" href="#example-the-high-school-and-beyond-data"></a>
</h3>
<p>Following their use by <span class="citation">Raudenbush &amp; Bryk
(2002)</span>, data from the 1982 <em>High School and Beyond</em> (HSB)
survey have become a staple of the literature on mixed-effects models.
The HSB data are used by <span class="citation">Fox &amp; Weisberg
(2019, sec. 7.2.2)</span> to illustrate the application of linear mixed
models to hierarchical data, and we’ll closely follow their example
here.</p>
<p>The HSB data are included in the <code>MathAchieve</code> and
<code>MathAchSchool</code> data sets in the <strong>nlme</strong>
package <span class="citation">(Pinheiro &amp; Bates, 2000)</span>.
<code>MathAchieve</code> includes individual-level data on 7185 students
in 160 high schools, and <code>MathAchSchool</code> includes
school-level data:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"MathAchieve"</span>, package<span class="op">=</span><span class="st">"nlme"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">MathAchieve</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 7185    6</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">MathAchieve</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Grouped Data: MathAch ~ SES | School</span></span>
<span><span class="co">#&gt;   School Minority    Sex    SES MathAch MEANSES</span></span>
<span><span class="co">#&gt; 1   1224       No Female -1.528   5.876  -0.428</span></span>
<span><span class="co">#&gt; 2   1224       No Female -0.588  19.708  -0.428</span></span>
<span><span class="co">#&gt; 3   1224       No   Male -0.528  20.349  -0.428</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="va">MathAchieve</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Grouped Data: MathAch ~ SES | School</span></span>
<span><span class="co">#&gt;      School Minority    Sex    SES MathAch MEANSES</span></span>
<span><span class="co">#&gt; 7183   9586       No Female  1.332  19.641   0.627</span></span>
<span><span class="co">#&gt; 7184   9586       No Female -0.008  16.241   0.627</span></span>
<span><span class="co">#&gt; 7185   9586       No Female  0.792  22.733   0.627</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"MathAchSchool"</span>, package<span class="op">=</span><span class="st">"nlme"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">MathAchSchool</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 160   7</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">MathAchSchool</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt;      School Size Sector PRACAD DISCLIM HIMINTY MEANSES</span></span>
<span><span class="co">#&gt; 1224   1224  842 Public   0.35   1.597       0  -0.428</span></span>
<span><span class="co">#&gt; 1288   1288 1855 Public   0.27   0.174       0   0.128</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="va">MathAchSchool</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt;      School Size   Sector PRACAD DISCLIM HIMINTY MEANSES</span></span>
<span><span class="co">#&gt; 9550   9550 1532   Public   0.45   0.791       0   0.059</span></span>
<span><span class="co">#&gt; 9586   9586  262 Catholic   1.00  -2.416       0   0.627</span></span></code></pre></div>
<p>The first few students are in school number 1224 and the last few in
school 9586.</p>
<p>We’ll use only the <code>School</code>, <code>SES</code> (students’
socioeconomic status), and <code>MathAch</code> (their score on a
standardized math-achievement test) variables in the
<code>MathAchieve</code> data set, and <code>Sector</code>
(<code>"Catholic"</code> or <code>"Public"</code>) in the
<code>MathAchSchool</code> data set.</p>
<p>Some data-management is required before fitting a mixed-effects model
to the HSB data, for which we use the <strong>dplyr</strong> package
<span class="citation">(Wickham, François, Henry, Müller, &amp; Vaughan,
2023)</span>:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://dplyr.tidyverse.org" class="external-link">"dplyr"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'dplyr'</span></span>
<span><span class="co">#&gt; The following objects are masked from 'package:stats':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     filter, lag</span></span>
<span><span class="co">#&gt; The following objects are masked from 'package:base':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     intersect, setdiff, setequal, union</span></span>
<span><span class="va">MathAchieve</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html" class="external-link">group_by</a></span><span class="op">(</span><span class="va">School</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html" class="external-link">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html" class="external-link">summarize</a></span><span class="op">(</span>mean.ses <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">SES</span><span class="op">)</span><span class="op">)</span> <span class="op">-&gt;</span> <span class="va">Temp</span></span>
<span><span class="va">Temp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/merge.html" class="external-link">merge</a></span><span class="op">(</span><span class="va">MathAchSchool</span>, <span class="va">Temp</span>, by<span class="op">=</span><span class="st">"School"</span><span class="op">)</span></span>
<span><span class="va">HSB</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/merge.html" class="external-link">merge</a></span><span class="op">(</span><span class="va">Temp</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"School"</span>, <span class="st">"Sector"</span>, <span class="st">"mean.ses"</span><span class="op">)</span><span class="op">]</span>,</span>
<span>             <span class="va">MathAchieve</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"School"</span>, <span class="st">"SES"</span>, <span class="st">"MathAch"</span><span class="op">)</span><span class="op">]</span>, by<span class="op">=</span><span class="st">"School"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">HSB</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">HSB</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">HSB</span><span class="op">$</span><span class="va">cses</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">HSB</span>, <span class="va">ses</span> <span class="op">-</span> <span class="va">mean.ses</span><span class="op">)</span></span></code></pre></div>
<p>In the process, we created two new school-level variables:
<code>meanses</code>, which is the average SES for students in each
school; and <code>cses</code>, which is school-average SES centered at
its mean. For details, see <span class="citation">Fox &amp; Weisberg
(2019, sec. 7.2.2)</span>.</p>
<p>Still following Fox and Weisberg, we proceed to use the
<code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code> function in the <strong>lme4</strong> package <span class="citation">(D. Bates, Mächler, Bolker, &amp; Walker, 2015)</span>
to fit a mixed model for math achievement to the HSB data:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/lme4/lme4/" class="external-link">"lme4"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: Matrix</span></span>
<span><span class="va">hsb.lmer</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer</a></span><span class="op">(</span><span class="va">mathach</span> <span class="op">~</span> <span class="va">mean.ses</span><span class="op">*</span><span class="va">cses</span> <span class="op">+</span> <span class="va">sector</span><span class="op">*</span><span class="va">cses</span></span>
<span>                   <span class="op">+</span> <span class="op">(</span><span class="va">cses</span> <span class="op">|</span> <span class="va">school</span><span class="op">)</span>, data<span class="op">=</span><span class="va">HSB</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">hsb.lmer</span>, correlation<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML ['lmerMod']</span></span>
<span><span class="co">#&gt; Formula: mathach ~ mean.ses * cses + sector * cses + (cses | school)</span></span>
<span><span class="co">#&gt;    Data: HSB</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 46504</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -3.159 -0.723  0.017  0.754  2.958 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev. Corr</span></span>
<span><span class="co">#&gt;  school   (Intercept)  2.380   1.543        </span></span>
<span><span class="co">#&gt;           cses         0.101   0.318    0.39</span></span>
<span><span class="co">#&gt;  Residual             36.721   6.060        </span></span>
<span><span class="co">#&gt; Number of obs: 7185, groups:  school, 160</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;                     Estimate Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept)           12.128      0.199   60.86</span></span>
<span><span class="co">#&gt; mean.ses               5.333      0.369   14.45</span></span>
<span><span class="co">#&gt; cses                   2.945      0.156   18.93</span></span>
<span><span class="co">#&gt; sectorCatholic         1.227      0.306    4.00</span></span>
<span><span class="co">#&gt; mean.ses:cses          1.039      0.299    3.48</span></span>
<span><span class="co">#&gt; cses:sectorCatholic   -1.643      0.240   -6.85</span></span></code></pre></div>
<p>We can then cross-validate at the cluster (i.e., school) level,</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">hsb.lmer</span>, k<span class="op">=</span><span class="fl">10</span>, clusterVariables<span class="op">=</span><span class="st">"school"</span>, seed<span class="op">=</span><span class="fl">5240</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 5240</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation based on 160 {school} clusters</span></span>
<span><span class="co">#&gt; cross-validation criterion = 39.157</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 39.148</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (38.066, 40.231)</span></span>
<span><span class="co">#&gt; full-sample criterion = 39.006</span></span></code></pre></div>
<p>or at the case (i.e., student) level,</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">hsb.lmer</span>, seed<span class="op">=</span><span class="fl">1575</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 1575</span></span>
<span><span class="co">#&gt; Warning in checkConv(attr(opt, "derivs"), opt$par, ctrl = control$checkConv, :</span></span>
<span><span class="co">#&gt; Model failed to converge with max|grad| = 0.00587038 (tol = 0.002, component 1)</span></span>
<span><span class="co">#&gt; boundary (singular) fit: see help('isSingular')</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 37.445</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 37.338</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (36.288, 38.388)</span></span>
<span><span class="co">#&gt; full-sample criterion = 36.068</span></span></code></pre></div>
<p>For cluster-level CV, the <code>clusterVariables</code> argument
tells <code><a href="../reference/cv.html">cv()</a></code> how the clusters are defined. Were there more
than one clustering variable, say classes within schools, these would be
provided as a character vector of variable names:
<code>clusterVariables = c("school", "class")</code>. For cluster-level
CV, the default is <code>k = "loo"</code>, that is, leave one cluster
out at a time; we instead specify <code>k = 10</code> folds of clusters,
each fold therefore comprising <span class="math inline">\(160/10 =
16\)</span> schools.</p>
<p>If the <code>clusterVariables</code> argument is omitted, then
case-level CV is employed, with <code>k = 10</code> folds as the
default, here each with <span class="math inline">\(7185/10 \approx
719\)</span> students. Notice that one of the 10 models refit with a
fold removed failed to converge. Convergence problems are common in
mixed-effects modeling. The apparent issue here is that an estimated
variance component is close to or equal to 0, which is at a boundary of
the parameter space. That shouldn’t disqualify the fitted model for the
kind of prediction required for cross-validation.</p>
<p>There is also a <code><a href="../reference/cv.html">cv()</a></code> method for linear mixed models fit
by the <code><a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link">lme()</a></code> function in the <strong>nlme</strong> package,
and the arguments for <code><a href="../reference/cv.html">cv()</a></code> in this case are the same as for
a model fit by <code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code> or <code><a href="https://rdrr.io/pkg/lme4/man/glmer.html" class="external-link">glmer()</a></code>. We
illustrate with the mixed model fit to the HSB data:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://svn.r-project.org/R-packages/trunk/nlme/" class="external-link">"nlme"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'nlme'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:lme4':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     lmList</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:dplyr':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     collapse</span></span>
<span><span class="va">hsb.lme</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link">lme</a></span><span class="op">(</span><span class="va">mathach</span> <span class="op">~</span> <span class="va">mean.ses</span><span class="op">*</span><span class="va">cses</span> <span class="op">+</span> <span class="va">sector</span><span class="op">*</span><span class="va">cses</span>,</span>
<span>                 random <span class="op">=</span> <span class="op">~</span> <span class="va">cses</span> <span class="op">|</span> <span class="va">school</span>, data<span class="op">=</span><span class="va">HSB</span>,</span>
<span>               control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>opt<span class="op">=</span><span class="st">"optim"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">hsb.lme</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed-effects model fit by REML</span></span>
<span><span class="co">#&gt;   Data: HSB </span></span>
<span><span class="co">#&gt;     AIC   BIC logLik</span></span>
<span><span class="co">#&gt;   46525 46594 -23252</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Formula: ~cses | school</span></span>
<span><span class="co">#&gt;  Structure: General positive-definite, Log-Cholesky parametrization</span></span>
<span><span class="co">#&gt;             StdDev   Corr  </span></span>
<span><span class="co">#&gt; (Intercept) 1.541177 (Intr)</span></span>
<span><span class="co">#&gt; cses        0.018174 0.006 </span></span>
<span><span class="co">#&gt; Residual    6.063492       </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:  mathach ~ mean.ses * cses + sector * cses </span></span>
<span><span class="co">#&gt;                       Value Std.Error   DF t-value p-value</span></span>
<span><span class="co">#&gt; (Intercept)         12.1282   0.19920 7022  60.886   0e+00</span></span>
<span><span class="co">#&gt; mean.ses             5.3367   0.36898  157  14.463   0e+00</span></span>
<span><span class="co">#&gt; cses                 2.9421   0.15122 7022  19.456   0e+00</span></span>
<span><span class="co">#&gt; sectorCatholic       1.2245   0.30611  157   4.000   1e-04</span></span>
<span><span class="co">#&gt; mean.ses:cses        1.0444   0.29107 7022   3.588   3e-04</span></span>
<span><span class="co">#&gt; cses:sectorCatholic -1.6421   0.23312 7022  -7.044   0e+00</span></span>
<span><span class="co">#&gt;  Correlation: </span></span>
<span><span class="co">#&gt;                     (Intr) men.ss cses   sctrCt mn.ss:</span></span>
<span><span class="co">#&gt; mean.ses             0.256                            </span></span>
<span><span class="co">#&gt; cses                 0.000  0.000                     </span></span>
<span><span class="co">#&gt; sectorCatholic      -0.699 -0.356  0.000              </span></span>
<span><span class="co">#&gt; mean.ses:cses        0.000  0.000  0.295  0.000       </span></span>
<span><span class="co">#&gt; cses:sectorCatholic  0.000  0.000 -0.696  0.000 -0.351</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Standardized Within-Group Residuals:</span></span>
<span><span class="co">#&gt;       Min        Q1       Med        Q3       Max </span></span>
<span><span class="co">#&gt; -3.170106 -0.724877  0.014892  0.754263  2.965498 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Observations: 7185</span></span>
<span><span class="co">#&gt; Number of Groups: 160</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">hsb.lme</span>, k<span class="op">=</span><span class="fl">10</span>, clusterVariables<span class="op">=</span><span class="st">"school"</span>, seed<span class="op">=</span><span class="fl">5240</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 5240</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation based on 160 {school} clusters</span></span>
<span><span class="co">#&gt; cross-validation criterion = 39.157</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 39.149</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (38.066, 40.232)</span></span>
<span><span class="co">#&gt; full-sample criterion = 39.006</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">hsb.lme</span>, seed<span class="op">=</span><span class="fl">1575</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 1575</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 37.442</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 37.402</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (36.351, 38.453)</span></span>
<span><span class="co">#&gt; full-sample criterion = 36.147</span></span></code></pre></div>
<p>We used the same random-number generator seeds as in the previous
example cross-validating the model fit by <code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code>, and so
the same folds are employed in both cases.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;The observant reader will notice that we set the
argument &lt;code&gt;control=list(opt="optim")&lt;/code&gt; in the call to
&lt;code&gt;&lt;a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link"&gt;lme()&lt;/a&gt;&lt;/code&gt;, changing the optimizer employed from the default
&lt;code&gt;"nlminb"&lt;/code&gt;. We did this because with the default optimizer,
&lt;code&gt;&lt;a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link"&gt;lme()&lt;/a&gt;&lt;/code&gt; encountered the same convergence issue as
&lt;code&gt;&lt;a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link"&gt;lmer()&lt;/a&gt;&lt;/code&gt;, but rather than issuing a warning,
&lt;code&gt;&lt;a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link"&gt;lme()&lt;/a&gt;&lt;/code&gt; failed, reporting an error. As it turns out, setting
the optimizer to &lt;code&gt;"optim"&lt;/code&gt; avoids this problem.&lt;/p&gt;'><sup>7</sup></a> The estimated
covariance components and fixed effects in the summary output differ
slightly between the <code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code> and <code><a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link">lme()</a></code>
solutions, although both functions seek to maximize the REML criterion.
This is, of course, to be expected when different algorithms are used
for numerical optimization. To the precision reported, the cluster-level
CV results for the <code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code> and <code><a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link">lme()</a></code> models are
identical, while the case-level CV results are very similar but not
identical.</p>
</div>
<div class="section level3">
<h3 id="example-contrived-hierarchical-data">Example: Contrived hierarchical data<a class="anchor" aria-label="anchor" href="#example-contrived-hierarchical-data"></a>
</h3>
<p>We introduce an artificial data set that exemplifies aspects of
cross-validation particular to hierarchical models. Using this data set,
we show that model comparisons employing cluster-based and those
employing case-based cross-validation may not agree on a “best” model.
Furthermore, commonly used measures of fit, such as mean-squared error,
do not necessarily become smaller as models become larger, even when the
models are nested, and even when the measure of fit is computed for the
whole data set.</p>
<p>Consider a researcher studying improvement in a skill, yodeling, for
example, among students enrolled in a four-year yodeling program. The
plan is to measure each student’s skill level at the beginning of the
program and every year thereafter until the end of the program,
resulting in five annual measurements for each student. It turns out
that yodeling appeals to students of all ages, and students enrolling in
the program range in age from 20 to 70. Moreover, participants’
untrained yodeling skill is similar at all ages, as is their rate of
progress with training. All students complete the four-year program.</p>
<p>The researcher, who has more expertise in yodeling than in modeling,
decides to model the response, <span class="math inline">\(y\)</span>,
yodeling skill, as a function of age, <span class="math inline">\(x\)</span>, reasoning that students get older
during their stay in the program, and (incorrectly) that age can serve
as a proxy for elapsed time. The researcher knows that a mixed model
should be used to account for clustering due to the expected similarity
of measurements taken from each student.</p>
<p>We start by generating the data, using parameters consistent with the
description above and meant to highlight the issues that arise in
cross-validating mixed-effects models:<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;We invite the interested reader to experiment with
varying the parameters of our example.&lt;/p&gt;"><sup>8</sup></a></p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Parameters:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">9693</span><span class="op">)</span> </span>
<span><span class="va">Nb</span> <span class="op">&lt;-</span> <span class="fl">100</span>     <span class="co"># number of groups</span></span>
<span><span class="va">Nw</span> <span class="op">&lt;-</span> <span class="fl">5</span>       <span class="co"># number of individuals within groups</span></span>
<span><span class="va">Bb</span> <span class="op">&lt;-</span> <span class="fl">0</span>       <span class="co"># between-group regression coefficient on group mean</span></span>
<span><span class="va">SDre</span> <span class="op">&lt;-</span> <span class="fl">2.0</span>   <span class="co"># between-group SD of random level relative to group mean of x</span></span>
<span><span class="va">SDwithin</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># within group SD</span></span>
<span><span class="va">Bw</span> <span class="op">&lt;-</span> <span class="fl">1</span>          <span class="co"># within group effect of x</span></span>
<span><span class="va">Ay</span> <span class="op">&lt;-</span> <span class="fl">10</span>         <span class="co"># intercept for response</span></span>
<span><span class="va">Ax</span> <span class="op">&lt;-</span> <span class="fl">20</span>         <span class="co"># starting level of x</span></span>
<span><span class="va">Nx</span> <span class="op">&lt;-</span> <span class="va">Nw</span><span class="op">*</span><span class="fl">10</span>      <span class="co"># number of distinct x values</span></span>
<span></span>
<span><span class="va">Data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  group <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">Nb</span>, each<span class="op">=</span><span class="va">Nw</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  x <span class="op">=</span> <span class="va">Ax</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">Nx</span>, length.out <span class="op">=</span> <span class="va">Nw</span><span class="op">*</span><span class="va">Nb</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">within</a></span><span class="op">(</span></span>
<span>    <span class="op">{</span></span>
<span>      <span class="va">xm</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ave.html" class="external-link">ave</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">group</span>, FUN <span class="op">=</span> <span class="va">mean</span><span class="op">)</span> <span class="co"># within-group mean</span></span>
<span>      <span class="va">y</span> <span class="op">&lt;-</span> <span class="va">Ay</span> <span class="op">+</span></span>
<span>        <span class="va">Bb</span> <span class="op">*</span> <span class="va">xm</span> <span class="op">+</span>                    <span class="co"># contextual effect</span></span>
<span>        <span class="va">Bw</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">xm</span><span class="op">)</span> <span class="op">+</span>              <span class="co"># within-group effect</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">Nb</span>, sd<span class="op">=</span><span class="va">SDre</span><span class="op">)</span><span class="op">[</span><span class="va">group</span><span class="op">]</span> <span class="op">+</span>  <span class="co"># random level by group</span></span>
<span>        <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">Nb</span><span class="op">*</span><span class="va">Nw</span>, sd<span class="op">=</span><span class="va">SDwithin</span><span class="op">)</span>    <span class="co"># random error within groups</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
<p>Here is a scatterplot of the data for a representative group of 10
(without loss of generality, the first 10) of 100 students, showing the
95% concentration ellipse for each cluster:<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;We find it convenient to use the
&lt;strong&gt;lattice&lt;/strong&gt; &lt;span class="citation"&gt;(Sarkar, 2008)&lt;/span&gt;
and &lt;strong&gt;latticeExtra&lt;/strong&gt; &lt;span class="citation"&gt;(Sarkar &amp;amp;
Andrews, 2022)&lt;/span&gt; packages for this and other graphs in this
section.&lt;/p&gt;'><sup>9</sup></a></p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://lattice.r-forge.r-project.org/" class="external-link">"lattice"</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="http://latticeextra.r-forge.r-project.org/" class="external-link">"latticeExtra"</a></span><span class="op">)</span></span>
<span><span class="va">plot</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">Nx</span>, <span class="op">]</span>, group<span class="op">=</span><span class="va">group</span>,</span>
<span>               ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">16</span><span class="op">)</span>,</span>
<span>               par.settings<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>superpose.symbol<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>pch<span class="op">=</span><span class="fl">1</span>, cex<span class="op">=</span><span class="fl">0.7</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/pkg/latticeExtra/man/layer.html" class="external-link">layer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/latticeExtra/man/panel.ellipse.html" class="external-link">panel.ellipse</a></span><span class="op">(</span><span class="va">...</span>, center.cex<span class="op">=</span><span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">plot</span> <span class="co"># display graph</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/plot1-1.png" alt="Hierarchical data set, showing the first 10 of 100 students." width="100%"><p class="caption">
Hierarchical data set, showing the first 10 of 100 students.
</p>
</div>
<p>The between-student effect of age is 0 but the within-student effect
is 1. Due to the large variation in ages between students, the
least-squares regression of yodeling skill on age (for the 500
observations among all 100 students) produces an estimated slope close
to 0 (though with a small <span class="math inline">\(p\)</span>-value),
because the slope is heavily weighted toward the between-student
effect:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data<span class="op">=</span><span class="va">Data</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ x, data = Data)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -5.771 -1.658 -0.089  1.552  7.624 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  9.05043    0.34719   26.07   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; x            0.02091    0.00727    2.87   0.0042 ** </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.35 on 498 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.0163, Adjusted R-squared:  0.0143 </span></span>
<span><span class="co">#&gt; F-statistic: 8.26 on 1 and 498 DF,  p-value: 0.00422</span></span></code></pre></div>
<p>The initial mixed-effects model that we fit to the data is a simple
random-intercepts model:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># random intercept only:</span></span>
<span><span class="va">mod.0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fl">1</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">group</span><span class="op">)</span>, <span class="va">Data</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod.0</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML ['lmerMod']</span></span>
<span><span class="co">#&gt; Formula: y ~ 1 + (1 | group)</span></span>
<span><span class="co">#&gt;    Data: Data</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 2103.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -2.0351 -0.7264 -0.0117  0.7848  2.0438 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  group    (Intercept) 2.90     1.70    </span></span>
<span><span class="co">#&gt;  Residual             2.71     1.65    </span></span>
<span><span class="co">#&gt; Number of obs: 500, groups:  group, 100</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept)   10.002      0.186    53.9</span></span></code></pre></div>
<p>We will shortly consider three other, more complex, mixed models;
because of data-management considerations, it is convenient to fit them
now, but we defer discussion of these models:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># effect of x and random intercept:</span></span>
<span><span class="va">mod.1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">group</span><span class="op">)</span>, <span class="va">Data</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># effect of x, contextual (student) mean of x, and random intercept:</span></span>
<span><span class="va">mod.2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span> <span class="op">+</span> <span class="va">xm</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">group</span><span class="op">)</span>, <span class="va">Data</span><span class="op">)</span></span>
<span>        <span class="co"># equivalent to y ~ I(x - xm) + xm + (1 | group)</span></span>
<span></span>
<span><span class="co"># model generating the data (where Bb = 0)</span></span>
<span><span class="va">mod.3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html" class="external-link">I</a></span><span class="op">(</span><span class="va">x</span> <span class="op">-</span> <span class="va">xm</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">group</span><span class="op">)</span>, <span class="va">Data</span><span class="op">)</span></span></code></pre></div>
<p>We proceed to obtain predictions from the random-intercept model
(<code>mod.0</code>) and the other models (<code>mod.1</code>,
<code>mod.2</code>, and <code>mod.3</code>) based on fixed effects
alone, as would be used for cross-validation based on clusters (i.e.,
students), and for fixed and random effects—so-called best linear
unbiased predictions or BLUPs—as would be used for cross-validation
based on cases (i.e., occasions within students):</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">within</a></span><span class="op">(</span><span class="va">Data</span>, <span class="op">{</span></span>
<span>  <span class="va">fit_mod0.fe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.0</span>, re.form <span class="op">=</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span> <span class="co"># fixed effects only</span></span>
<span>  <span class="va">fit_mod0.re</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.0</span><span class="op">)</span> <span class="co"># fixed and random effects (BLUPs)</span></span>
<span>  <span class="va">fit_mod1.fe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.1</span>, re.form <span class="op">=</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  <span class="va">fit_mod1.re</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.1</span><span class="op">)</span></span>
<span>  <span class="va">fit_mod2.fe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.2</span>, re.form <span class="op">=</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  <span class="va">fit_mod2.re</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.2</span><span class="op">)</span></span>
<span>  <span class="va">fit_mod3.fe</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.3</span>, re.form <span class="op">=</span> <span class="op">~</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  <span class="va">fit_mod3.re</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">mod.3</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<p>We then prepare the data for plotting:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Data_long</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/reshape.html" class="external-link">reshape</a></span><span class="op">(</span><span class="va">Data</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">Nx</span>, <span class="op">]</span>, direction <span class="op">=</span> <span class="st">"long"</span>, sep <span class="op">=</span> <span class="st">"."</span>, </span>
<span>              timevar <span class="op">=</span> <span class="st">"effect"</span>, varying <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grep</a></span><span class="op">(</span><span class="st">"\\."</span>, <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">Data</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">Nx</span>, <span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Data_long</span><span class="op">$</span><span class="va">id</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">Data_long</span><span class="op">)</span></span>
<span><span class="va">Data_long</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/reshape.html" class="external-link">reshape</a></span><span class="op">(</span><span class="va">Data_long</span>, direction <span class="op">=</span> <span class="st">"long"</span>, sep <span class="op">=</span> <span class="st">"_"</span>, </span>
<span>              timevar <span class="op">=</span> <span class="st">"modelcode"</span>,  varying <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/grep.html" class="external-link">grep</a></span><span class="op">(</span><span class="st">"_"</span>, <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">Data_long</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Data_long</span><span class="op">$</span><span class="va">model</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"~ 1"</span>, <span class="st">"~ 1 + x"</span>, <span class="st">"~ 1 + x + xm"</span>, <span class="st">"~ 1 + I(x - xm)"</span><span class="op">)</span></span>
<span>  <span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/match.html" class="external-link">match</a></span><span class="op">(</span><span class="va">Data_long</span><span class="op">$</span><span class="va">modelcode</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"mod0"</span>, <span class="st">"mod1"</span>, <span class="st">"mod2"</span>, <span class="st">"mod3"</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Predictions based on the random-intercept model <code>mod.0</code>
for the first 10 students are shown in the following graph:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">plot</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod0"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"fe"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod0"</span> <span class="op">&amp;</span>  <span class="va">effect</span> <span class="op">==</span> <span class="st">"re"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span></span>
<span>  main<span class="op">=</span><span class="st">"Model: y ~ 1 + (1 | group)"</span>,</span>
<span>  key<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    corner<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.05</span><span class="op">)</span>,</span>
<span>    text<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fixed effects only"</span>,<span class="st">"fixed and random"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    lines<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/plot-fits-mod0-1.png" alt="Predictions from the random intercept model." width="100%"><p class="caption">
Predictions from the random intercept model.
</p>
</div>
<p>The fixed-effect predictions for the various individuals are
identical—the estimated fixed-effects intercept or estimated general
mean of <span class="math inline">\(y\)</span>—while the BLUPs are the
sums of the fixed-effects intercept and the random intercepts, and are
only slightly shrunken towards the general mean. Because in our
artificial data there is no population relationship between age and
skill, the fixed-effect-only predictions and the BLUPs are not very
different.</p>
<p>Our next model, <code>mod.1</code>, includes a fixed intercept and
fixed effect of <code>x</code> along with a random intercept:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod.1</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML ['lmerMod']</span></span>
<span><span class="co">#&gt; Formula: y ~ x + (1 | group)</span></span>
<span><span class="co">#&gt;    Data: Data</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 1564.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -2.9016 -0.6350  0.0188  0.5541  2.8293 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  group    (Intercept) 192.941  13.890  </span></span>
<span><span class="co">#&gt;  Residual               0.257   0.507  </span></span>
<span><span class="co">#&gt; Number of obs: 500, groups:  group, 100</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept) -33.9189     1.5645   -21.7</span></span>
<span><span class="co">#&gt; x             0.9653     0.0158    61.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;   (Intr)</span></span>
<span><span class="co">#&gt; x -0.460</span></span></code></pre></div>
<p>Predictions from this model appear in the following graph:</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">plot</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod1"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"fe"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod1"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"re"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span></span>
<span>  main<span class="op">=</span><span class="st">"Model: y ~ 1 + x + (1 | group)"</span>,</span>
<span>  ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">15</span>, <span class="fl">35</span><span class="op">)</span>,</span>
<span>  key<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    corner<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.95</span>, <span class="fl">0.05</span><span class="op">)</span>,</span>
<span>    text<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fixed effects only"</span>,<span class="st">"fixed and random"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    lines<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/plot-fits-mod1-1.png" alt="Predictions from the model with random intercepts and $x$ as a fixed-effect predictor." width="100%"><p class="caption">
Predictions from the model with random intercepts and <span class="math inline">\(x\)</span> as a fixed-effect predictor.
</p>
</div>
<p>The BLUPs fit the observed data very closely, but predictions based
on the fixed effects alone, with a common intercept and slope for all
clusters, are very poor—indeed, much worse than the fixed-effects-only
predictions based on the simpler random-intercept model,
<code>mod.0</code>. We therefore anticipate (and show later in this
section) that case-based cross-validation will prefer <code>mod1</code>
to <code>mod0</code>, but that cluster-based cross-validation will
prefer <code>mod0</code> to <code>mod1</code>.</p>
<p>Our third model, <code>mod.2</code>, includes the contextual effect
of <span class="math inline">\(x\)</span>—that is, the cluster mean
<code>xm</code>—along with <span class="math inline">\(x\)</span> and
the intercept in the fixed-effect part of the model, and a random
intercept:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod.2</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML ['lmerMod']</span></span>
<span><span class="co">#&gt; Formula: y ~ x + xm + (1 | group)</span></span>
<span><span class="co">#&gt;    Data: Data</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 1169.2</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -2.9847 -0.6375  0.0019  0.5568  2.7325 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  group    (Intercept) 3.399    1.844   </span></span>
<span><span class="co">#&gt;  Residual             0.255    0.505   </span></span>
<span><span class="co">#&gt; Number of obs: 500, groups:  group, 100</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept)   9.4787     0.6171    15.4</span></span>
<span><span class="co">#&gt; x             0.9915     0.0160    62.1</span></span>
<span><span class="co">#&gt; xm           -0.9800     0.0206   -47.7</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;    (Intr) x     </span></span>
<span><span class="co">#&gt; x   0.000       </span></span>
<span><span class="co">#&gt; xm -0.600 -0.777</span></span></code></pre></div>
<p>This model is equivalent to fitting
<code>y ~ I(x - xm) + xm + (1 | group)</code>, which is the model that
generated the data once the coefficient of the contextual predictor
<code>xm</code> is set to 0 (as it is in <code>mod.3</code>, discussed
below).</p>
<p>Predictions from model <code>mod.2</code> appear in the following
graph:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">plot</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod2"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"fe"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod2"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"re"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span></span>
<span>  main<span class="op">=</span><span class="st">"Model: y ~ 1 + x + xm + (1 | group)"</span>,</span>
<span>  ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">16</span><span class="op">)</span>,</span>
<span>  key<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    corner<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.05</span><span class="op">)</span>,</span>
<span>    text<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fixed effects only"</span>,<span class="st">"fixed and random"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    lines<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/plot-fits-mod2-1.png" alt="Predictors from the model with random intercepts, $x$, and the group (student) mean of $x$ as predictors." width="100%"><p class="caption">
Predictors from the model with random intercepts, <span class="math inline">\(x\)</span>, and the group (student) mean of <span class="math inline">\(x\)</span> as predictors.
</p>
</div>
<p>Depending on the estimated variance parameters of the model, a mixed
model like <code>mod.2</code> will apply varying degrees of shrinkage to
the random-intercept BLUPs that correspond to variation in the heights
of the parallel fitted lines for the individual students. In our
contrived data, the <code>mod.2</code> applies little shrinkage,
allowing substantial variability in the heights of the fitted lines,
which closely approach the observed values for each student. The fit of
the mixed model <code>mod.2</code> is consequently similar to that of a
fixed-effects model with age and a categorical predictor for individual
students (i.e., treating students as a factor, and not shown here).</p>
<p>The mixed model <code>mod.2</code> therefore fits individual
observations well, and we anticipate a favorable assessment using
individual-based cross-validation. In contrast, the large variability in
the BLUPs results in larger residuals for predictions based on fixed
effects alone, and so we expect that cluster-based cross-validation
won’t show an advantage for model <code>mod.2</code> compared to the
smaller model <code>mod.0</code>, which includes only fixed and random
intercepts.</p>
<p>Had the mixed model applied considerable shrinkage, then neither
cluster-based nor case-based cross-validation would show much
improvement over the random-intercept-only model. In our experience, the
degree of shrinkage does not vary smoothly as parameters are changed but
tends to be “all or nothing,” and near the tipping point, the behavior
of estimates can be affected considerably by the choice of algorithm
used to fit the model.</p>
<p>Finally, <code>mod.3</code> directly estimates the model used to
generate the data. As mentioned, it is a constrained version of
<code>mod.2</code>, with the coefficient of <code>xm</code> set to 0,
and with <code>x</code> expressed as a deviation from the cluster mean
<code>xm</code>:</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mod.3</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by REML ['lmerMod']</span></span>
<span><span class="co">#&gt; Formula: y ~ I(x - xm) + (1 | group)</span></span>
<span><span class="co">#&gt;    Data: Data</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; REML criterion at convergence: 1163.2</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -2.9770 -0.6320  0.0063  0.5603  2.7249 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  group    (Intercept) 3.391    1.842   </span></span>
<span><span class="co">#&gt;  Residual             0.255    0.505   </span></span>
<span><span class="co">#&gt; Number of obs: 500, groups:  group, 100</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept)   10.002      0.185    53.9</span></span>
<span><span class="co">#&gt; I(x - xm)      0.992      0.016    62.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;           (Intr)</span></span>
<span><span class="co">#&gt; I(x - xm) 0.000</span></span></code></pre></div>
<p>The predictions from <code>mod.3</code> are therefore similar to
those from <code>mod.2</code>:</p>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="op">(</span><span class="va">plot</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod3"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"fe"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/lattice/man/xyplot.html" class="external-link">xyplot</a></span><span class="op">(</span><span class="va">fit</span> <span class="op">~</span> <span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">Data_long</span>, <span class="va">modelcode</span> <span class="op">==</span> <span class="st">"mod3"</span> <span class="op">&amp;</span> <span class="va">effect</span> <span class="op">==</span> <span class="st">"re"</span><span class="op">)</span>,</span>
<span>         groups<span class="op">=</span><span class="va">group</span>, type<span class="op">=</span><span class="st">"l"</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span></span>
<span>  main<span class="op">=</span><span class="st">"Model: y ~ 1 + I(x - xm) + (1 | group)"</span>,</span>
<span>  ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">16</span><span class="op">)</span>,</span>
<span>  key<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>    corner<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.05</span><span class="op">)</span>,</span>
<span>    text<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fixed effects only"</span>,<span class="st">"fixed and random"</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    lines<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lty<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/plot-fits-mod3-1.png" alt="Predictions from the estimated model generating the data." width="100%"><p class="caption">
Predictions from the estimated model generating the data.
</p>
</div>
<p>We next carry out case-based cross-validation, which, as we have
explained, is based on both fixed and predicted random effects (i.e.,
BLUPs), and cluster-based cross-validation, which is based on fixed
effects only. In order to reduce between-model random variability in
comparisons of models, we apply <code><a href="../reference/cv.html">cv()</a></code> to the list of models
created by the <code><a href="../reference/models.html">models()</a></code> function (introduced previously),
performing cross-validation with the same folds for each model:</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">modlist</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/models.html">models</a></span><span class="op">(</span><span class="st">"~ 1"</span><span class="op">=</span><span class="va">mod.0</span>, <span class="st">"~ 1 + x"</span><span class="op">=</span><span class="va">mod.1</span>, </span>
<span>                  <span class="st">"~ 1 + x + xm"</span><span class="op">=</span><span class="va">mod.2</span>, <span class="st">"~ 1 + I(x - xm)"</span><span class="op">=</span><span class="va">mod.3</span><span class="op">)</span></span>
<span><span class="va">cvs_clusters</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">modlist</span>, data<span class="op">=</span><span class="va">Data</span>, cluster<span class="op">=</span><span class="st">"group"</span>, k<span class="op">=</span><span class="fl">10</span>, seed<span class="op">=</span><span class="fl">6449</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cvs_clusters</span>, main<span class="op">=</span><span class="st">"Model Comparison, Cluster-Based CV"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/cross-validation-clusters-1.png" alt="10-fold cluster-based cross-validation comparing random intercept models with varying fixed effects. The error bars show the 95% confidence interval around the CV estimate of the MSE for each model." width="100%"><p class="caption">
10-fold cluster-based cross-validation comparing random intercept models
with varying fixed effects. The error bars show the 95% confidence
interval around the CV estimate of the MSE for each model.
</p>
</div>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvs_cases</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">modlist</span>, data<span class="op">=</span><span class="va">Data</span>, seed<span class="op">=</span><span class="fl">9693</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cvs_cases</span>, main<span class="op">=</span><span class="st">"Model Comparison, Case-Based CV"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/cross-validation-cases-1.png" alt="10-fold case-based cross-validation comparing random intercept models with varying fixed effects." width="100%"><p class="caption">
10-fold case-based cross-validation comparing random intercept models
with varying fixed effects.
</p>
</div>
<p>In summary, model <code>mod.1</code>, with <span class="math inline">\(x\)</span> alone and without the contextual mean
of <span class="math inline">\(x\)</span>, is assessed as fitting very
poorly by cluster-based CV, but relatively much better by case-based CV.
Model <code>mod.2</code>, which includes both <span class="math inline">\(x\)</span> and its contextual mean, produces
better results using both cluster-based and case-based CV. The
data-generating model, <code>mod.3</code>, which includes the fixed
effect of <code>x - xm</code> in place of separate terms in
<code>x</code> and <code>xm</code>, isn’t distinguishable from model
<code>mod.2</code>, which includes <code>x</code> and <code>xm</code>
separately, even though <code>mod.2</code> has an unnecessary parameter
(recall that the population coefficient of <code>xm</code> is 0 when
<code>x</code> is expressed as deviations from the contextual mean).
These conclusions are consistent with our observations based on graphing
predictions from the various models, and they illustrate the
desirability of assessing mixed-effect models at different hierarchical
levels.</p>
</div>
<div class="section level3">
<h3 id="example-crossed-random-effects">Example: Crossed random effects<a class="anchor" aria-label="anchor" href="#example-crossed-random-effects"></a>
</h3>
<p>Crossed random effects arise when the structure of the data aren’t
strictly hierarchical. Nevertheless, crossed and nested random effects
can be handled in much the same manner, by refitting the mixed-effects
model to the data with a fold of clusters or cases removed and using the
refitted model to predict the response in the removed fold.</p>
<p>We’ll illustrate with data on pig growth, introduced by <span class="citation">Diggle, Liang, &amp; Zeger (1994, Table 3.1)</span>.
The data are in the <code>Pigs</code> data frame in the
<strong>cv</strong> package:</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Pigs</span>, <span class="fl">9</span><span class="op">)</span></span>
<span><span class="co">#&gt;   id week weight</span></span>
<span><span class="co">#&gt; 1  1    1   24.0</span></span>
<span><span class="co">#&gt; 2  1    2   32.0</span></span>
<span><span class="co">#&gt; 3  1    3   39.0</span></span>
<span><span class="co">#&gt; 4  1    4   42.5</span></span>
<span><span class="co">#&gt; 5  1    5   48.0</span></span>
<span><span class="co">#&gt; 6  1    6   54.5</span></span>
<span><span class="co">#&gt; 7  1    7   61.0</span></span>
<span><span class="co">#&gt; 8  1    8   65.0</span></span>
<span><span class="co">#&gt; 9  1    9   72.0</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span><span class="op">~</span> <span class="va">id</span> <span class="op">+</span> <span class="va">week</span>, data<span class="op">=</span><span class="va">Pigs</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;    week</span></span>
<span><span class="co">#&gt; id  1 2 3 4 5 6 7 8 9</span></span>
<span><span class="co">#&gt;   1 1 1 1 1 1 1 1 1 1</span></span>
<span><span class="co">#&gt;   2 1 1 1 1 1 1 1 1 1</span></span>
<span><span class="co">#&gt;   3 1 1 1 1 1 1 1 1 1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span><span class="op">~</span> <span class="va">id</span> <span class="op">+</span> <span class="va">week</span>, data<span class="op">=</span><span class="va">Pigs</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;     week</span></span>
<span><span class="co">#&gt; id   1 2 3 4 5 6 7 8 9</span></span>
<span><span class="co">#&gt;   46 1 1 1 1 1 1 1 1 1</span></span>
<span><span class="co">#&gt;   47 1 1 1 1 1 1 1 1 1</span></span>
<span><span class="co">#&gt;   48 1 1 1 1 1 1 1 1 1</span></span></code></pre></div>
<p>Each of 48 pigs is observed weekly over a period of 9 weeks, with the
weight of the pig recorded in kg. The data are in “long” format, as is
appropriate for use with the <code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code> function in the
<strong>lme4</strong> package. The data are very regular, with no
missing cases.</p>
<p>The following graph, showing the growth trajectories of the pigs, is
similar to Figure 3.1 in <span class="citation">Diggle et al.
(1994)</span>; we add an overall least-squares line and a loess smooth,
which are nearly indistinguishable:</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">weight</span> <span class="op">~</span> <span class="va">week</span>, data<span class="op">=</span><span class="va">Pigs</span>, type<span class="op">=</span><span class="st">"n"</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/unique.html" class="external-link">unique</a></span><span class="op">(</span><span class="va">Pigs</span><span class="op">$</span><span class="va">id</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Pigs</span>, <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span>x<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">9</span>, y<span class="op">=</span><span class="va">Pigs</span><span class="op">[</span><span class="va">id</span> <span class="op">==</span> <span class="va">i</span>, <span class="st">"weight"</span><span class="op">]</span>,</span>
<span>                   col<span class="op">=</span><span class="st">"gray"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">weight</span> <span class="op">~</span> <span class="va">week</span>, data<span class="op">=</span><span class="va">Pigs</span><span class="op">)</span>, col<span class="op">=</span><span class="st">"blue"</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Pigs</span>, <span class="fu"><a href="https://rdrr.io/r/stats/scatter.smooth.html" class="external-link">loess.smooth</a></span><span class="op">(</span><span class="va">week</span>, <span class="va">weight</span>, span<span class="op">=</span><span class="fl">0.5</span><span class="op">)</span><span class="op">)</span>,</span>
<span>      col<span class="op">=</span><span class="st">"magenta"</span>, lty<span class="op">=</span><span class="fl">2</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/pigs-graph-1.png" alt="Growth trajectories for 48 pigs, with overall least-squares line (sold blue) and loess line (broken magenta)." width="60%"><p class="caption">
Growth trajectories for 48 pigs, with overall least-squares line (sold
blue) and loess line (broken magenta).
</p>
</div>
<p>The individual “growth curves” and the overall trend are generally
linear, with some tendency for variability of pig weight to increase
over weeks (a feature of the data that we ignore in the mixed model that
we fit to the data below).</p>
<p>The <strong>Stata</strong> mixed-effects models manual proposes a
model with crossed random effects for the <code>Pigs</code> data <span class="citation">(StataCorp LLC, 2023, p. 37)</span>:</p>
<blockquote>
<p>[S]uppose that we wish to fit <span class="math display">\[
\mathrm{weight}_{ij} = \beta_0 + \beta_1 \mathrm{week}_{ij} + u_i + v_j
+ \varepsilon_{ij}
\]</span> for the <span class="math inline">\(i = 1, \ldots, 9\)</span>
weeks and <span class="math inline">\(j = 1, \dots, 48\)</span> pigs and
<span class="math display">\[
u_i \sim N(0, \sigma^2_u); v_j \sim N(0, \sigma^2_v ); \varepsilon_{ij}
\sim N(0, \sigma^2_\varepsilon)
\]</span> all independently. That is, we assume an overall
population-average growth curve <span class="math inline">\(\beta_0 +
\beta_1 \mathrm{week}\)</span> and a random pig-specific shift. In other
words, the effect due to week, <span class="math inline">\(u_i\)</span>,
is systematic to that week and common to all pigs. The rationale behind
[this model] could be that, assuming that the pigs were measured
contemporaneously, we might be concerned that week-specific random
factors such as weather and feeding patterns had significant systematic
effects on all pigs.</p>
</blockquote>
<p>Although we might prefer an alternative model,<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;These are repeated-measures data, which would be more
conventionally modeled with autocorrelated errors within pigs. The
&lt;code&gt;&lt;a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link"&gt;lme()&lt;/a&gt;&lt;/code&gt; function in the &lt;strong&gt;nlme&lt;/strong&gt; package, for
example, is capable of fitting a mixed-model of this form.&lt;/p&gt;'><sup>10</sup></a> we think that this
is a reasonable specification.</p>
<p>The <strong>Stata</strong> manual fits the mixed model by maximum
likelihood (rather than REML), and we duplicate the results reported
there using <code><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer()</a></code>:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmer.html" class="external-link">lmer</a></span><span class="op">(</span><span class="va">weight</span> <span class="op">~</span> <span class="va">week</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">id</span><span class="op">)</span> <span class="op">+</span> <span class="op">(</span><span class="fl">1</span> <span class="op">|</span> <span class="va">week</span><span class="op">)</span>,</span>
<span>            data<span class="op">=</span><span class="va">Pigs</span>, REML<span class="op">=</span><span class="cn">FALSE</span>, <span class="co"># i.e., ML</span></span>
<span>            control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/lme4/man/lmerControl.html" class="external-link">lmerControl</a></span><span class="op">(</span>optimizer<span class="op">=</span><span class="st">"bobyqa"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.p</span><span class="op">)</span></span>
<span><span class="co">#&gt; Linear mixed model fit by maximum likelihood  ['lmerMod']</span></span>
<span><span class="co">#&gt; Formula: weight ~ week + (1 | id) + (1 | week)</span></span>
<span><span class="co">#&gt;    Data: Pigs</span></span>
<span><span class="co">#&gt; Control: lmerControl(optimizer = "bobyqa")</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;      AIC      BIC   logLik deviance df.resid </span></span>
<span><span class="co">#&gt;   2037.6   2058.0  -1013.8   2027.6      427 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Scaled residuals: </span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -3.775 -0.542  0.005  0.476  3.982 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Random effects:</span></span>
<span><span class="co">#&gt;  Groups   Name        Variance Std.Dev.</span></span>
<span><span class="co">#&gt;  id       (Intercept) 14.836   3.852   </span></span>
<span><span class="co">#&gt;  week     (Intercept)  0.085   0.292   </span></span>
<span><span class="co">#&gt;  Residual              4.297   2.073   </span></span>
<span><span class="co">#&gt; Number of obs: 432, groups:  id, 48; week, 9</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Fixed effects:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value</span></span>
<span><span class="co">#&gt; (Intercept)  19.3556     0.6334    30.6</span></span>
<span><span class="co">#&gt; week          6.2099     0.0539   115.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation of Fixed Effects:</span></span>
<span><span class="co">#&gt;      (Intr)</span></span>
<span><span class="co">#&gt; week -0.426</span></span></code></pre></div>
<p>We opt for the non-default <code>"bobyqa"</code> optimizer because it
provides more numerically stable results for subsequent cross-validation
in this example.</p>
<p>We can then cross-validate the model by omitting folds composed of
pigs, folds composed of weeks, or folds composed of pig-weeks (which in
the <code>Pigs</code> data set correspond to individual cases, using
only the fixed effects):</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.p</span>, clusterVariables<span class="op">=</span><span class="st">"id"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation based on 48 {id} clusters</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.973</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.965</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (17.125, 22.805)</span></span>
<span><span class="co">#&gt; full-sample criterion = 19.201</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.p</span>, clusterVariables<span class="op">=</span><span class="st">"week"</span><span class="op">)</span></span>
<span><span class="co">#&gt; boundary (singular) fit: see help('isSingular')</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation based on 9 {week} clusters</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.312</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.305</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (16.566, 22.044)</span></span>
<span><span class="co">#&gt; full-sample criterion = 19.201</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.p</span>, clusterVariables<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"id"</span>, <span class="st">"week"</span><span class="op">)</span>, k<span class="op">=</span><span class="fl">10</span>, seed<span class="op">=</span><span class="fl">8469</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 8469</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation based on 432 {id, week} clusters</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.235</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.233</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (16.493, 21.973)</span></span>
<span><span class="co">#&gt; full-sample criterion = 19.201</span></span></code></pre></div>
<p>We can also cross-validate the individual cases taking account of the
random effects (employing the same 10 folds):</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.p</span>, k<span class="op">=</span><span class="fl">10</span>, seed<span class="op">=</span><span class="fl">8469</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 8469</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 5.1583</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 5.0729</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (4.123, 6.0229)</span></span>
<span><span class="co">#&gt; full-sample criterion = 3.796</span></span></code></pre></div>
<p>Because these predictions are based on BLUPs, they are more accurate
than the predictions based only on fixed effects.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Even though there is only one observation per
combination of pigs and weeks, we can use the BLUP for the omitted case
because of the crossed structure of the random effects; that is each
pig-week has a pig random effect and a week random effect. Although it
probably isn’t sensible, we can imagine a mixed model for the pig data
that employs nested random effects, which would be specified by
&lt;code&gt;lmer(weight ~ week + (1 | id/week), data=Pigs)&lt;/code&gt;—that is, a
random intercept that varies by combinations of &lt;code&gt;id&lt;/code&gt; (pig)
and &lt;code&gt;week&lt;/code&gt;. This model can’t be fit, however: With only one
case per combination of &lt;code&gt;id&lt;/code&gt; and &lt;code&gt;week&lt;/code&gt;, the
nested random-effect variance is indistinguishable from the case-level
variance.&lt;/p&gt;"><sup>11</sup></a> As well, the
difference between the MSE computed for the model fit to the full data
and the CV estimates of the MSE is greater here than for cluster-based
predictions.</p>
</div>
</div>
<div class="section level2">
<h2 id="replicating-cross-validation">Replicating cross-validation<a class="anchor" aria-label="anchor" href="#replicating-cross-validation"></a>
</h2>
<p>Assuming that the number of cases <span class="math inline">\(n\)</span> is a multiple of the number of folds
<span class="math inline">\(k\)</span>—a slightly simplifying
assumption—the number of possible partitions of cases into folds is
<span class="math inline">\(\frac{n!}{[(n/k)!]^k}\)</span>, a number
that grows very large very quickly. For example, for <span class="math inline">\(n = 10\)</span> and <span class="math inline">\(k
= 5\)</span>, so that the folds are each of size <span class="math inline">\(n/k = 2\)</span>, there are <span class="math inline">\(113,400\)</span> possible partitions; for <span class="math inline">\(n=100\)</span> and <span class="math inline">\(k=5\)</span>, where <span class="math inline">\(n/k = 20\)</span>, still a small problem, the
number of possible partitions is truly astronomical, <span class="math inline">\(1.09\times 10^{66}\)</span>.</p>
<p>Because the partition into folds that’s employed is selected
randomly, the resulting CV criterion estimates are subject to sampling
error. (An exception is LOO cross-validation, which is not at all
random.) To get a sense of the magnitude of the sampling error, we can
repeat the CV procedure with different randomly selected partitions into
folds. All of the CV functions in the <strong>cv</strong> package are
capable of repeated cross-validation, with the number of repetitions
controlled by the <code>reps</code> argument, which defaults to
<code>1</code>.</p>
<p>Here, for example, is 10-fold CV for the Mroz logistic regression,
repeated 5 times:</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, seed<span class="op">=</span><span class="fl">248</span>, reps<span class="op">=</span><span class="fl">5</span>, </span>
<span>   method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 248</span></span>
<span><span class="co">#&gt; R RNG seed set to 68134</span></span>
<span><span class="co">#&gt; R RNG seed set to 767359</span></span>
<span><span class="co">#&gt; R RNG seed set to 556270</span></span>
<span><span class="co">#&gt; R RNG seed set to 882966</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 1:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31301</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.27967, 0.34635)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 2:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.31607</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3117</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.27847, 0.34493)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 3:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.31474</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.30862</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.27543, 0.34181)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 4:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31807</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28462, 0.35152)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 5:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31926</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28581, 0.35271)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Average:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.31983 (0.003887)</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31394 (0.0040093)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<p>When <code>reps</code> &gt; <code>1</code>, the result returned by
<code><a href="../reference/cv.html">cv()</a></code> is an object of class <code>"cvList"</code>—literally
a list of <code>"cv"</code> objects. The results are reported for each
repetition and then averaged across repetitions, with the standard
deviations of the CV criterion and the biased-adjusted CV criterion
given in parentheses. In this example, there is therefore little
variation across repetitions, increasing our confidence in the
reliability of the results.</p>
<p>Notice that the seed that’s set in the <code><a href="../reference/cv.html">cv()</a></code> command
pertains to the first repetition and the seeds for the remaining
repetitions are then selected pseudo-randomly.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Because of the manner in which the computation is
performed, the order of the replicates in the &lt;code&gt;"cvList"&lt;/code&gt;
object returned by &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; isn’t the same as the order in
which the replicates are computed. Each element of the result, however,
is a &lt;code&gt;"cv"&lt;/code&gt; object with the correct random-number seed saved,
and so this technical detail can be safely ignored. The individual
&lt;code&gt;"cv"&lt;/code&gt; objects are printed in the order in which they are
stored rather than the order in which they are computed.&lt;/p&gt;'><sup>12</sup></a> Setting the first
seed, however, makes the entire process easily replicable, and the seed
for each repetition is stored in the corresponding element of the
<code>"cvList"</code> object (which isn’t, however, saved in the
example).</p>
<p>It’s also possible to replicate CV when comparing competing models
via the <code><a href="../reference/cv.html">cv()</a></code> method for <code>"modList"</code> objects.
Recall our comparison of polynomial regressions of varying degree fit to
the <code>Auto</code> data; we performed 10-fold CV for each of 10
models. Here, we replicate that process 5 times for each model and graph
the results:</p>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.auto.reps</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="fu"><a href="../reference/models.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>                        <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>                 data<span class="op">=</span><span class="va">Auto</span>, seed<span class="op">=</span><span class="fl">8004</span>, reps<span class="op">=</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.auto.reps</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/model-comparison-with-reps-1.png" alt=" Replicated cross-validated 10-fold CV as a function of polynomial degree, $p$" width="100%"><p class="caption">
Replicated cross-validated 10-fold CV as a function of polynomial
degree, <span class="math inline">\(p\)</span>
</p>
</div>
<p>The graph shows both the average CV criterion and its range for each
of the competing models.</p>
</div>
<div class="section level2">
<h2 id="cross-validating-model-selection">Cross-validating model selection<a class="anchor" aria-label="anchor" href="#cross-validating-model-selection"></a>
</h2>
<div class="section level3">
<h3 id="a-preliminary-example">A preliminary example<a class="anchor" aria-label="anchor" href="#a-preliminary-example"></a>
</h3>
<p>As <span class="citation">Hastie, Tibshirani, &amp; Friedman (2009,
sec. 7.10.2: “The Wrong and Right Way to Do Cross-validation”)</span>
explain, if the whole data are used to select or fine-tune a statistical
model, subsequent cross-validation of the model is intrinsically
misleading, because the model is selected to fit the whole data,
including the part of the data that remains when each fold is
removed.</p>
<p>The following example is similar in spirit to one employed by <span class="citation">Hastie et al. (2009)</span>. Suppose that we randomly
generate <span class="math inline">\(n = 1000\)</span> independent
observations for a response variable variable <span class="math inline">\(y \sim N(\mu = 10, \sigma^2 = 0)\)</span>, and
independently sample <span class="math inline">\(1000\)</span>
observations for <span class="math inline">\(p = 100\)</span>
“predictors,” <span class="math inline">\(x_1, \ldots, x_{100}\)</span>,
each from <span class="math inline">\(x_j \sim N(0, 1)\)</span>. The
response has nothing to do with the predictors and so the population
linear-regression model <span class="math inline">\(y_i = \alpha +
\beta_1 x_{i1} + \cdots + \beta_{100} x_{i,100} + \varepsilon_i\)</span>
has <span class="math inline">\(\alpha = 10\)</span> and all <span class="math inline">\(\beta_j = 0\)</span>.</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">24361</span><span class="op">)</span> <span class="co"># for reproducibility</span></span>
<span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean<span class="op">=</span><span class="fl">10</span><span class="op">)</span>,</span>
<span>  X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">*</span><span class="fl">100</span><span class="op">)</span>, <span class="fl">1000</span>, <span class="fl">100</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">D</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">6</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;         y      X.1      X.2      X.3       X.4       X.5</span></span>
<span><span class="co">#&gt; 1 10.0316 -1.23886 -0.26487 -0.03539 -2.576973  0.811048</span></span>
<span><span class="co">#&gt; 2  9.6650  0.12287 -0.17744  0.37290 -0.935138  0.628673</span></span>
<span><span class="co">#&gt; 3 10.0232 -0.95052 -0.73487 -1.05978  0.882944  0.023918</span></span>
<span><span class="co">#&gt; 4  8.9910  1.13571  0.32411  0.11037  1.376303 -0.422114</span></span>
<span><span class="co">#&gt; 5  9.0712  1.49474  1.87538  0.10575  0.292140 -0.184568</span></span>
<span><span class="co">#&gt; 6 11.3493 -0.18453 -0.78037 -1.23804 -0.010949  0.691034</span></span></code></pre></div>
<p>Least-squares provides accurate estimates of the regression constant
<span class="math inline">\(\alpha = 10\)</span> and the error variance
<span class="math inline">\(\sigma^2 = 1\)</span> for the “null model”
including only the regression constant; moreover, the omnibus <span class="math inline">\(F\)</span>-test of the correct null hypothesis
that all of the <span class="math inline">\(\beta\)</span>s are 0 for
the “full model” with all 100 <span class="math inline">\(x\)</span>s is
associated with a large <span class="math inline">\(p\)</span>-value:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="va">D</span><span class="op">)</span></span>
<span><span class="va">m.null</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="fl">1</span>, data<span class="op">=</span><span class="va">D</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova</a></span><span class="op">(</span><span class="va">m.null</span>, <span class="va">m.full</span><span class="op">)</span></span>
<span><span class="co">#&gt; Analysis of Variance Table</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model 1: y ~ 1</span></span>
<span><span class="co">#&gt; Model 2: y ~ X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 + </span></span>
<span><span class="co">#&gt;     X.11 + X.12 + X.13 + X.14 + X.15 + X.16 + X.17 + X.18 + X.19 + </span></span>
<span><span class="co">#&gt;     X.20 + X.21 + X.22 + X.23 + X.24 + X.25 + X.26 + X.27 + X.28 + </span></span>
<span><span class="co">#&gt;     X.29 + X.30 + X.31 + X.32 + X.33 + X.34 + X.35 + X.36 + X.37 + </span></span>
<span><span class="co">#&gt;     X.38 + X.39 + X.40 + X.41 + X.42 + X.43 + X.44 + X.45 + X.46 + </span></span>
<span><span class="co">#&gt;     X.47 + X.48 + X.49 + X.50 + X.51 + X.52 + X.53 + X.54 + X.55 + </span></span>
<span><span class="co">#&gt;     X.56 + X.57 + X.58 + X.59 + X.60 + X.61 + X.62 + X.63 + X.64 + </span></span>
<span><span class="co">#&gt;     X.65 + X.66 + X.67 + X.68 + X.69 + X.70 + X.71 + X.72 + X.73 + </span></span>
<span><span class="co">#&gt;     X.74 + X.75 + X.76 + X.77 + X.78 + X.79 + X.80 + X.81 + X.82 + </span></span>
<span><span class="co">#&gt;     X.83 + X.84 + X.85 + X.86 + X.87 + X.88 + X.89 + X.90 + X.91 + </span></span>
<span><span class="co">#&gt;     X.92 + X.93 + X.94 + X.95 + X.96 + X.97 + X.98 + X.99 + X.100</span></span>
<span><span class="co">#&gt;   Res.Df RSS  Df Sum of Sq    F Pr(&gt;F)</span></span>
<span><span class="co">#&gt; 1    999 974                          </span></span>
<span><span class="co">#&gt; 2    899 888 100      85.2 0.86   0.82</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.null</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ 1, data = D)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -3.458 -0.681  0.019  0.636  2.935 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   9.9370     0.0312     318   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.987 on 999 degrees of freedom</span></span></code></pre></div>
<p>Next, using the <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> function in the
<strong>MASS</strong> package <span class="citation">(Venables &amp;
Ripley, 2002)</span>, let us perform a forward stepwise regression to
select a “best” model, starting with the null model, and using AIC as
the model-selection criterion (see the help page for
<code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> for details):<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;It’s generally advantageous to start with the largest
model, here the one with 100 predictors, and proceed by backward
elimination. In this demonstration, however, where all of the &lt;span class="math inline"&gt;\(\beta\)&lt;/span&gt;s are really 0, the selected model
will be small, and so we proceed by forward selection from the null
model to save computing time.&lt;/p&gt;'><sup>13</sup></a></p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="http://www.stats.ox.ac.uk/pub/MASS4/" class="external-link">"MASS"</a></span><span class="op">)</span>  <span class="co"># for stepAIC()</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'MASS'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:dplyr':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     select</span></span>
<span><span class="va">m.select</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC</a></span><span class="op">(</span><span class="va">m.null</span>,</span>
<span>                    direction<span class="op">=</span><span class="st">"forward"</span>, trace<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>                    scope<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lower<span class="op">=</span><span class="op">~</span><span class="fl">1</span>, upper<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">formula</a></span><span class="op">(</span><span class="va">m.full</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.select</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = y ~ X.99 + X.90 + X.87 + X.40 + X.65 + X.91 + X.53 + </span></span>
<span><span class="co">#&gt;     X.45 + X.31 + X.56 + X.61 + X.60 + X.46 + X.35 + X.92, data = D)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -3.262 -0.645  0.024  0.641  3.118 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   9.9372     0.0310  320.80   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; X.99         -0.0910     0.0308   -2.95   0.0032 ** </span></span>
<span><span class="co">#&gt; X.90         -0.0820     0.0314   -2.62   0.0090 ** </span></span>
<span><span class="co">#&gt; X.87         -0.0694     0.0311   -2.24   0.0256 *  </span></span>
<span><span class="co">#&gt; X.40         -0.0476     0.0308   -1.55   0.1221    </span></span>
<span><span class="co">#&gt; X.65         -0.0552     0.0315   -1.76   0.0795 .  </span></span>
<span><span class="co">#&gt; X.91          0.0524     0.0308    1.70   0.0894 .  </span></span>
<span><span class="co">#&gt; X.53         -0.0492     0.0305   -1.61   0.1067    </span></span>
<span><span class="co">#&gt; X.45          0.0554     0.0318    1.74   0.0818 .  </span></span>
<span><span class="co">#&gt; X.31          0.0452     0.0311    1.46   0.1457    </span></span>
<span><span class="co">#&gt; X.56          0.0543     0.0327    1.66   0.0972 .  </span></span>
<span><span class="co">#&gt; X.61         -0.0508     0.0317   -1.60   0.1091    </span></span>
<span><span class="co">#&gt; X.60         -0.0513     0.0319   -1.61   0.1083    </span></span>
<span><span class="co">#&gt; X.46          0.0516     0.0327    1.58   0.1153    </span></span>
<span><span class="co">#&gt; X.35          0.0470     0.0315    1.49   0.1358    </span></span>
<span><span class="co">#&gt; X.92          0.0443     0.0310    1.43   0.1533    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.973 on 984 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.0442, Adjusted R-squared:  0.0296 </span></span>
<span><span class="co">#&gt; F-statistic: 3.03 on 15 and 984 DF,  p-value: 8.34e-05</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">D</span><span class="op">$</span><span class="va">y</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.select</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.93063</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>The resulting model has 15 predictors, a very modest <span class="math inline">\(R^2 = .044\)</span>, but a small <span class="math inline">\(p\)</span>-value for its omnibus <span class="math inline">\(F\)</span>-test (which, of course, is entirely
spurious because the same data were used to select and test the model).
The MSE for the selected model is smaller than the true error variance
<span class="math inline">\(\sigma^2 = 1\)</span>, as is the estimated
error variance for the selected model, <span class="math inline">\(\widehat{\sigma}^2 = 0.973^2 = 0.947\)</span>.</p>
<p>If we cross-validate the selected model, we also obtain an optimistic
estimate of its predictive power (although the confidence interval for
the bias-adjusted MSE includes 1):</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.select</span>, seed<span class="op">=</span><span class="fl">2529</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 2529</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.95937</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.95785</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.87661, 1.0391)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.93063</span></span></code></pre></div>
<p>The <code><a href="../reference/cvSelect.html">cvSelect()</a></code> function in the <strong>cv</strong>
package allows us to cross-validate the whole model-selection procedure.
The first argument to <code><a href="../reference/cvSelect.html">cvSelect()</a></code> is a model-selection
function capable of refitting the model with a fold omitted and
returning a CV criterion. The <code><a href="../reference/cvSelect.html">selectStepAIC()</a></code> function,
also in <strong>cv</strong> and based on <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code>, is
suitable for use with <code><a href="../reference/cvSelect.html">cvSelect()</a></code>:</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.select</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cvSelect.html">cvSelect</a></span><span class="op">(</span><span class="va">selectStepAIC</span>, data<span class="op">=</span><span class="va">D</span>, seed<span class="op">=</span><span class="fl">3791</span>,</span>
<span>                      model<span class="op">=</span><span class="va">m.null</span>, direction<span class="op">=</span><span class="st">"forward"</span>,</span>
<span>                      scope<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lower<span class="op">=</span><span class="op">~</span><span class="fl">1</span>, </span>
<span>                                 upper<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/formula.html" class="external-link">formula</a></span><span class="op">(</span><span class="va">m.full</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 3791</span></span>
<span><span class="va">cv.select</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 1.0687</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 1.0612</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.97172, 1.1506)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.93063</span></span></code></pre></div>
<p>The other arguments to <code><a href="../reference/cvSelect.html">cvSelect()</a></code> are:</p>
<ul>
<li>
<code>data</code>, the data set to which the model is fit;</li>
<li>
<code>seed</code>, an optional seed for R’s pseudo-random-number
generator; as for <code><a href="../reference/cv.html">cv()</a></code>, if the seed isn’t supplied by the
user, a seed is randomly selected and saved;</li>
<li>additional arguments required by the model-selection function, here
the starting <code>model</code> argument, the <code>direction</code> of
model selection, and the <code>scope</code> of models considered (from
the model with only a regression constant to the model with all 100
predictors).</li>
</ul>
<p>By default, <code><a href="../reference/cvSelect.html">cvSelect()</a></code> performs 10-fold CV, and produces
an estimate of MSE for the model-selection procedure even
<em>larger</em> than the true error variance, <span class="math inline">\(\sigma^2 = 1\)</span>.</p>
<p>Also by default, when the number of folds is 10 or fewer,
<code><a href="../reference/cvSelect.html">cvSelect()</a></code> saves the coefficients of the selected models.
In this example, the <code><a href="../reference/cvSelect.html">compareFolds()</a></code> function reveals that
the variables retained by the model-selection process in the several
folds are quite different:</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cvSelect.html">compareFolds</a></span><span class="op">(</span><span class="va">cv.select</span><span class="op">)</span></span>
<span><span class="co">#&gt;         (Intercept)    X.87    X.90    X.99    X.91    X.54    X.53    X.56</span></span>
<span><span class="co">#&gt; Fold 1       9.9187 -0.0615 -0.0994 -0.0942  0.0512  0.0516                </span></span>
<span><span class="co">#&gt; Fold 2       9.9451 -0.0745 -0.0899 -0.0614          0.0587          0.0673</span></span>
<span><span class="co">#&gt; Fold 3       9.9423 -0.0783 -0.0718 -0.0987  0.0601                  0.0512</span></span>
<span><span class="co">#&gt; Fold 4       9.9410 -0.0860 -0.0831 -0.0867  0.0570         -0.0508        </span></span>
<span><span class="co">#&gt; Fold 5       9.9421 -0.0659 -0.0849 -0.1004  0.0701  0.0511 -0.0487  0.0537</span></span>
<span><span class="co">#&gt; Fold 6       9.9633 -0.0733 -0.0874 -0.0960  0.0555  0.0629 -0.0478        </span></span>
<span><span class="co">#&gt; Fold 7       9.9279 -0.0618 -0.0960 -0.0838  0.0533         -0.0464        </span></span>
<span><span class="co">#&gt; Fold 8       9.9453 -0.0610 -0.0811 -0.0818          0.0497 -0.0612  0.0560</span></span>
<span><span class="co">#&gt; Fold 9       9.9173 -0.0663 -0.0894 -0.1100  0.0504  0.0524          0.0747</span></span>
<span><span class="co">#&gt; Fold 10      9.9449 -0.0745 -0.0906 -0.0891  0.0535  0.0482 -0.0583  0.0642</span></span>
<span><span class="co">#&gt;            X.40    X.45    X.65    X.68    X.92    X.15    X.26    X.46    X.60</span></span>
<span><span class="co">#&gt; Fold 1                  -0.0590                 -0.0456  0.0658  0.0608        </span></span>
<span><span class="co">#&gt; Fold 2                                   0.0607          0.0487                </span></span>
<span><span class="co">#&gt; Fold 3  -0.0496         -0.0664          0.0494                                </span></span>
<span><span class="co">#&gt; Fold 4  -0.0597  0.0579 -0.0531          0.0519 -0.0566                 -0.0519</span></span>
<span><span class="co">#&gt; Fold 5                           0.0587                          0.0527 -0.0603</span></span>
<span><span class="co">#&gt; Fold 6  -0.0596  0.0552          0.0474                                        </span></span>
<span><span class="co">#&gt; Fold 7           0.0572          0.0595                                        </span></span>
<span><span class="co">#&gt; Fold 8           0.0547 -0.0617  0.0453  0.0493 -0.0613  0.0591  0.0703 -0.0588</span></span>
<span><span class="co">#&gt; Fold 9  -0.0552  0.0573 -0.0635  0.0492         -0.0513  0.0484         -0.0507</span></span>
<span><span class="co">#&gt; Fold 10 -0.0558                          0.0529                  0.0710        </span></span>
<span><span class="co">#&gt;            X.61     X.8    X.28    X.29    X.31    X.35    X.70    X.89    X.17</span></span>
<span><span class="co">#&gt; Fold 1  -0.0490          0.0616 -0.0537                  0.0638                </span></span>
<span><span class="co">#&gt; Fold 2           0.0671                  0.0568                  0.0523        </span></span>
<span><span class="co">#&gt; Fold 3  -0.0631          0.0616                                                </span></span>
<span><span class="co">#&gt; Fold 4           0.0659         -0.0549          0.0527                  0.0527</span></span>
<span><span class="co">#&gt; Fold 5           0.0425                  0.0672  0.0613          0.0493        </span></span>
<span><span class="co">#&gt; Fold 6           0.0559         -0.0629  0.0498          0.0487                </span></span>
<span><span class="co">#&gt; Fold 7                                                           0.0611  0.0472</span></span>
<span><span class="co">#&gt; Fold 8  -0.0719                                          0.0586                </span></span>
<span><span class="co">#&gt; Fold 9                   0.0525                                                </span></span>
<span><span class="co">#&gt; Fold 10 -0.0580                                  0.0603                        </span></span>
<span><span class="co">#&gt;            X.25     X.4    X.64    X.81    X.97    X.11     X.2    X.33    X.47</span></span>
<span><span class="co">#&gt; Fold 1                                   0.0604          0.0575                </span></span>
<span><span class="co">#&gt; Fold 2   0.0478          0.0532  0.0518                                        </span></span>
<span><span class="co">#&gt; Fold 3                           0.0574                          0.0473        </span></span>
<span><span class="co">#&gt; Fold 4                   0.0628                                                </span></span>
<span><span class="co">#&gt; Fold 5   0.0518                                                                </span></span>
<span><span class="co">#&gt; Fold 6                                           0.0521                        </span></span>
<span><span class="co">#&gt; Fold 7           0.0550                                                        </span></span>
<span><span class="co">#&gt; Fold 8                                                                         </span></span>
<span><span class="co">#&gt; Fold 9                                   0.0556                          0.0447</span></span>
<span><span class="co">#&gt; Fold 10          0.0516                                                        </span></span>
<span><span class="co">#&gt;             X.6    X.72    X.73    X.77    X.79 X.88</span></span>
<span><span class="co">#&gt; Fold 1   0.0476                                     </span></span>
<span><span class="co">#&gt; Fold 2                   0.0514                     </span></span>
<span><span class="co">#&gt; Fold 3                                              </span></span>
<span><span class="co">#&gt; Fold 4                                  -0.0473     </span></span>
<span><span class="co">#&gt; Fold 5           0.0586                         0.07</span></span>
<span><span class="co">#&gt; Fold 6                          -0.0489             </span></span>
<span><span class="co">#&gt; Fold 7                                              </span></span>
<span><span class="co">#&gt; Fold 8                                              </span></span>
<span><span class="co">#&gt; Fold 9                                              </span></span>
<span><span class="co">#&gt; Fold 10</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="mrozs-logistic-regression-revisited">Mroz’s logistic regression revisited<a class="anchor" aria-label="anchor" href="#mrozs-logistic-regression-revisited"></a>
</h3>
<p>For a contrasting example we apply model selection to Mroz’s logistic
regression for married women’s labor-force participation. First, recall
the logistic regression model that we fit to the <code>Mroz</code>
data:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 921.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
<p>Applying stepwise model selection Mroz’s logistic regression, using
BIC as the model-selection criterion (via the argument
<code>k=log(nrow(Mroz))</code> to <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code>) selects 5 of
the 7 original predictors:</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz.sel</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                      trace<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz.sel</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ k5 + age + wc + lwg + inc, family = binomial, </span></span>
<span><span class="co">#&gt;     data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)   2.9019     0.5429    5.35  9.0e-08 ***</span></span>
<span><span class="co">#&gt; k5           -1.4318     0.1932   -7.41  1.3e-13 ***</span></span>
<span><span class="co">#&gt; age          -0.0585     0.0114   -5.13  2.9e-07 ***</span></span>
<span><span class="co">#&gt; wcyes         0.8724     0.2064    4.23  2.4e-05 ***</span></span>
<span><span class="co">#&gt; lwg           0.6157     0.1501    4.10  4.1e-05 ***</span></span>
<span><span class="co">#&gt; inc          -0.0337     0.0078   -4.32  1.6e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  906.46  on 747  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 918.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 3</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">BayesRule</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">$</span><span class="va">lfp</span> <span class="op">==</span> <span class="st">"yes"</span>,</span>
<span>          <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">m.mroz.sel</span>, type<span class="op">=</span><span class="st">"response"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.31873</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "y != round(yhat)"</span></span></code></pre></div>
<p>Bayes rule applied to the selected model misclassifies 32% of the
cases in the <code>Mroz</code> data.</p>
<p>Cross-validating the selected model produces a similar, slightly
larger, estimate of misclassification, about 33%:</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz.sel</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, seed<span class="op">=</span><span class="fl">345266</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 345266</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.33068</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.33332</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.2997, 0.36695)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.31873</span></span></code></pre></div>
<p>Is this estimate of predictive performance optimistic?</p>
<p>We proceed to apply the model-selection procedure by
cross-validation, producing more or less the same result:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz.sel.cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cvSelect.html">cvSelect</a></span><span class="op">(</span><span class="va">selectStepAIC</span>, <span class="va">Mroz</span>, </span>
<span>                          seed<span class="op">=</span><span class="fl">6681</span>,</span>
<span>                          criterion<span class="op">=</span><span class="va">BayesRule</span>,</span>
<span>                          model<span class="op">=</span><span class="va">m.mroz</span>,</span>
<span>                          AIC<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 6681</span></span>
<span><span class="va">m.mroz.sel.cv</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.33068</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.33452</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.3009, 0.36815)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.31873</span></span></code></pre></div>
<p>Setting <code>AIC=FALSE</code> in the call to <code><a href="../reference/cvSelect.html">cvSelect()</a></code>
uses the BIC rather than the AIC as the model-selection criterion. As it
turns out, exactly the same predictors are selected when each of the 10
folds are omitted, and the several coefficient estimates are very
similar, as we show using <code><a href="../reference/cvSelect.html">compareFolds()</a></code>:</p>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cvSelect.html">compareFolds</a></span><span class="op">(</span><span class="va">m.mroz.sel.cv</span><span class="op">)</span></span>
<span><span class="co">#&gt;         (Intercept)     age     inc      k5     lwg wcyes</span></span>
<span><span class="co">#&gt; Fold 1       2.5014 -0.0454 -0.0388 -1.3613  0.5653  0.85</span></span>
<span><span class="co">#&gt; Fold 2       3.0789 -0.0659 -0.0306 -1.5335  0.6923  0.79</span></span>
<span><span class="co">#&gt; Fold 3       3.0141 -0.0595 -0.0305 -1.3994  0.5428  0.86</span></span>
<span><span class="co">#&gt; Fold 4       2.7251 -0.0543 -0.0354 -1.4474  0.6298  1.09</span></span>
<span><span class="co">#&gt; Fold 5       2.7617 -0.0566 -0.0320 -1.4752  0.6324  0.74</span></span>
<span><span class="co">#&gt; Fold 6       3.0234 -0.0621 -0.0348 -1.4537  0.6618  0.94</span></span>
<span><span class="co">#&gt; Fold 7       2.9615 -0.0600 -0.0351 -1.4127  0.5835  0.97</span></span>
<span><span class="co">#&gt; Fold 8       2.9598 -0.0603 -0.0329 -1.3865  0.6210  0.69</span></span>
<span><span class="co">#&gt; Fold 9       3.2481 -0.0650 -0.0381 -1.4138  0.6093  0.94</span></span>
<span><span class="co">#&gt; Fold 10      2.7724 -0.0569 -0.0295 -1.4503  0.6347  0.85</span></span></code></pre></div>
<p>In this example, therefore, we appear to obtain a realistic estimate
of model performance directly from the selected model, because there is
little added uncertainty induced by model selection.</p>
</div>
<div class="section level3">
<h3 id="cross-validating-choice-of-transformations-in-regression">Cross-validating choice of transformations in regression<a class="anchor" aria-label="anchor" href="#cross-validating-choice-of-transformations-in-regression"></a>
</h3>
<p>The <strong>cv</strong> package also provides a
<code><a href="../reference/cvSelect.html">cvSelect()</a></code> procedure, <code><a href="../reference/cvSelect.html">selectTrans()</a></code>, for
choosing transformations of the predictors and the response in
regression.</p>
<p>Some background: As <span class="citation">Weisberg (2014, sec.
8.2)</span> explains, there are technical advantages to having (numeric)
predictors in linear regression analysis that are themselves linearly
related. If the predictors <em>aren’t</em> linearly related, then the
relationships between them can often be straightened by power
transformations. Transformations can be selected after graphical
examination of the data, or by analytic methods. Once the relationships
between the predictors are linearized, it can be advantageous similarly
to transform the response variable towards normality.</p>
<p>Selecting transformations analytically raises the possibility of
automating the process, as would be required for cross-validation. One
could, in principle, apply graphical methods to select transformations
for each fold, but because a data analyst couldn’t forget the choices
made for previous folds, the process wouldn’t really be applied
independently to the folds.</p>
<p>To illustrate, we adapt an example appearing in several places in
<span class="citation">Fox &amp; Weisberg (2019)</span> (for example in
Chapter 3 on transforming data), using data on the prestige and other
characteristics of 102 Canadian occupations circa 1970. The data are in
the <code>Prestige</code> data frame in the <strong>carData</strong>
package:</p>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Prestige"</span>, package<span class="op">=</span><span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">)</span></span>
<span><span class="co">#&gt;                     education income women prestige census type</span></span>
<span><span class="co">#&gt; gov.administrators      13.11  12351 11.16     68.8   1113 prof</span></span>
<span><span class="co">#&gt; general.managers        12.26  25879  4.02     69.1   1130 prof</span></span>
<span><span class="co">#&gt; accountants             12.77   9271 15.70     63.4   1171 prof</span></span>
<span><span class="co">#&gt; purchasing.officers     11.42   8865  9.11     56.8   1175 prof</span></span>
<span><span class="co">#&gt; chemists                14.62   8403 11.68     73.5   2111 prof</span></span>
<span><span class="co">#&gt; physicists              15.64  11030  5.13     77.6   2113 prof</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">)</span></span>
<span><span class="co">#&gt;    education         income          women          prestige        census    </span></span>
<span><span class="co">#&gt;  Min.   : 6.38   Min.   :  611   Min.   : 0.00   Min.   :14.8   Min.   :1113  </span></span>
<span><span class="co">#&gt;  1st Qu.: 8.45   1st Qu.: 4106   1st Qu.: 3.59   1st Qu.:35.2   1st Qu.:3120  </span></span>
<span><span class="co">#&gt;  Median :10.54   Median : 5930   Median :13.60   Median :43.6   Median :5135  </span></span>
<span><span class="co">#&gt;  Mean   :10.74   Mean   : 6798   Mean   :28.98   Mean   :46.8   Mean   :5402  </span></span>
<span><span class="co">#&gt;  3rd Qu.:12.65   3rd Qu.: 8187   3rd Qu.:52.20   3rd Qu.:59.3   3rd Qu.:8312  </span></span>
<span><span class="co">#&gt;  Max.   :15.97   Max.   :25879   Max.   :97.51   Max.   :87.2   Max.   :9517  </span></span>
<span><span class="co">#&gt;    type   </span></span>
<span><span class="co">#&gt;  bc  :44  </span></span>
<span><span class="co">#&gt;  prof:31  </span></span>
<span><span class="co">#&gt;  wc  :23  </span></span>
<span><span class="co">#&gt;  NA's: 4  </span></span>
<span><span class="co">#&gt;           </span></span>
<span><span class="co">#&gt; </span></span></code></pre></div>
<p>The variables in the <code>Prestige</code> data set are:</p>
<ul>
<li>
<code>education</code>: average years of education for incumbents in
the occupation, from the 1971 Canadian Census.</li>
<li>
<code>income</code>: average dollars of annual income for the
occupation, from the Census.</li>
<li>
<code>women</code>: percentage of occupational incumbents who were
women, also from the Census.</li>
<li>
<code>prestige</code>: the average prestige rating of the occupation
on a 0–100 “thermometer” scale, in a Canadian social survey conducted
around the same time.</li>
<li>
<code>type</code>, type of occupation, and <code>census</code>, the
Census occupational code, which are not used in our example.</li>
</ul>
<p>The object of a regression analysis for the <code>Prestige</code>
data (and their original purpose) is to predict occupational prestige
from the other variables in the data set.</p>
<p>A scatterplot matrix (using the <code><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix()</a></code>
function in the <strong>car</strong> package) of the numeric variables
in the data reveals that the distributions of <code>income</code> and
<code>women</code> are positively skewed, and that some of the
relationships among the three predictors, and between the predictors and
the response (i.e., <code>prestige</code>), are nonlinear:</p>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://r-forge.r-project.org/projects/car/" class="external-link">"car"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: carData</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'car'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:dplyr':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     recode</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span><span class="op">~</span> <span class="va">prestige</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>,</span>
<span>                  data<span class="op">=</span><span class="va">Prestige</span>, smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/scatterplot-matrix-1.png" alt="Scatterplot matrix for the `Prestige` data." width="100%"><p class="caption">
Scatterplot matrix for the <code>Prestige</code> data.
</p>
</div>
<p>The <code><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform()</a></code> function in the
<strong>car</strong> package transforms variables towards multivariate
normality by a generalization of Box and Cox’s maximum-likelihood-like
approach <span class="citation">(Box &amp; Cox, 1964)</span>. Several
“families” of power transformations can be used, including the original
Box-Cox family, simple powers (and roots), and two adaptations of the
Box-Cox family to data that may include negative values and zeros: the
Box-Cox-with-negatives family and the Yeo-Johnson family; see <span class="citation">Weisberg (2014, Chapter 8)</span>, and <span class="citation">Fox &amp; Weisberg (2019, Chapter 3)</span> for
details. Because <code>women</code> has some zero values, we use the
Yeo-Johnson family:</p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">trans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">income</span>, <span class="va">education</span>, <span class="va">women</span><span class="op">)</span> <span class="op">~</span> <span class="fl">1</span>,</span>
<span>                         data<span class="op">=</span><span class="va">Prestige</span>, family<span class="op">=</span><span class="st">"yjPower"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">trans</span><span class="op">)</span></span>
<span><span class="co">#&gt; yjPower Transformations to Multinormality </span></span>
<span><span class="co">#&gt;           Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; income       0.2678        0.33       0.1051       0.4304</span></span>
<span><span class="co">#&gt; education    0.5162        1.00      -0.2822       1.3145</span></span>
<span><span class="co">#&gt; women        0.1630        0.16       0.0112       0.3149</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  Likelihood ratio test that all transformation parameters are equal to 0</span></span>
<span><span class="co">#&gt;                              LRT df    pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0 0 0) 15.739  3 0.00128</span></span></code></pre></div>
<p>We thus have evidence of the desirability of transforming
<code>income</code> (by the <span class="math inline">\(1/3\)</span>
power) and <code>women</code> (by the <span class="math inline">\(0.16\)</span> power—which is close to the “0”
power, i.e., the log transformation), but not <code>education</code>.
Applying the “rounded” power transformations makes the predictors
better-behaved:</p>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">P</span> <span class="op">&lt;-</span> <span class="va">Prestige</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"prestige"</span>, <span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="op">(</span><span class="va">lambdas</span> <span class="op">&lt;-</span> <span class="va">trans</span><span class="op">$</span><span class="va">roundlam</span><span class="op">)</span></span>
<span><span class="co">#&gt;    income education     women </span></span>
<span><span class="co">#&gt;   0.33000   1.00000   0.16302</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">lambdas</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">var</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">P</span><span class="op">[</span>, <span class="va">var</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/bcPower.html" class="external-link">yjPower</a></span><span class="op">(</span><span class="va">P</span><span class="op">[</span>, <span class="va">var</span><span class="op">]</span>, lambda<span class="op">=</span><span class="va">lambdas</span><span class="op">[</span><span class="va">var</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">P</span><span class="op">)</span></span>
<span><span class="co">#&gt;     prestige        income       education         women     </span></span>
<span><span class="co">#&gt;  Min.   :14.8   Min.   :22.2   Min.   : 6.38   Min.   :0.00  </span></span>
<span><span class="co">#&gt;  1st Qu.:35.2   1st Qu.:44.2   1st Qu.: 8.45   1st Qu.:1.73  </span></span>
<span><span class="co">#&gt;  Median :43.6   Median :50.3   Median :10.54   Median :3.36  </span></span>
<span><span class="co">#&gt;  Mean   :46.8   Mean   :50.8   Mean   :10.74   Mean   :3.50  </span></span>
<span><span class="co">#&gt;  3rd Qu.:59.3   3rd Qu.:56.2   3rd Qu.:12.65   3rd Qu.:5.59  </span></span>
<span><span class="co">#&gt;  Max.   :87.2   Max.   :83.6   Max.   :15.97   Max.   :6.83</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span><span class="op">~</span> <span class="va">prestige</span> <span class="op">+</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>,</span>
<span>                  data<span class="op">=</span><span class="va">P</span>, smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/transformed-predictors-1.png" alt="Scatterplot matrix for the `Prestige` data with the predictors transformed." width="100%"><p class="caption">
Scatterplot matrix for the <code>Prestige</code> data with the
predictors transformed.
</p>
</div>
<p>Comparing the MSE for the regressions with the original and
transformed predictors shows a advantage to the latter:</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.pres</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>, data<span class="op">=</span><span class="va">Prestige</span><span class="op">)</span></span>
<span><span class="va">m.pres.trans</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">prestige</span> <span class="op">~</span> <span class="va">income</span> <span class="op">+</span> <span class="va">education</span> <span class="op">+</span> <span class="va">women</span>, data<span class="op">=</span><span class="va">P</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Prestige</span><span class="op">$</span><span class="va">prestige</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.pres</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 59.153</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">P</span><span class="op">$</span><span class="va">prestige</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.pres.trans</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 50.6</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>Similarly, component+residual plots for the two regressions, produced
by the <code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots()</a></code> function in the <strong>car</strong>
package, suggest that the partial relationship of <code>prestige</code>
to <code>income</code> is more nearly linear in the transformed data,
but the transformation of <code>women</code> fails to capture what
appears to be a slight quadratic partial relationship; the partial
relationship of <code>prestige</code> to <code>education</code> is close
to linear in both regressions:</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m.pres</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/CR-plots-untransformed-1.png" alt="Component+residual plots for the `Prestige` regression with the original predictors." width="672"><p class="caption">
Component+residual plots for the <code>Prestige</code> regression with
the original predictors.
</p>
</div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m.pres.trans</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/CR-plots-transformed-1.png" alt="Component+residual plots for the `Prestige` regression with transformed predictors." width="672"><p class="caption">
Component+residual plots for the <code>Prestige</code> regression with
transformed predictors.
</p>
</div>
<p>Having transformed the predictors towards multinormality, we now
consider whether there’s evidence for transforming the response (using
<code><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform()</a></code> for Box and Cox’s original method), and we
discover that there’s not:</p>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="va">m.pres.trans</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; bcPower Transformation to Normality </span></span>
<span><span class="co">#&gt;    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; Y1    1.0194           1       0.6773       1.3615</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that transformation parameter is equal to 0</span></span>
<span><span class="co">#&gt;  (log transformation)</span></span>
<span><span class="co">#&gt;                          LRT df     pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0) 32.217  1 1.38e-08</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that no transformation is needed</span></span>
<span><span class="co">#&gt;                            LRT df  pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (1) 0.012384  1 0.911</span></span></code></pre></div>
<p>The <code><a href="../reference/cvSelect.html">selectTrans()</a></code> function in the <strong>cv</strong>
package automates the process of selecting predictor and response
transformations. The function takes a <code>data</code> set and
“working” <code>model</code> as arguments, along with the candidate
<code>predictors</code> and <code>response</code> for transformation,
and the transformation <code>family</code> to employ. If the
<code>predictors</code> argument is missing then only the response is
transformed, and if the <code>response</code> argument is missing, only
the supplied predictors are transformed. The default <code>family</code>
for transforming the predictors is <code>"bcPower"</code>—the original
Box-Cox family—as is the default <code>family.y</code> for transforming
the response; here we specify <code>family="yjPower</code> because of
the zeros in <code>women</code>. <code><a href="../reference/cvSelect.html">selectTrans()</a></code> returns the
result of applying a lack-of-fit criterion to the model after the
selected transformation is applied, with the default
<code>criterion=mse</code>:</p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cvSelect.html">selectTrans</a></span><span class="op">(</span>data<span class="op">=</span><span class="va">Prestige</span>, model<span class="op">=</span><span class="va">m.pres</span>,</span>
<span>            predictors<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span>,</span>
<span>            response<span class="op">=</span><span class="st">"prestige"</span>, family<span class="op">=</span><span class="st">"yjPower"</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 50.6</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p><code><a href="../reference/cvSelect.html">selectTrans()</a></code> also takes an optional
<code>indices</code> argument, making it suitable for doing computations
on a subset of the data (i.e., a CV fold), and hence for use with
<code><a href="../reference/cvSelect.html">cvSelect()</a></code> (see <code><a href="../reference/cvSelect.html">?selectTrans</a></code> for details):</p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cvs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cvSelect.html">cvSelect</a></span><span class="op">(</span><span class="va">selectTrans</span>, data<span class="op">=</span><span class="va">Prestige</span>, model<span class="op">=</span><span class="va">m.pres</span>, seed<span class="op">=</span><span class="fl">1463</span>,</span>
<span>                predictors<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"income"</span>, <span class="st">"education"</span>, <span class="st">"women"</span><span class="op">)</span>,</span>
<span>                response<span class="op">=</span><span class="st">"prestige"</span>,</span>
<span>                family<span class="op">=</span><span class="st">"yjPower"</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 1463</span></span>
<span><span class="va">cvs</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 54.487</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 54.308</span></span>
<span><span class="co">#&gt; full-sample criterion = 50.6</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.pres</span>, seed<span class="op">=</span><span class="fl">1463</span><span class="op">)</span> <span class="co"># untransformed model with same folds</span></span>
<span><span class="co">#&gt; R RNG seed set to 1463</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 63.293</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 63.073</span></span>
<span><span class="co">#&gt; full-sample criterion = 59.153</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cvSelect.html">compareFolds</a></span><span class="op">(</span><span class="va">cvs</span><span class="op">)</span></span>
<span><span class="co">#&gt;         lam.education lam.income lam.women lambda</span></span>
<span><span class="co">#&gt; Fold 1          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 2          1.000      0.330     0.169      1</span></span>
<span><span class="co">#&gt; Fold 3          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 4          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 5          1.000      0.330     0.000      1</span></span>
<span><span class="co">#&gt; Fold 6          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 7          1.000      0.330     0.330      1</span></span>
<span><span class="co">#&gt; Fold 8          1.000      0.330     0.000      1</span></span>
<span><span class="co">#&gt; Fold 9          1.000      0.330     0.000      1</span></span>
<span><span class="co">#&gt; Fold 10         1.000      0.330     0.000      1</span></span></code></pre></div>
<p>The results suggest that the predictive power of the transformed
regression is reliably greater than that of the untransformed regression
(though in both case, the cross-validated MSE is considerably higher
than the MSE computed for the whole data). Examining the selected
transformations for each fold reveals that the predictor
<code>education</code> and the response <code>prestige</code> are never
transformed; that the <span class="math inline">\(1/3\)</span> power is
selected for <code>income</code> in all of the folds; and that the
transformation selected for <code>women</code> varies narrowly across
the folds between the <span class="math inline">\(0\)</span>th power
(i.e., log) and the <span class="math inline">\(1/3\)</span> power.</p>
</div>
<div class="section level3">
<h3 id="selecting-both-transformations-and-predictorsvenables">Selecting both transformations and predictors<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The presentation in the section benefits from an email
conversation with Bill Venables, who of course isn’t responsible for the
use to which we’ve put his insightful remarks.&lt;/p&gt;"><sup>14</sup></a><a class="anchor" aria-label="anchor" href="#selecting-both-transformations-and-predictorsvenables"></a>
</h3>
<p>As we mentioned, <span class="citation">Hastie et al. (2009, sec.
7.10.2: “The Wrong and Right Way to Do Cross-validation”)</span> explain
that honest cross-validation has to take account of model specification
and selection. Statistical modeling is at least partly a craft, and one
could imagine applying that craft to successive partial data sets, each
with a fold removed. The resulting procedure would be tedious, though
possibly worth the effort, but it would also be difficult to realize in
practice: After all, we can hardly erase our memory of statistical
modeling choices between analyzing partial data sets.</p>
<p>Alternatively, if we’re able to automate the process of model
selection, then we can more realistically apply CV mechanically. That’s
what we did in the preceding two sections, first for predictor selection
and then for selection of transformations in regression. In this
section, we consider the case where we both select variable
transformations and then proceed to select predictors. It’s insufficient
to apply these steps sequentially, first, for example, using
<code><a href="../reference/cvSelect.html">cvSelect()</a></code> with <code><a href="../reference/cvSelect.html">selectTrans()</a></code> and then with
<code><a href="../reference/cvSelect.html">selectStepAIC()</a></code>; rather we should apply the whole
model-selection procedure with each fold omitted. The
<code>selectTransAndStepAIC()</code> function, also supplied by the
<strong>cv</strong> package, does exactly that.</p>
<p>To illustrate this process, we return to the <code>Auto</code> data
set:</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt;       mpg         cylinders     displacement   horsepower        weight    </span></span>
<span><span class="co">#&gt;  Min.   : 9.0   Min.   :3.00   Min.   : 68   Min.   : 46.0   Min.   :1613  </span></span>
<span><span class="co">#&gt;  1st Qu.:17.0   1st Qu.:4.00   1st Qu.:105   1st Qu.: 75.0   1st Qu.:2225  </span></span>
<span><span class="co">#&gt;  Median :22.8   Median :4.00   Median :151   Median : 93.5   Median :2804  </span></span>
<span><span class="co">#&gt;  Mean   :23.4   Mean   :5.47   Mean   :194   Mean   :104.5   Mean   :2978  </span></span>
<span><span class="co">#&gt;  3rd Qu.:29.0   3rd Qu.:8.00   3rd Qu.:276   3rd Qu.:126.0   3rd Qu.:3615  </span></span>
<span><span class="co">#&gt;  Max.   :46.6   Max.   :8.00   Max.   :455   Max.   :230.0   Max.   :5140  </span></span>
<span><span class="co">#&gt;                                                                            </span></span>
<span><span class="co">#&gt;   acceleration       year        origin                     name    </span></span>
<span><span class="co">#&gt;  Min.   : 8.0   Min.   :70   Min.   :1.00   amc matador       :  5  </span></span>
<span><span class="co">#&gt;  1st Qu.:13.8   1st Qu.:73   1st Qu.:1.00   ford pinto        :  5  </span></span>
<span><span class="co">#&gt;  Median :15.5   Median :76   Median :1.00   toyota corolla    :  5  </span></span>
<span><span class="co">#&gt;  Mean   :15.5   Mean   :76   Mean   :1.58   amc gremlin       :  4  </span></span>
<span><span class="co">#&gt;  3rd Qu.:17.0   3rd Qu.:79   3rd Qu.:2.00   amc hornet        :  4  </span></span>
<span><span class="co">#&gt;  Max.   :24.8   Max.   :82   Max.   :3.00   chevrolet chevette:  4  </span></span>
<span><span class="co">#&gt;                                             (Other)           :365</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span><span class="op">~</span> <span class="va">year</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; year</span></span>
<span><span class="co">#&gt; 70 71 72 73 74 75 76 77 78 79 80 81 82 </span></span>
<span><span class="co">#&gt; 29 27 28 40 26 30 34 28 36 29 27 28 30</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span><span class="op">~</span> <span class="va">origin</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; origin</span></span>
<span><span class="co">#&gt;   1   2   3 </span></span>
<span><span class="co">#&gt; 245  68  79</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/xtabs.html" class="external-link">xtabs</a></span><span class="op">(</span><span class="op">~</span> <span class="va">cylinders</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; cylinders</span></span>
<span><span class="co">#&gt;   3   4   5   6   8 </span></span>
<span><span class="co">#&gt;   4 199   3  83 103</span></span></code></pre></div>
<p>We previously used the <code>Auto</code> here in a preliminary
example where we employed CV to inform the selection of the order of a
polynomial regression of <code>mpg</code> on <code>horsepower</code>.
Here, we consider more generally the problem of predicting
<code>mpg</code> from the other variables in the <code>Auto</code> data.
We begin with a bit of data management, and then examine the pairwise
relationships among the numeric variables in the data set:</p>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Auto</span><span class="op">$</span><span class="va">cylinders</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">cylinders</span>,</span>
<span>                         labels<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"3.4"</span>, <span class="st">"3.4"</span>, <span class="st">"5.6"</span>, <span class="st">"5.6"</span>, <span class="st">"8"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">Auto</span><span class="op">$</span><span class="va">year</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">year</span><span class="op">)</span></span>
<span><span class="va">Auto</span><span class="op">$</span><span class="va">origin</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">origin</span>,</span>
<span>                      labels<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"America"</span>, <span class="st">"Europe"</span>, <span class="st">"Japan"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">rownames</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/make.names.html" class="external-link">make.names</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">name</span>, unique<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">Auto</span><span class="op">$</span><span class="va">name</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span><span class="op">~</span> <span class="va">mpg</span> <span class="op">+</span> <span class="va">displacement</span> <span class="op">+</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="va">weight</span> <span class="op">+</span> <span class="va">acceleration</span>, </span>
<span>                  smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-explore-1.png" alt="Scatterplot matrix for the numeric variables in the `Auto` data" width="100%"><p class="caption">
Scatterplot matrix for the numeric variables in the <code>Auto</code>
data
</p>
</div>
<p>A comment before we proceed: <code>origin</code> is clearly
categorical and so converting it to a factor is natural, but we could
imagine treating <code>cylinders</code> and <code>year</code> as numeric
predictors. There are, however, only 5 distinct values of
<code>cylinders</code> (ranging from 3 to 8), but cars with 3 or 5
cylinders are rare. and none of the cars has 7 cylinders. There are
similarly only 13 distinct years between 1970 and 1982 in the data, and
the relationship between <code>mpg</code> and <code>year</code> is
difficult to characterize.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Of course, making the decision to treat
&lt;code&gt;year&lt;/code&gt; as a factor on this basis could be construed as
cheating in the current context, which illustrates the difficulty of
automating the whole model-selection process. It’s rarely desirable, in
our opinion, to forgo exploration of the data to ensure the purity of
model validation. We believe, however, that it’s still useful to
automate as much of the process as we can to obtain a more realistic, if
still biased, estimate of the predictive power of a model.&lt;/p&gt;"><sup>15</sup></a> It’s apparent that most these variables
are positively skewed and that many of the pairwise relationships among
them are nonlinear.</p>
<p>We begin with a “working model” that specifies linear partial
relationships of the response to the numeric predictors:</p>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ ., data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span><span class="co">#&gt; -9.006 -1.745 -0.092  1.525 10.950 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)  37.034132   1.969393   18.80  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; cylinders5.6 -2.602941   0.655200   -3.97  8.5e-05 ***</span></span>
<span><span class="co">#&gt; cylinders8   -0.582458   1.171452   -0.50  0.61934    </span></span>
<span><span class="co">#&gt; displacement  0.017425   0.006734    2.59  0.01004 *  </span></span>
<span><span class="co">#&gt; horsepower   -0.041353   0.013379   -3.09  0.00215 ** </span></span>
<span><span class="co">#&gt; weight       -0.005548   0.000632   -8.77  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration  0.061527   0.088313    0.70  0.48643    </span></span>
<span><span class="co">#&gt; year71        0.968058   0.837390    1.16  0.24841    </span></span>
<span><span class="co">#&gt; year72       -0.601435   0.825115   -0.73  0.46652    </span></span>
<span><span class="co">#&gt; year73       -0.687689   0.740272   -0.93  0.35351    </span></span>
<span><span class="co">#&gt; year74        1.375576   0.876500    1.57  0.11741    </span></span>
<span><span class="co">#&gt; year75        0.929929   0.859072    1.08  0.27974    </span></span>
<span><span class="co">#&gt; year76        1.559893   0.822505    1.90  0.05867 .  </span></span>
<span><span class="co">#&gt; year77        2.909416   0.841729    3.46  0.00061 ***</span></span>
<span><span class="co">#&gt; year78        3.175198   0.798940    3.97  8.5e-05 ***</span></span>
<span><span class="co">#&gt; year79        5.019299   0.845759    5.93  6.8e-09 ***</span></span>
<span><span class="co">#&gt; year80        9.099763   0.897293   10.14  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year81        6.688660   0.885218    7.56  3.3e-13 ***</span></span>
<span><span class="co">#&gt; year82        8.071125   0.870668    9.27  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; originEurope  2.046664   0.517124    3.96  9.1e-05 ***</span></span>
<span><span class="co">#&gt; originJapan   2.144887   0.507717    4.22  3.0e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 2.92 on 371 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.867,  Adjusted R-squared:  0.86 </span></span>
<span><span class="co">#&gt; F-statistic:  121 on 20 and 371 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: mpg</span></span>
<span><span class="co">#&gt;              Sum Sq  Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; cylinders       292   2   17.09 7.9e-08 ***</span></span>
<span><span class="co">#&gt; displacement     57   1    6.70  0.0100 *  </span></span>
<span><span class="co">#&gt; horsepower       82   1    9.55  0.0021 ** </span></span>
<span><span class="co">#&gt; weight          658   1   76.98 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration      4   1    0.49  0.4864    </span></span>
<span><span class="co">#&gt; year           3017  12   29.40 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; origin          190   2   11.13 2.0e-05 ***</span></span>
<span><span class="co">#&gt; Residuals      3173 371                    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-working-model-1.png" alt="Component+residual plots for the working model fit to the `Auto` data" width="100%"><p class="caption">
Component+residual plots for the working model fit to the
<code>Auto</code> data
</p>
</div>
<p>The component+residual plots, created with the <code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots()</a></code>
function in the previously loaded <strong>car</strong> package, clearly
reveal the inadequacy of the model.</p>
<p>We proceed to transform the numeric predictors towards
multi-normality:</p>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">num.predictors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"displacement"</span>, <span class="st">"horsepower"</span>, <span class="st">"weight"</span>, <span class="st">"acceleration"</span><span class="op">)</span></span>
<span><span class="va">tr.x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">[</span>, <span class="va">num.predictors</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">tr.x</span><span class="op">)</span></span>
<span><span class="co">#&gt; bcPower Transformations to Multinormality </span></span>
<span><span class="co">#&gt;              Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; displacement   -0.0509           0      -0.2082       0.1065</span></span>
<span><span class="co">#&gt; horsepower     -0.1249           0      -0.2693       0.0194</span></span>
<span><span class="co">#&gt; weight         -0.0870           0      -0.2948       0.1208</span></span>
<span><span class="co">#&gt; acceleration    0.3061           0      -0.0255       0.6376</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that transformation parameters are equal to 0</span></span>
<span><span class="co">#&gt;  (all log transformations)</span></span>
<span><span class="co">#&gt;                                LRT df  pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0 0 0 0) 4.8729  4 0.301</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that no transformations are needed</span></span>
<span><span class="co">#&gt;                                LRT df   pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (1 1 1 1) 390.08  4 &lt;2e-16</span></span></code></pre></div>
<p>We then apply the (rounded) transformations—all, as it turns out,
logs—to the data and re-estimate the model:</p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">A</span> <span class="op">&lt;-</span> <span class="va">Auto</span></span>
<span><span class="va">powers</span> <span class="op">&lt;-</span> <span class="va">tr.x</span><span class="op">$</span><span class="va">roundlam</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">pred</span> <span class="kw">in</span> <span class="va">num.predictors</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">A</span><span class="op">[</span>, <span class="va">pred</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/bcPower.html" class="external-link">bcPower</a></span><span class="op">(</span><span class="va">A</span><span class="op">[</span>, <span class="va">pred</span><span class="op">]</span>, lambda<span class="op">=</span><span class="va">powers</span><span class="op">[</span><span class="va">pred</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">A</span><span class="op">)</span></span>
<span><span class="co">#&gt;                           mpg cylinders displacement horsepower weight</span></span>
<span><span class="co">#&gt; chevrolet.chevelle.malibu  18         8       5.7268     4.8675 8.1617</span></span>
<span><span class="co">#&gt; buick.skylark.320          15         8       5.8579     5.1059 8.2142</span></span>
<span><span class="co">#&gt; plymouth.satellite         18         8       5.7621     5.0106 8.1421</span></span>
<span><span class="co">#&gt; amc.rebel.sst              16         8       5.7170     5.0106 8.1412</span></span>
<span><span class="co">#&gt; ford.torino                17         8       5.7104     4.9416 8.1458</span></span>
<span><span class="co">#&gt; ford.galaxie.500           15         8       6.0615     5.2883 8.3759</span></span>
<span><span class="co">#&gt;                           acceleration year  origin</span></span>
<span><span class="co">#&gt; chevrolet.chevelle.malibu       2.4849   70 America</span></span>
<span><span class="co">#&gt; buick.skylark.320               2.4423   70 America</span></span>
<span><span class="co">#&gt; plymouth.satellite              2.3979   70 America</span></span>
<span><span class="co">#&gt; amc.rebel.sst                   2.4849   70 America</span></span>
<span><span class="co">#&gt; ford.torino                     2.3514   70 America</span></span>
<span><span class="co">#&gt; ford.galaxie.500                2.3026   70 America</span></span>
<span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">m.auto</span>, data<span class="op">=</span><span class="va">A</span><span class="op">)</span></span></code></pre></div>
<p>Finally, we perform Box-Cox regression to transform the response
(also obtaining a log transformation):</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/powerTransform.html" class="external-link">powerTransform</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; bcPower Transformation to Normality </span></span>
<span><span class="co">#&gt;    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd</span></span>
<span><span class="co">#&gt; Y1    0.0024           0      -0.1607       0.1654</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that transformation parameter is equal to 0</span></span>
<span><span class="co">#&gt;  (log transformation)</span></span>
<span><span class="co">#&gt;                              LRT df  pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (0) 0.00080154  1 0.977</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Likelihood ratio test that no transformation is needed</span></span>
<span><span class="co">#&gt;                          LRT df   pval</span></span>
<span><span class="co">#&gt; LR test, lambda = (1) 124.13  1 &lt;2e-16</span></span>
<span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">m</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(mpg) ~ cylinders + displacement + horsepower + </span></span>
<span><span class="co">#&gt;     weight + acceleration + year + origin, data = A)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -0.3341 -0.0577  0.0041  0.0607  0.3808 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)    8.8965     0.3582   24.84  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; cylinders5.6  -0.0636     0.0257   -2.47    0.014 *  </span></span>
<span><span class="co">#&gt; cylinders8    -0.0769     0.0390   -1.97    0.049 *  </span></span>
<span><span class="co">#&gt; displacement   0.0280     0.0515    0.54    0.587    </span></span>
<span><span class="co">#&gt; horsepower    -0.2901     0.0563   -5.15  4.2e-07 ***</span></span>
<span><span class="co">#&gt; weight        -0.5427     0.0819   -6.62  1.2e-10 ***</span></span>
<span><span class="co">#&gt; acceleration  -0.1421     0.0563   -2.52    0.012 *  </span></span>
<span><span class="co">#&gt; year71         0.0250     0.0289    0.87    0.387    </span></span>
<span><span class="co">#&gt; year72        -0.0168     0.0289   -0.58    0.562    </span></span>
<span><span class="co">#&gt; year73        -0.0426     0.0260   -1.64    0.103    </span></span>
<span><span class="co">#&gt; year74         0.0493     0.0304    1.62    0.106    </span></span>
<span><span class="co">#&gt; year75         0.0472     0.0296    1.59    0.112    </span></span>
<span><span class="co">#&gt; year76         0.0709     0.0284    2.49    0.013 *  </span></span>
<span><span class="co">#&gt; year77         0.1324     0.0293    4.52  8.2e-06 ***</span></span>
<span><span class="co">#&gt; year78         0.1447     0.0278    5.21  3.1e-07 ***</span></span>
<span><span class="co">#&gt; year79         0.2335     0.0292    7.99  1.7e-14 ***</span></span>
<span><span class="co">#&gt; year80         0.3238     0.0317   10.22  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year81         0.2565     0.0309    8.29  2.1e-15 ***</span></span>
<span><span class="co">#&gt; year82         0.3076     0.0304   10.13  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; originEurope   0.0492     0.0195    2.52    0.012 *  </span></span>
<span><span class="co">#&gt; originJapan    0.0441     0.0195    2.26    0.024 *  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.104 on 371 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.911,  Adjusted R-squared:  0.906 </span></span>
<span><span class="co">#&gt; F-statistic:  189 on 20 and 371 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: log(mpg)</span></span>
<span><span class="co">#&gt;              Sum Sq  Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; cylinders      0.07   2    3.05   0.048 *  </span></span>
<span><span class="co">#&gt; displacement   0.00   1    0.30   0.587    </span></span>
<span><span class="co">#&gt; horsepower     0.29   1   26.54 4.2e-07 ***</span></span>
<span><span class="co">#&gt; weight         0.48   1   43.88 1.2e-10 ***</span></span>
<span><span class="co">#&gt; acceleration   0.07   1    6.37   0.012 *  </span></span>
<span><span class="co">#&gt; year           4.45  12   34.13 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; origin         0.08   2    3.71   0.025 *  </span></span>
<span><span class="co">#&gt; Residuals      4.03 371                    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>The transformed numeric variables are much better-behaved:</p>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span><span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span> <span class="op">+</span> <span class="va">displacement</span> <span class="op">+</span> <span class="va">horsepower</span> <span class="op">+</span> <span class="va">weight</span> </span>
<span>                  <span class="op">+</span> <span class="va">acceleration</span>, </span>
<span>                  smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>spread<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span>, data<span class="op">=</span><span class="va">A</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-transformed-scatterplot-matrix-1.png" alt="Scatterplot matrix for the transformed numeric variables in the `Auto` data" width="100%"><p class="caption">
Scatterplot matrix for the transformed numeric variables in the
<code>Auto</code> data
</p>
</div>
<p>And the partial relationships in the model fit to the transformed
data are much more nearly linear:</p>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/Auto-CR-plots-transformed-1.png" alt="Component+residual plots for the model fit to the transformed `Auto` data" width="100%"><p class="caption">
Component+residual plots for the model fit to the transformed
<code>Auto</code> data
</p>
</div>
<p>Having transformed both the numeric predictors and the response, we
proceed to use the <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> function in the
<strong>MASS</strong> package to perform predictor selection, employing
the BIC model-selection criterion (by setting the <code>k</code>
argument of <code><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC()</a></code> to <span class="math inline">\(\log(n)\)</span>):</p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.step</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/stepAIC.html" class="external-link">stepAIC</a></span><span class="op">(</span><span class="va">m</span>, k<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">A</span><span class="op">)</span><span class="op">)</span>, trace<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = log(mpg) ~ horsepower + weight + acceleration + </span></span>
<span><span class="co">#&gt;     year + origin, data = A)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -0.3523 -0.0568  0.0068  0.0674  0.3586 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)   9.43459    0.26153   36.07  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; horsepower   -0.27625    0.05614   -4.92  1.3e-06 ***</span></span>
<span><span class="co">#&gt; weight       -0.60907    0.05600  -10.88  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration -0.13138    0.05319   -2.47  0.01397 *  </span></span>
<span><span class="co">#&gt; year71        0.02798    0.02894    0.97  0.33412    </span></span>
<span><span class="co">#&gt; year72       -0.00711    0.02845   -0.25  0.80274    </span></span>
<span><span class="co">#&gt; year73       -0.03953    0.02601   -1.52  0.12947    </span></span>
<span><span class="co">#&gt; year74        0.05275    0.02999    1.76  0.07936 .  </span></span>
<span><span class="co">#&gt; year75        0.05320    0.02928    1.82  0.07004 .  </span></span>
<span><span class="co">#&gt; year76        0.07432    0.02821    2.63  0.00878 ** </span></span>
<span><span class="co">#&gt; year77        0.13793    0.02888    4.78  2.6e-06 ***</span></span>
<span><span class="co">#&gt; year78        0.14588    0.02753    5.30  2.0e-07 ***</span></span>
<span><span class="co">#&gt; year79        0.23604    0.02908    8.12  7.0e-15 ***</span></span>
<span><span class="co">#&gt; year80        0.33527    0.03115   10.76  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year81        0.26287    0.03056    8.60  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; year82        0.32339    0.02961   10.92  &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; originEurope  0.05582    0.01678    3.33  0.00097 ***</span></span>
<span><span class="co">#&gt; originJapan   0.04355    0.01748    2.49  0.01314 *  </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 0.105 on 374 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.909,  Adjusted R-squared:  0.905 </span></span>
<span><span class="co">#&gt; F-statistic:  220 on 17 and 374 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/Anova.html" class="external-link">Anova</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span></span>
<span><span class="co">#&gt; Anova Table (Type II tests)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Response: log(mpg)</span></span>
<span><span class="co">#&gt;              Sum Sq  Df F value  Pr(&gt;F)    </span></span>
<span><span class="co">#&gt; horsepower     0.27   1   24.21 1.3e-06 ***</span></span>
<span><span class="co">#&gt; weight         1.30   1  118.28 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; acceleration   0.07   1    6.10  0.0140 *  </span></span>
<span><span class="co">#&gt; year           4.76  12   36.05 &lt; 2e-16 ***</span></span>
<span><span class="co">#&gt; origin         0.14   2    6.21  0.0022 ** </span></span>
<span><span class="co">#&gt; Residuals      4.11 374                    </span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span></code></pre></div>
<p>The selected model includes three of the numeric predictors,
<code>horsepower</code>, <code>weight</code>, and
<code>acceleration</code>, along with the factors <code>year</code> and
<code>origin</code>. We can calculate the MSE for this model, but we
expect that the result will be optimistic because we used the whole data
to help specify the model</p>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 6.5121</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>This is considerably smaller than the MSE for the original working
model:</p>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 8.0932</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "(y - yhat)^2"</span></span></code></pre></div>
<p>A perhaps subtle point is that we compute the MSE for the selected
model on the original <code>mpg</code> response scale rather than the
log scale, so as to make the selected model comparable to the working
model. That’s slightly uncomfortable given the skewed distribution of
<code>mpg</code>. An alternative is to use the median absolute error
instead of the mean-squared error, employing the
<code><a href="../reference/cost-functions.html">medAbsErr()</a></code> function from the <strong>cv</strong>
package:</p>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cost-functions.html">medAbsErr</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.step</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.3396</span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">medAbsErr</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.6661</span></span></code></pre></div>
<p>Now let’s use <code><a href="../reference/cvSelect.html">cvSelect()</a></code> with
<code>selectTransAndStepAIC()</code> to automate and cross-validate the
whole model-specification process:</p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">num.predictors</span></span>
<span><span class="co">#&gt; [1] "displacement" "horsepower"   "weight"       "acceleration"</span></span>
<span><span class="va">cvs</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cvSelect.html">cvSelect</a></span><span class="op">(</span><span class="va">selectTransStepAIC</span>, data<span class="op">=</span><span class="va">Auto</span>, seed<span class="op">=</span><span class="fl">76692</span>, model<span class="op">=</span><span class="va">m.auto</span>,</span>
<span>                predictors<span class="op">=</span><span class="va">num.predictors</span>,</span>
<span>                response<span class="op">=</span><span class="st">"mpg"</span>, AIC<span class="op">=</span><span class="cn">FALSE</span>, criterion<span class="op">=</span><span class="va">medAbsErr</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 76692</span></span>
<span><span class="va">cvs</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; cross-validation criterion = 1.4951</span></span>
<span><span class="co">#&gt; full-sample criterion = 1.3396</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cvSelect.html">compareFolds</a></span><span class="op">(</span><span class="va">cvs</span><span class="op">)</span></span>
<span><span class="co">#&gt;         (Intercept) horsepower lam.acceleration lam.displacement lam.horsepower</span></span>
<span><span class="co">#&gt; Fold 1      9.71384   -0.17408          0.50000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 2      9.21713   -0.31480          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 3      9.61824   -0.19248          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 4      8.69910   -0.25523          0.50000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 5      9.14403   -0.14934          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 6      9.63481   -0.16739          0.50000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 7      9.98933   -0.36847          0.00000          0.00000       -0.15447</span></span>
<span><span class="co">#&gt; Fold 8      9.06301   -0.29721          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 9      8.88315   -0.22684          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt; Fold 10     9.61727   -0.17086          0.00000          0.00000        0.00000</span></span>
<span><span class="co">#&gt;         lam.weight   lambda   weight   year71   year72   year73   year74</span></span>
<span><span class="co">#&gt; Fold 1     0.00000  0.00000 -0.74636  0.03764 -0.00327 -0.02477  0.05606</span></span>
<span><span class="co">#&gt; Fold 2     0.00000  0.00000 -0.47728  0.02173 -0.01488 -0.03770  0.04312</span></span>
<span><span class="co">#&gt; Fold 3     0.00000  0.00000 -0.72085  0.01128 -0.02569 -0.03872  0.05187</span></span>
<span><span class="co">#&gt; Fold 4     0.00000  0.00000 -0.53846  0.02153 -0.02922 -0.05181  0.04136</span></span>
<span><span class="co">#&gt; Fold 5     0.00000  0.00000 -0.69081  0.02531 -0.01062 -0.04625  0.05039</span></span>
<span><span class="co">#&gt; Fold 6     0.00000  0.00000 -0.74049  0.02456  0.00759 -0.03412  0.06266</span></span>
<span><span class="co">#&gt; Fold 7     0.00000  0.00000 -0.72843  0.02532 -0.01271 -0.04144  0.04568</span></span>
<span><span class="co">#&gt; Fold 8     0.00000  0.00000 -0.46392  0.02702 -0.02041 -0.05605  0.04437</span></span>
<span><span class="co">#&gt; Fold 9     0.00000  0.00000 -0.47136  0.00860 -0.03620 -0.04835  0.01906</span></span>
<span><span class="co">#&gt; Fold 10    0.00000  0.00000 -0.73550  0.02937 -0.00899 -0.03814  0.05408</span></span>
<span><span class="co">#&gt;           year75   year76   year77   year78   year79   year80   year81   year82</span></span>
<span><span class="co">#&gt; Fold 1   0.07080  0.07250  0.14420  0.14281  0.23266  0.35127  0.25635  0.30546</span></span>
<span><span class="co">#&gt; Fold 2   0.04031  0.06718  0.13094  0.14917  0.21871  0.33192  0.26196  0.30943</span></span>
<span><span class="co">#&gt; Fold 3   0.03837  0.06399  0.11593  0.12601  0.20499  0.32821  0.24478  0.29204</span></span>
<span><span class="co">#&gt; Fold 4   0.04072  0.05537  0.12292  0.14083  0.22878  0.32947  0.25140  0.27248</span></span>
<span><span class="co">#&gt; Fold 5   0.05596  0.07044  0.13356  0.14724  0.24675  0.33331  0.26938  0.32594</span></span>
<span><span class="co">#&gt; Fold 6   0.06940  0.07769  0.14211  0.14647  0.23532  0.34761  0.26737  0.33062</span></span>
<span><span class="co">#&gt; Fold 7   0.03614  0.07385  0.12976  0.14040  0.23976  0.33998  0.27652  0.30659</span></span>
<span><span class="co">#&gt; Fold 8   0.06573  0.08135  0.13158  0.13987  0.23011  0.32880  0.25886  0.30538</span></span>
<span><span class="co">#&gt; Fold 9   0.03018  0.05846  0.10536  0.11722  0.20665  0.31533  0.23352  0.29375</span></span>
<span><span class="co">#&gt; Fold 10  0.04881  0.07862  0.14101  0.14313  0.23258  0.35649  0.26214  0.32421</span></span>
<span><span class="co">#&gt;         acceleration displacement cylinders5.6 cylinders8 originEurope</span></span>
<span><span class="co">#&gt; Fold 1                                                                </span></span>
<span><span class="co">#&gt; Fold 2      -0.18909     -0.09197                                     </span></span>
<span><span class="co">#&gt; Fold 3                                                                </span></span>
<span><span class="co">#&gt; Fold 4      -0.03484                  -0.09080   -0.10909             </span></span>
<span><span class="co">#&gt; Fold 5                                                         0.06261</span></span>
<span><span class="co">#&gt; Fold 6                                                                </span></span>
<span><span class="co">#&gt; Fold 7                                                                </span></span>
<span><span class="co">#&gt; Fold 8      -0.17676     -0.10542                                     </span></span>
<span><span class="co">#&gt; Fold 9      -0.14514     -0.13452                                     </span></span>
<span><span class="co">#&gt; Fold 10                                                               </span></span>
<span><span class="co">#&gt;         originJapan</span></span>
<span><span class="co">#&gt; Fold 1             </span></span>
<span><span class="co">#&gt; Fold 2             </span></span>
<span><span class="co">#&gt; Fold 3             </span></span>
<span><span class="co">#&gt; Fold 4             </span></span>
<span><span class="co">#&gt; Fold 5         0.04</span></span>
<span><span class="co">#&gt; Fold 6             </span></span>
<span><span class="co">#&gt; Fold 7             </span></span>
<span><span class="co">#&gt; Fold 8             </span></span>
<span><span class="co">#&gt; Fold 9             </span></span>
<span><span class="co">#&gt; Fold 10</span></span></code></pre></div>
<p>Here, as for <code><a href="../reference/cvSelect.html">selectTrans()</a></code>, the <code>predictors</code>
and <code>response</code> arguments specify candidate variables for
transformation, and <code>AIC=FALSE</code> uses the BIC for model
selection. The starting model, <code>m.auto</code>, is the working model
fit to the <code>Auto</code> data. The CV criterion isn’t bias-adjusted
because median absolute error isn’t a mean of casewise error
components.</p>
<p>Some noteworthy points:</p>
<ul>
<li>
<code><a href="../reference/cvSelect.html">selectTransStepAIC()</a></code> automatically computes CV cost
criteria, here the median absolute error, on the untransformed response
scale.</li>
<li>The estimate of the median absolute error that we obtain by
cross-validating the whole model-specification process is a little
larger than the median absolute error computed for the model we fit to
the <code>Auto</code> data separately selecting transformations of the
predictors and the response and then selecting predictors for the whole
data set.</li>
<li>When we look at the transformations and predictors selected with
each of the 10 folds omitted (i.e., the output of
<code><a href="../reference/cvSelect.html">compareFolds()</a></code>), we see that there is little uncertainty in
choosing variable transformations (the <code>lam.*</code>s for the <span class="math inline">\(x\)</span>s and <code>lambda</code> for <span class="math inline">\(y\)</span> in the output), but considerably more
uncertainty in subsequently selecting predictors:
<code>horsepower</code>, <code>weight</code>, and <code>year</code> are
always included among the selected predictors; <code>acceleration</code>
and <code>displacement</code> are each included respectively in 4 and 3
of 10 selected models; and <code>cylinders</code> and
<code>origin</code> are each included in only 1 of 10 models. Recall
that when we selected predictors for the full data, we obtained a model
with <code>horsepower</code>, <code>weight</code>,
<code>acceleration</code>, <code>year</code>, and
<code>origin</code>.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="parallel-computations">Parallel computations<a class="anchor" aria-label="anchor" href="#parallel-computations"></a>
</h2>
<p>The CV functions in the <strong>cv</strong> package are all capable
of performing parallel computations by setting the <code>ncores</code>
argument (specifying the number of computer cores to be used) to a
number &gt; <code>1</code> (which is the default). Parallel computation
can be advantageous for large problems, reducing the execution time of
the program.</p>
<p>To illustrate, let’s time model selection in Mroz’s logistic
regression, repeating the computation as performed previously and then
doing it in parallel using 2 cores:</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">m.mroz.sel.cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cvSelect.html">cvSelect</a></span><span class="op">(</span><span class="va">selectStepAIC</span>, <span class="va">Mroz</span>,</span>
<span>                          seed<span class="op">=</span><span class="fl">6681</span>,</span>
<span>                          criterion<span class="op">=</span><span class="va">BayesRule</span>,</span>
<span>                          model<span class="op">=</span><span class="va">m.mroz</span>,</span>
<span>                          AIC<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 6681</span></span>
<span><span class="co">#&gt;    user  system elapsed </span></span>
<span><span class="co">#&gt;   0.506   0.000   0.506</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span><span class="va">m.mroz.sel.cv.p</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cvSelect.html">cvSelect</a></span><span class="op">(</span><span class="va">selectStepAIC</span>, <span class="va">Mroz</span>,</span>
<span>                          seed<span class="op">=</span><span class="fl">6681</span>,</span>
<span>                          criterion<span class="op">=</span><span class="va">BayesRule</span>,</span>
<span>                          model<span class="op">=</span><span class="va">m.mroz</span>,</span>
<span>                          AIC<span class="op">=</span><span class="cn">FALSE</span>,</span>
<span>                          ncores<span class="op">=</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 6681</span></span>
<span><span class="co">#&gt;    user  system elapsed </span></span>
<span><span class="co">#&gt;   0.069   0.000   1.454</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/all.equal-methods.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">m.mroz.sel.cv</span>, <span class="va">m.mroz.sel.cv.p</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>In this small problem, the parallel computation is actually
<em>slower</em>, because there is an overhead cost to parallelization,
but we can see that it produces the same result as before.</p>
</div>
<div class="section level2">
<h2 id="computational-notes">Computational notes<a class="anchor" aria-label="anchor" href="#computational-notes"></a>
</h2>
<div class="section level3">
<h3 id="efficient-computations-for-linear-and-generalized-linear-models">Efficient computations for linear and generalized linear models<a class="anchor" aria-label="anchor" href="#efficient-computations-for-linear-and-generalized-linear-models"></a>
</h3>
<p>The most straightforward way to implement cross-validation in R for
statistical modeling functions that are written in the canonical manner
is to use <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> to refit the model with each fold
removed. This is the approach taken in the default method for
<code><a href="../reference/cv.html">cv()</a></code>, and it is appropriate if the cases are independently
sampled. Refitting the model in this manner for each fold is generally
feasible when the number of folds in modest, but can be prohibitively
costly for leave-one-out cross-validation when the number of cases is
large.</p>
<p>The <code>"lm"</code> and <code>"glm"</code> methods for
<code><a href="../reference/cv.html">cv()</a></code> take advantage of computational efficiencies by
avoiding refitting the model with each fold removed. Consider, in
particular, the weighted linear model <span class="math inline">\(\mathbf{y}_{n \times 1} = \mathbf{X}_{n \times
p}\boldsymbol{\beta}_{p \times 1} + \boldsymbol{\varepsilon}_{n \times
1}\)</span>, where <span class="math inline">\(\boldsymbol{\varepsilon}
\sim \mathbf{N}_n \left(\mathbf{0}, \sigma^2 \mathbf{W}^{-1}_{n \times
n}\right)\)</span>. Here, <span class="math inline">\(\mathbf{y}\)</span> is the response vector, <span class="math inline">\(\mathbf{X}\)</span> the model matrix, and <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> the error
vector, each for <span class="math inline">\(n\)</span> cases, and <span class="math inline">\(\boldsymbol{\beta}\)</span> is the vector of <span class="math inline">\(p\)</span> population regression coefficients. The
errors are assumed to be multivariately normally distributed with 0
means and covariance matrix <span class="math inline">\(\sigma^2
\mathbf{W}^{-1}\)</span>, where <span class="math inline">\(\mathbf{W} =
\mathrm{diag}(w_i)\)</span> is a diagonal matrix of inverse-variance
weights. For the linear model with constant error variance, the weight
matrix is taken to be <span class="math inline">\(\mathbf{W} =
\mathbf{I}_n\)</span>, the order-<span class="math inline">\(n\)</span>
identity matrix.</p>
<p>The weighted-least-squares (WLS) estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is <span class="citation">(see, e.g., Fox, 2016, sec. 12.2.2)</span> <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;This is a definitional formula, which assumes that the
model matrix &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is of full
column rank, and which can be subject to numerical instability when
&lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is ill-conditioned.
&lt;code&gt;&lt;a href="https://rdrr.io/r/stats/lm.html" class="external-link"&gt;lm()&lt;/a&gt;&lt;/code&gt; uses the singular-value decomposition of the model
matrix to obtain computationally more stable results.&lt;/p&gt;'><sup>16</sup></a> <span class="math display">\[
\mathbf{b}_{\mathrm{WLS}} = \left( \mathbf{X}^T \mathbf{W} \mathbf{X}
\right)^{-1}
  \mathbf{X}^T \mathbf{W} \mathbf{y}
\]</span></p>
<p>Fitted values are then <span class="math inline">\(\widehat{\mathbf{y}} =
\mathbf{X}\mathbf{b}_{\mathrm{WLS}}\)</span>.</p>
<p>The LOO fitted value for the <span class="math inline">\(i\)</span>th
case can be efficiently computed by <span class="math inline">\(\widehat{y}_{-i} = y_i - e_i/(1 - h_i)\)</span>
where <span class="math inline">\(h_i = \mathbf{x}^T_i \left(
\mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{x}_i\)</span>
(the so-called “hatvalue”). Here, <span class="math inline">\(\mathbf{x}^T_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span>, and <span class="math inline">\(\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th row written as a column vector. This
approach can break down when one or more hatvalues are equal to 1, in
which case the formula for <span class="math inline">\(\widehat{y}_{-i}\)</span> requires division by
0.</p>
<p>To compute cross-validated fitted values when the folds contain more
than one case, we make use of the Woodbury matrix identify <span class="citation">("Woodbury matrix identity", 2023)</span>, <span class="math display">\[
\left(\mathbf{A}_{m \times m} + \mathbf{U}_{m \times k}
\mathbf{C}_{k \times k} \mathbf{V}_{k \times m} \right)^{-1} =
\mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U} \left(\mathbf{C}^{-1} +
\mathbf{VA}^{-1}\mathbf{U} \right)^{-1} \mathbf{VA}^{-1}
\]</span> where <span class="math inline">\(\mathbf{A}\)</span> is a
nonsingular order-<span class="math inline">\(n\)</span> matrix. We
apply this result by letting <span class="math display">\[\begin{align*}
    \mathbf{A} &amp;= \mathbf{X}^T \mathbf{W} \mathbf{X} \\
    \mathbf{U} &amp;= \mathbf{X}_\mathbf{j}^T \\
    \mathbf{V} &amp;= - \mathbf{X}_\mathbf{j} \\
    \mathbf{C} &amp;= \mathbf{W}_\mathbf{j} \\
\end{align*}\]</span> where the subscript <span class="math inline">\(\mathbf{j} = (i_{j1}, \ldots, i_{jm})^T\)</span>
represents the vector of indices for the cases in the <span class="math inline">\(j\)</span>th fold, <span class="math inline">\(j =
1, \ldots, k\)</span>. The negative sign in <span class="math inline">\(\mathbf{V} = - \mathbf{X}_\mathbf{j}\)</span>
reflects the <em>removal</em>, rather than addition, of the cases in
<span class="math inline">\(\mathbf{j}\)</span>.</p>
<p>Applying the Woodbury identity isn’t quite as fast as using the
hatvalues, but it is generally much faster than refitting the model. A
disadvantage of the Woodbury identity, however, is that it entails
explicit matrix inversion and thus may be numerically unstable. The
inverse of <span class="math inline">\(\mathbf{A} = \mathbf{X}^T
\mathbf{W} \mathbf{X}\)</span> is available directly in the
<code>"lm"</code> object, but the second term on the right-hand side of
the Woodbury identity requires a matrix inversion with each fold
deleted. (In contrast, the inverse of each <span class="math inline">\(\mathbf{C} = \mathbf{W}_\mathbf{j}\)</span> is
straightforward because <span class="math inline">\(\mathbf{W}\)</span>
is diagonal.)</p>
<p>The Woodbury identity also requires that the model matrix be of full
rank. We impose that restriction in our code by removing redundant
regressors from the model matrix for all of the cases, but that doesn’t
preclude rank deficiency from surfacing when a fold is removed. Rank
deficiency of <span class="math inline">\(\mathbf{X}\)</span> doesn’t
disqualify cross-validation because all we need are fitted values under
the estimated model.</p>
<p><code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code> computes the maximum-likelihood estimates for a
generalized linear model by iterated weighted least squares <span class="citation">(see, e.g., Fox &amp; Weisberg, 2019, sec.
6.12)</span>. The last iteration is therefore just a WLS fit of the
“working response” on the model matrix using “working weights.” Both the
working weights and the working response at convergence are available
from the information in the object returned by <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code>.</p>
<p>We then treat re-estimation of the model with a case or cases deleted
as a WLS problem, using the hatvalues or the Woodbury matrix identity.
The resulting fitted values for the deleted fold aren’t exact—that is,
except for the Gaussian family, the result isn’t identical to what we
would obtain by literally refitting the model—but in our (limited)
experience, the approximation is very good, especially for LOO CV, which
is when we would be most tempted to use it. Nevertheless, because these
results are approximate, the default for the <code>"glm"</code>
<code><a href="../reference/cv.html">cv()</a></code> method is to perform the exact computation, which
entails refitting the model with each fold omitted.</p>
</div>
<div class="section level3">
<h3 id="computation-of-the-bias-corrected-cv-criterion-and-confidence-intervals">Computation of the bias-corrected CV criterion and confidence
intervals<a class="anchor" aria-label="anchor" href="#computation-of-the-bias-corrected-cv-criterion-and-confidence-intervals"></a>
</h3>
<p>Let <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}})\)</span> represent a cross-validation cost
criterion, such as mean-squared error, computed for all of the <span class="math inline">\(n\)</span> values of the response <span class="math inline">\(\mathbf{y}\)</span> based on fitted values <span class="math inline">\(\widehat{\mathbf{y}}\)</span> from the model fit
to all of the data. We require that <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}})\)</span> is the mean of casewise components, that
is, <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}) = \frac{1}{n}\sum_{i=1}^n\mathrm{cv}(y_i,
\widehat{y}_i)\)</span>.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;span class="citation"&gt;Arlot &amp;amp; Celisse
(2010)&lt;/span&gt; term the casewise loss, &lt;span class="math inline"&gt;\(\mathrm{cv}(y_i, \widehat{y}_i)\)&lt;/span&gt;, the
“contrast function.”&lt;/p&gt;'><sup>17</sup></a> For example, <span class="math inline">\(\mathrm{MSE}(\mathbf{y}, \widehat{\mathbf{y}}) =
\frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_i)^2\)</span>.</p>
<p>We divide the <span class="math inline">\(n\)</span> cases into <span class="math inline">\(k\)</span> folds of approximately <span class="math inline">\(n_j \approx n/k\)</span> cases each, where <span class="math inline">\(n = \sum n_j\)</span>. As above, let <span class="math inline">\(\mathbf{j}\)</span> denote the indices of the
cases in the <span class="math inline">\(j\)</span>th fold.</p>
<p>Now define <span class="math inline">\(\mathrm{CV}_j =
\mathrm{CV}(\mathbf{y}, \widehat{\mathbf{y}}^{(j)})\)</span>. The
superscript <span class="math inline">\((j)\)</span> on <span class="math inline">\(\widehat{\mathbf{y}}^{(j)}\)</span> represents
fitted values computed for all of the cases from the model with fold
<span class="math inline">\(j\)</span> omitted. Let <span class="math inline">\(\widehat{\mathbf{y}}^{(-i)}\)</span> represent the
vector of fitted values for all <span class="math inline">\(n\)</span>
cases where the fitted value for the <span class="math inline">\(i\)</span>th case is computed from the model fit
with the fold including the <span class="math inline">\(i\)</span>th
case omitted (i.e., fold <span class="math inline">\(j\)</span> for
which <span class="math inline">\(i \in \mathbf{j}\)</span>).</p>
<p>Then the cross-validation criterion is just <span class="math inline">\(\mathrm{CV} = \mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}^{(-i)})\)</span>. Following <span class="citation">Davison &amp; Hinkley (1997, pp. 293–295)</span>, the
bias-adjusted cross-validation criterion is <span class="math display">\[
\mathrm{CV}_{\mathrm{adj}} = \mathrm{CV} + \mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}) - \frac{1}{n} \sum_{j=1}^{k} n_j \mathrm{CV}_j
\]</span></p>
<p>We compute the standard error of CV as <span class="math display">\[
\mathrm{SE}(\mathrm{CV}) = \frac{1}{\sqrt n} \sqrt{ \frac{\sum_{i=1}^n
\left[ \mathrm{cv}(y_i, \widehat{y}_i^{(-i)} ) - \mathrm{CV} \right]^2
}{n - 1} }
\]</span> that is, as the standard deviation of the casewise components
of CV divided by the square-root of the number of cases.</p>
<p>We then use <span class="math inline">\(\mathrm{SE}(\mathrm{CV})\)</span> to construct a
<span class="math inline">\(100 \times (1 - \alpha)\)</span>% confidence
interval around the <em>adjusted</em> CV estimate of error: <span class="math display">\[
\left[ \mathrm{CV}_{\mathrm{adj}} - z_{1 -
\alpha/2}\mathrm{SE}(\mathrm{CV}), \mathrm{CV}_{\mathrm{adj}} + z_{1 -
\alpha/2}\mathrm{SE}(\mathrm{CV})  \right]
\]</span> where <span class="math inline">\(z_{1 - \alpha/2}\)</span> is
the <span class="math inline">\(1 - \alpha/2\)</span> quantile of the
standard-normal distribution (e.g, <span class="math inline">\(z \approx
1.96\)</span> for a 95% confidence interval, for which <span class="math inline">\(1 - \alpha/2 = .975\)</span>).</p>
<p><span class="citation">S. Bates et al. (2023)</span> show that the
coverage of this confidence interval is poor for small samples, and they
suggest a much more computationally intensive procedure, called
<em>nested cross-validation</em>, to compute better estimates of error
and confidence intervals with better coverage for small samples. We may
implement Bates et al.’s approach in a later release of the
<strong>cv</strong> package. At present we use the confidence interval
above for sufficiently large <span class="math inline">\(n\)</span>,
which, based on Bates et al.’s results, we take by default to be <span class="math inline">\(n \ge 400\)</span>.</p>
</div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-ArlotCelisse:2010" class="csl-entry">
Arlot, S., &amp; Celisse, A. (2010). <span class="nocase">A survey of
cross-validation procedures for model selection</span>. <em>Statistics
Surveys</em>, <em>4</em>, 40–79. Retrieved from <a href="https://doi.org/10.1214/09-SS054" class="external-link">https://doi.org/10.1214/09-SS054</a>
</div>
<div id="ref-BatesEtAl:2015" class="csl-entry">
Bates, D., Mächler, M., Bolker, B., &amp; Walker, S. (2015). Fitting
linear mixed-effects models using <span class="nocase">lme4</span>.
<em>Journal of Statistical Software</em>, <em>67</em>(1), 1–48.
</div>
<div id="ref-BatesHastieTibshirani:2023" class="csl-entry">
Bates, S., Hastie, T., &amp; Tibshirani, R. (2023). Cross-validation:
What does it estimate and how well does it do it? <em>Journal of the
American Statistical Association</em>, <em>in press</em>. Retrieved from
<a href="https://doi.org/10.1080/01621459.2023.2197686" class="external-link">https://doi.org/10.1080/01621459.2023.2197686</a>
</div>
<div id="ref-BoxCox:1964" class="csl-entry">
Box, G. E. P., &amp; Cox, D. R. (1964). An analysis of transformations.
<em>Journal of the Royal Statistical Society, Series
<span>B</span></em>, <em>26</em>, 211–252.
</div>
<div id="ref-CantyRipley2022" class="csl-entry">
Canty, A., &amp; Ripley, B. D. (2022). <em>Boot: Bootstrap
<span>R</span> (<span>S</span>-plus) functions</em>.
</div>
<div id="ref-DavisonHinkley:1997" class="csl-entry">
Davison, A. C., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and
their applications</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-DiggleLiangZeger:1994" class="csl-entry">
Diggle, P. J., Liang, K.-Y., &amp; Zeger, S. L. (1994). <em>Analysis of
longitudinal data</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Fox:2016" class="csl-entry">
Fox, J. (2016). <em>Applied regression analysis and generalized linear
models</em> (Second edition). Thousand Oaks <span>CA</span>: Sage.
</div>
<div id="ref-FoxWeisberg:2019" class="csl-entry">
Fox, J., &amp; Weisberg, S. (2019). <em>An <span>R</span> companion to
applied regression</em> (Third edition). Thousand Oaks <span>CA</span>:
Sage.
</div>
<div id="ref-Harrell:2015" class="csl-entry">
Harrell, F., Jr. (2015). <em>Regression modeling strategies</em> (Second
edition). New York: Springer.
</div>
<div id="ref-HastieTibshiraniFriedman:2009" class="csl-entry">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements
of statistical learning: Data mining, inference, and prediction</em>
(Second edition). New York: Springer. Retrieved from <a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf" class="external-link">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</div>
<div id="ref-JamesEtAl:2021" class="csl-entry">
James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). <em>An
introduction to statistical learning with applications in
<span>R</span></em> (Second edition). New York: Springer.
</div>
<div id="ref-Mersmann:2023" class="csl-entry">
Mersmann, O. (2023). <em>Microbenchmark: Accurate timing functions</em>.
Retrieved from <a href="https://CRAN.R-project.org/package=microbenchmark" class="external-link">https://CRAN.R-project.org/package=microbenchmark</a>
</div>
<div id="ref-PinheiroBates:2000" class="csl-entry">
Pinheiro, J. C., &amp; Bates, D. M. (2000). <em>Mixed-effects models in
<span>S</span> and <span>S-PLUS</span></em>. New York: Springer.
</div>
<div id="ref-RaudenbushBryk:2002" class="csl-entry">
Raudenbush, S. W., &amp; Bryk, A. S. (2002). <em>Hierarchical linear
models: Applications and data analysis methods</em> (Second edition).
Thousand Oaks <span>CA</span>: Sage.
</div>
<div id="ref-Sarkar:2008" class="csl-entry">
Sarkar, D. (2008). <em>Lattice: Multivariate data visualization with
<span>R</span></em>. New York: Springer. Retrieved from <a href="http://lmdvr.r-forge.r-project.org" class="external-link">http://lmdvr.r-forge.r-project.org</a>
</div>
<div id="ref-SarkarAndrews:2022" class="csl-entry">
Sarkar, D., &amp; Andrews, F. (2022). <em><span class="nocase">latticeExtra</span>: Extra graphical utilities based on
<span class="nocase">lattice</span></em>. Retrieved from <a href="https://CRAN.R-project.org/package=latticeExtra" class="external-link">https://CRAN.R-project.org/package=latticeExtra</a>
</div>
<div id="ref-Stata:2023" class="csl-entry">
StataCorp LLC. (2023). <em>Stata multilevel mixed-effects reference
manual, release 18</em>. College Station <span>TX</span>: Stata Press.
Retrieved from <a href="https://www.stata.com/manuals/me.pdf" class="external-link">https://www.stata.com/manuals/me.pdf</a>
</div>
<div id="ref-Vehtari:2023" class="csl-entry">
Vehtari, A. (2023). Cross-validation <span>FAQ</span>. Retrieved October
15, 2023, from <a href="https://users.aalto.fi/~ave/CV-FAQ.html" class="external-link">https://users.aalto.fi/~ave/CV-FAQ.html</a>
</div>
<div id="ref-VenablesRipley:2002" class="csl-entry">
Venables, W. N., &amp; Ripley, B. D. (2002). <em>Modern applied
statistics with <span>S</span></em> (Fourth edition). New York:
Springer.
</div>
<div id="ref-Weisberg:2014" class="csl-entry">
Weisberg, S. (2014). <em>Applied linear regression</em> (Second
edition). Hoboken <span>NJ</span>: Wiley.
</div>
<div id="ref-WickhamEtAl:2023" class="csl-entry">
Wickham, H., François, R., Henry, L., Müller, K., &amp; Vaughan, D.
(2023). <em>Dplyr: A grammar of data manipulation</em>. Retrieved from
<a href="https://CRAN.R-project.org/package=dplyr" class="external-link">https://CRAN.R-project.org/package=dplyr</a>
</div>
<div id="ref-Wikipedia-Woodbury:2023" class="csl-entry">
"Woodbury matrix identity". (2023). Woodbury matrix
identity—<span>W</span>ikipedia<span>,</span> the free encyclopedia.
Retrieved from <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity" class="external-link">https://en.wikipedia.org/wiki/Woodbury_matrix_identity</a>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by John Fox, Georges Monette.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
