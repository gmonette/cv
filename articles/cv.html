<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Cross-validating regression models • cv</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Cross-validating regression models">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">cv</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">2.0.3</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/cv.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/cv-extend.html">Extending the cv package</a></li>
    <li><a class="dropdown-item" href="../articles/cv-mixed.html">Cross-validating mixed-effects models</a></li>
    <li><a class="dropdown-item" href="../articles/cv-notes.html">Computational and technical notes on cross-validating regression models</a></li>
    <li><a class="dropdown-item" href="../articles/cv-selection.html">Cross-validating model selection</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/gmonette/cv/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Cross-validating regression models</h1>
            <h3 data-toc-skip class="subtitle">Introduction to the cv
package</h3>
                        <h4 data-toc-skip class="author">John Fox and
Georges Monette</h4>
            
            <h4 data-toc-skip class="date">2024-10-16</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/gmonette/cv/blob/main/vignettes/cv.Rmd" class="external-link"><code>vignettes/cv.Rmd</code></a></small>
      <div class="d-none name"><code>cv.Rmd</code></div>
    </div>

    
    
<p>This vignette covers the basics of using the <strong>cv</strong>
package for cross-validation. The first, and major, section of the
vignette consists of examples that fit linear and generalized linear
models to data sets with independently sampled cases. Brief sections
follow on replicating cross-validation, manipulating the objects
produced by <code><a href="../reference/cv.html">cv()</a></code> and related functions, and employing
parallel computations.</p>
<p>There are several other vignettes associated with the
<strong>cv</strong> package: on cross-validating mixed-effects models;
on cross-validating model-selection procedures; and on technical
details, such as computational procedures.</p>
<div class="section level2">
<h2 id="cross-validation">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h2>
<p>Cross-validation (CV) is an essentially simple and intuitively
reasonable approach to estimating the predictive accuracy of regression
models. CV is developed in many standard sources on regression modeling
and “machine learning”—we particularly recommend <span class="citation">James, Witten, Hastie, &amp; Tibshirani (2021, secs.
5.1, 5.3)</span>—and so we will describe the method only briefly here
before taking up computational issues and some examples. See <span class="citation">Arlot &amp; Celisse (2010)</span> for a wide-ranging,
if technical, survey of cross-validation and related methods that
emphasizes the statistical properties of CV.</p>
<p>Validating research by replication on independently collected data is
a common scientific norm. Emulating this process in a single study by
data-division is less common: The data are randomly divided into two,
possibly equal-size, parts; the first part is used to develop and fit a
statistical model; and then the second part is used to assess the
adequacy of the model fit to the first part of the data. Data-division,
however, suffers from two problems: (1) Dividing the data decreases the
sample size and thus increases sampling error; and (2), even more
disconcertingly, particularly in smaller samples, the results can vary
substantially based on the random division of the data: See <span class="citation">Harrell (2015, sec. 5.3)</span> for this and other
remarks about data-division and cross-validation.</p>
<p>Cross-validation speaks to both of these issues. In CV, the data are
randomly divided as equally as possible into several, say
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>,
parts, called “folds.” The statistical model is fit
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
times, leaving each fold out in turn. Each fitted model is then used to
predict the response variable for the cases in the omitted fold. A CV
criterion or “cost” measure, such as the mean-squared error (“MSE”) of
prediction, is then computed using these predicted values. In the
extreme
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k = n</annotation></semantics></math>,
the number of cases in the data, thus omitting individual cases and
refitting the model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
times—a procedure termed “leave-one-out (LOO) cross-validation.”</p>
<p>Because the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
models are each fit to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n - 1</annotation></semantics></math>
cases, LOO CV produces a nearly unbiased estimate of prediction error.
The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
regression models are highly statistically dependent, however, based as
they are on nearly the same data, and so the resulting estimate of
prediction error has larger variance than if the predictions from the
models fit to the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
data sets were independent.</p>
<p>Because predictions are based on smaller data sets, each of size
approximately
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mi>n</mi><mi>/</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">n - n/k</annotation></semantics></math>,
estimated prediction error for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">k = 5</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>
(commonly employed choices) is more biased than estimated prediction
error for LOO CV. It is possible, however, to correct
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV for bias (see below).</p>
<p>The relative <em>variance</em> of prediction error for LOO CV and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV (with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&lt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k &lt; n</annotation></semantics></math>)
is more complicated: Because the overlap between the data sets with each
fold omitted is smaller for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV than for LOO CV, the dependencies among the predictions are smaller
for the former than for the latter, tending to produce smaller variance
in prediction error for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV. In contrast, there are factors that tend to inflate the variance of
prediction error in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV, including the reduced size of the data sets with each fold omitted
and the randomness induced by the selection of folds—in LOO CV the folds
are not random.</p>
</div>
<div class="section level2">
<h2 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h2>
<div class="section level3">
<h3 id="polynomial-regression-for-the-auto-data">Polynomial regression for the <code>Auto</code> data<a class="anchor" aria-label="anchor" href="#polynomial-regression-for-the-auto-data"></a>
</h3>
<p>The data for this example are drawn from the <strong>ISLR2</strong>
package for R, associated with <span class="citation">James et al.
(2021)</span>. The presentation here is close (though not identical) to
that in the original source <span class="citation">(James et al., 2021,
secs. 5.1, 5.3)</span>, and it demonstrates the use of the
<code><a href="../reference/cv.html">cv()</a></code> function in the <strong>cv</strong> package.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;span class="citation"&gt;James et al. (2021)&lt;/span&gt; use
the &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv.glm()&lt;/a&gt;&lt;/code&gt; function in the &lt;strong&gt;boot&lt;/strong&gt; package
&lt;span class="citation"&gt;(Canty &amp;amp; Ripley, 2022; Davison &amp;amp; Hinkley,
1997)&lt;/span&gt;. Despite its name, &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv.glm()&lt;/a&gt;&lt;/code&gt; is an independent
function and not a method of a &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; generic function.&lt;/p&gt;'><sup>1</sup></a></p>
<p>The <code>Auto</code> dataset contains information about 392
cars:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Auto"</span>, package<span class="op">=</span><span class="st">"ISLR2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mpg cylinders displacement horsepower weight acceleration year origin</span></span>
<span><span class="co">#&gt; 1  18         8          307        130   3504         12.0   70      1</span></span>
<span><span class="co">#&gt; 2  15         8          350        165   3693         11.5   70      1</span></span>
<span><span class="co">#&gt; 3  18         8          318        150   3436         11.0   70      1</span></span>
<span><span class="co">#&gt; 4  16         8          304        150   3433         12.0   70      1</span></span>
<span><span class="co">#&gt; 5  17         8          302        140   3449         10.5   70      1</span></span>
<span><span class="co">#&gt; 6  15         8          429        198   4341         10.0   70      1</span></span>
<span><span class="co">#&gt;                        name</span></span>
<span><span class="co">#&gt; 1 chevrolet chevelle malibu</span></span>
<span><span class="co">#&gt; 2         buick skylark 320</span></span>
<span><span class="co">#&gt; 3        plymouth satellite</span></span>
<span><span class="co">#&gt; 4             amc rebel sst</span></span>
<span><span class="co">#&gt; 5               ford torino</span></span>
<span><span class="co">#&gt; 6          ford galaxie 500</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 392   9</span></span></code></pre></div>
<p>With the exception of <code>origin</code> (which we don’t use here),
these variables are largely self-explanatory, except possibly for units
of measurement: for details see
<code><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html" class="external-link">help("Auto", package="ISLR2")</a></code>.</p>
<p>We’ll focus here on the relationship of <code>mpg</code> (miles per
gallon) to <code>horsepower</code>, as displayed in the following
scatterplot:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fmpg-horsepower-scatterplot-1.png" alt="`mpg` vs `horsepower` for the `Auto` data" width="100%"><p class="caption">
<code>mpg</code> vs <code>horsepower</code> for the <code>Auto</code>
data
</p>
</div>
<p>The relationship between the two variables is monotone, decreasing,
and nonlinear. Following <span class="citation">James et al.
(2021)</span>, we’ll consider approximating the relationship by a
polynomial regression, with the degree of the polynomial
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
ranging from 1 (a linear regression) to 10.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Although it serves to illustrate the use of CV, a
polynomial is probably not the best choice here. Consider, for example
the scatterplot for log-transformed &lt;code&gt;mpg&lt;/code&gt; and
&lt;code&gt;horsepower&lt;/code&gt;, produced by
&lt;code&gt;plot(mpg ~ horsepower, data=Auto, log="xy")&lt;/code&gt; (execution of
which is left to the reader).&lt;/p&gt;'><sup>2</sup></a> Polynomial fits for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p = 1</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>
are shown in the following figure:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="va">horsepower</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Auto</span>,</span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span>,</span>
<span>                       length <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">p</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span>  <span class="va">mpg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">m</span>, newdata <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>horsepower <span class="op">=</span> <span class="va">horsepower</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="va">horsepower</span>,</span>
<span>        <span class="va">mpg</span>,</span>
<span>        col <span class="op">=</span> <span class="va">p</span> <span class="op">+</span> <span class="fl">1</span>,</span>
<span>        lty <span class="op">=</span> <span class="va">p</span>,</span>
<span>        lwd <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topright"</span>,</span>
<span>  legend <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">6</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  title <span class="op">=</span> <span class="st">"Degree"</span>,</span>
<span>  inset <span class="op">=</span> <span class="fl">0.02</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fmpg-horsepower-scatterplot-polynomials-1.png" alt="`mpg` vs `horsepower` for the `Auto` data" width="100%"><p class="caption">
<code>mpg</code> vs <code>horsepower</code> for the <code>Auto</code>
data
</p>
</div>
<p>The linear fit is clearly inappropriate; the fits for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">p = 2</annotation></semantics></math>
(quadratic) through
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math>
are very similar; and the fit for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">p = 5</annotation></semantics></math>
may over-fit the data by chasing one or two relatively high
<code>mpg</code> values at the right (but see the CV results reported
below).</p>
<p>The following graph shows two measures of estimated (squared) error
as a function of polynomial-regression degree: The mean-squared error
(“MSE”), defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝖬</mi><mi>𝖲</mi><mi>𝖤</mi></mrow><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\mathsf{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_i)^2</annotation></semantics></math>,
and the usual residual variance, defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mn>2</mn></msup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mo>−</mo><mi>p</mi><mo>−</mo><mn>1</mn></mrow></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover><mi>y</mi><mo accent="true">̂</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\widehat{\sigma}^2 = \frac{1}{n - p - 1} \sum_{i=1}^n (y_i - \widehat{y}_i)^2</annotation></semantics></math>.
The former necessarily declines with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
(or, more strictly, can’t increase with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>),
while the latter gets slightly larger for the largest values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>,
with the “best” value, by a small margin, for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>7</mn></mrow><annotation encoding="application/x-tex">p = 7</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://gmonette.github.io/cv/">"cv"</a></span><span class="op">)</span> <span class="co"># for mse() and other functions</span></span>
<span><span class="co">#&gt; Loading required package: doParallel</span></span>
<span><span class="co">#&gt; Loading required package: foreach</span></span>
<span><span class="co">#&gt; Loading required package: iterators</span></span>
<span><span class="co">#&gt; Loading required package: parallel</span></span>
<span></span>
<span><span class="va">var</span> <span class="op">&lt;-</span> <span class="va">mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">p</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span>  <span class="va">mse</span><span class="op">[</span><span class="va">p</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">var</span><span class="op">[</span><span class="va">p</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span> <span class="op">^</span> <span class="fl">2</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/range.html" class="external-link">range</a></span><span class="op">(</span><span class="va">mse</span>, <span class="va">var</span><span class="op">)</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"n"</span>,</span>
<span>  xlab <span class="op">=</span> <span class="st">"Degree of polynomial, p"</span>,</span>
<span>  ylab <span class="op">=</span> <span class="st">"Estimated Squared Error"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span></span>
<span>  <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  <span class="va">mse</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"b"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span></span>
<span>  <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  <span class="va">var</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  pch <span class="op">=</span> <span class="fl">17</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"b"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topright"</span>,</span>
<span>  inset <span class="op">=</span> <span class="fl">0.02</span>,</span>
<span>  legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hat</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span> <span class="op">^</span> <span class="fl">2</span><span class="op">)</span>, <span class="st">"MSE"</span><span class="op">)</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">1</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">3</span><span class="op">:</span><span class="fl">2</span>,</span>
<span>  pch <span class="op">=</span> <span class="fl">17</span><span class="op">:</span><span class="fl">16</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fmpg-horsepower-MSE-se-1.png" alt="Estimated squared error as a function of polynomial degree, $p$" width="100%"><p class="caption">
Estimated squared error as a function of polynomial degree,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math></p>
</div>
<p>The code for this graph uses the <code><a href="../reference/cost-functions.html">mse()</a></code> function from the
<strong>cv</strong> package to compute the MSE for each fit.</p>
<div class="section level4">
<h4 id="using-cv">Using <code>cv()</code><a class="anchor" aria-label="anchor" href="#using-cv"></a>
</h4>
<p>The generic <code><a href="../reference/cv.html">cv()</a></code> function has an <code>"lm"</code>
method, which by default performs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">k = 10</annotation></semantics></math>-fold
CV:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 2), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 841111</span></span>
<span><span class="co">#&gt; cross-validation criterion (mse) = 19.25</span></span></code></pre></div>
<p>The <code>"lm"</code> method by default uses <code><a href="../reference/cost-functions.html">mse()</a></code> as
the CV criterion and the Woodbury matrix identity <span class="citation">(Hager, 1989)</span> to update the regression with each
fold deleted without having literally to refit the model. (Computational
details are discussed in a separate vignette.) The <code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code>
method for <code>"cv"</code> objects simply print the cross-validation
criterion, here the MSE. We can use <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> to obtain
more information (as we’ll do routinely in the sequel):</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">cv.m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 411860</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.145</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.136</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>The summary reports the CV estimate of MSE, a biased-adjusted
estimate of the MSE (the bias adjustment is explained in the final
section), and the MSE is also computed for the original, full-sample
regression. Because the division of the data into 10 folds is random,
<code><a href="../reference/cv.html">cv()</a></code> explicitly (randomly) generates and saves a seed for
R’s pseudo-random number generator, to make the results replicable. The
user can also specify the seed directly via the <code>seed</code>
argument to <code><a href="../reference/cv.html">cv()</a></code>.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method for <code>"cv"</code> objects graphs
the CV criterion (here MSE) by fold or the coefficients estimates with
each fold deleted:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.m.auto</span><span class="op">)</span> <span class="co"># CV criterion</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fplot.cv.crit-1.png" alt="CV criterion (MSE) for cases in each fold." width="90%"><p class="caption">
CV criterion (MSE) for cases in each fold.
</p>
</div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.m.auto</span>, what<span class="op">=</span><span class="st">"coefficients"</span><span class="op">)</span> <span class="co"># coefficient estimates</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fplot.cv.coefs-1.png" alt="Regression coefficients with each fold omitted." width="90%"><p class="caption">
Regression coefficients with each fold omitted.
</p>
</div>
<p>To perform LOO CV, we can set the <code>k</code> argument to
<code><a href="../reference/cv.html">cv()</a></code> to the number of cases in the data, here
<code>k=392</code>, or, more conveniently, to <code>k="loo"</code> or
<code>k="n"</code>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span></code></pre></div>
<p>For LOO CV of a linear model, <code><a href="../reference/cv.html">cv()</a></code> by default uses the
hatvalues from the model fit to the full data for the LOO updates, and
reports only the CV estimate of MSE. Alternative methods are to use the
Woodbury matrix identity or the “naive” approach of literally refitting
the model with each case omitted. All three methods produce exact
results for a linear model (within the precision of floating-point
computations):</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span>, method <span class="op">=</span> <span class="st">"naive"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: naive</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span>, method <span class="op">=</span> <span class="st">"Woodbury"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>The <code>"naive"</code> and <code>"Woodbury"</code> methods also
return the bias-adjusted estimate of MSE and the full-sample MSE, but
bias isn’t an issue for LOO CV.</p>
</div>
<div class="section level4">
<h4 id="comparing-competing-models">Comparing competing models<a class="anchor" aria-label="anchor" href="#comparing-competing-models"></a>
</h4>
<p>The <code><a href="../reference/cv.html">cv()</a></code> function also has a method that can be applied
to a list of regression models for the same data, composed using the
<code><a href="../reference/cv.modList.html">models()</a></code> function. For
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
CV, the same folds are used for the competing models, which reduces
random error in their comparison. This result can also be obtained by
specifying a common seed for R’s random-number generator while applying
<code><a href="../reference/cv.html">cv()</a></code> separately to each model, but employing a list of
models is more convenient for both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-fold
and LOO CV (where there is no random component to the composition of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
folds).</p>
<p>We illustrate with the polynomial regression models of varying degree
for the <code>Auto</code> data (discussed previously), beginning by
fitting and saving the 10 models:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">command</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"m."</span>, <span class="va">p</span>, <span class="st">"&lt;- lm(mpg ~ poly(horsepower, "</span>, <span class="va">p</span>,</span>
<span>                    <span class="st">"), data=Auto)"</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/eval.html" class="external-link">eval</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/parse.html" class="external-link">parse</a></span><span class="op">(</span>text <span class="op">=</span> <span class="va">command</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/ls.html" class="external-link">objects</a></span><span class="op">(</span>pattern <span class="op">=</span> <span class="st">"m\\.[0-9]"</span><span class="op">)</span></span>
<span><span class="co">#&gt;  [1] "m.1"  "m.10" "m.2"  "m.3"  "m.4"  "m.5"  "m.6"  "m.7"  "m.8"  "m.9"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.2</span><span class="op">)</span> <span class="co"># for example, the quadratic fit</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 2), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span></code></pre></div>
<p>The convoluted code within the loop to produce the 10 models insures
that the model formulas are of the form, e.g.,
<code>mpg ~ poly(horsepower, 2)</code> rather than
<code>mpg ~ poly(horsepower, p)</code>, which will be useful to us in a
separate vignette, where we consider cross-validating the
model-selection process.</p>
<p>We then apply <code><a href="../reference/cv.html">cv()</a></code> to the list of 10 models (the
<code>data</code> argument is required):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># 10-fold CV</span></span>
<span><span class="va">cv.auto.10</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/cv.modList.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>         <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Auto</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">2120</span></span>
<span><span class="op">)</span></span>
<span><span class="va">cv.auto.10</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span> <span class="co"># for the linear and quadratic models</span></span>
<span><span class="co">#&gt; Model model.1:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.246 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.2:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.346</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">cv.auto.10</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.1:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.246</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 24.23</span></span>
<span><span class="co">#&gt; full-sample criterion = 23.944</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.2:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.346</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.327</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.3:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.396</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.373</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.945</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.4:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.444</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.414</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.876</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.5:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.014</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 18.983</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.427</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.6:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.954</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 18.916</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.241</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.7:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.898</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 18.854</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.078</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.8:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.126</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.068</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.066</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.9:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.342</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.269</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.027</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.10:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; cross-validation criterion = 20.012</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.882</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.01</span></span>
<span></span>
<span><span class="co"># LOO CV</span></span>
<span><span class="va">cv.auto.loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.modList.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>                         <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>                  data <span class="op">=</span> <span class="va">Auto</span>,</span>
<span>                  k <span class="op">=</span> <span class="st">"loo"</span><span class="op">)</span></span>
<span><span class="va">cv.auto.loo</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span> <span class="co"># linear and quadratic models</span></span>
<span><span class="co">#&gt; Model model.1:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.232 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.2:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">cv.auto.loo</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.1:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.232</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.2:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.3:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.335</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.4:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.424</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.5:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.033</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.6:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.979</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.7:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.833</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.8:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.961</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.9:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.069</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.10:</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.491</span></span></code></pre></div>
<p>Because we didn’t supply names for the models in the calls to the
<code><a href="../reference/cv.modList.html">models()</a></code> function, the names <code>model.1</code>,
<code>model.2</code>, etc., are generated by the function.</p>
<p>Finally, we extract and graph the adjusted MSEs for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>-fold
CV and the MSEs for LOO CV (see the section below on manipulating
<code>"cv"</code> objects:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.mse.10</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">cv.auto.10</span>, </span>
<span>                           rows<span class="op">=</span><span class="st">"cv"</span>,             </span>
<span>                           columns<span class="op">=</span><span class="st">"criteria"</span></span>
<span>                           <span class="op">)</span><span class="op">$</span><span class="va">adjusted.criterion</span></span>
<span><span class="va">cv.mse.loo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">cv.auto.loo</span>, </span>
<span>                           rows<span class="op">=</span><span class="st">"cv"</span>,             </span>
<span>                           columns<span class="op">=</span><span class="st">"criteria"</span></span>
<span>                           <span class="op">)</span><span class="op">$</span><span class="va">criterion</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span>,</span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/range.html" class="external-link">range</a></span><span class="op">(</span><span class="va">cv.mse.10</span>, <span class="va">cv.mse.loo</span><span class="op">)</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"n"</span>,</span>
<span>  xlab <span class="op">=</span> <span class="st">"Degree of polynomial, p"</span>,</span>
<span>  ylab <span class="op">=</span> <span class="st">"Cross-Validated MSE"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span></span>
<span>  <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  <span class="va">cv.mse.10</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">1</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  pch <span class="op">=</span> <span class="fl">16</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"b"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span></span>
<span>  <span class="fl">1</span><span class="op">:</span><span class="fl">10</span>,</span>
<span>  <span class="va">cv.mse.loo</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">3</span>,</span>
<span>  pch <span class="op">=</span> <span class="fl">17</span>,</span>
<span>  type <span class="op">=</span> <span class="st">"b"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span></span>
<span>  <span class="st">"topright"</span>,</span>
<span>  inset <span class="op">=</span> <span class="fl">0.02</span>,</span>
<span>  legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"10-Fold CV"</span>, <span class="st">"LOO CV"</span><span class="op">)</span>,</span>
<span>  lwd <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  lty <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">1</span>,</span>
<span>  col <span class="op">=</span> <span class="fl">3</span><span class="op">:</span><span class="fl">2</span>,</span>
<span>  pch <span class="op">=</span> <span class="fl">17</span><span class="op">:</span><span class="fl">16</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fpolynomial-regression-CV-graph-1.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$" width="100%"><p class="caption">
Cross-validated 10-fold and LOO MSE as a function of polynomial degree,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math></p>
</div>
<p>Alternatively, we can use the <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method for
<code>"cvModList"</code> objects to compare the models, though with
separate graphs for 10-fold and LOO CV:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.auto.10</span>, main<span class="op">=</span><span class="st">"Polynomial Regressions, 10-Fold CV"</span>,</span>
<span>     axis.args<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>labels<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, xlab<span class="op">=</span><span class="st">"Degree of Polynomial, p"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.auto.loo</span>, main<span class="op">=</span><span class="st">"Polynomial Regressions, LOO CV"</span>,</span>
<span>     axis.args<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>labels<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span>, xlab<span class="op">=</span><span class="st">"Degree of Polynomial, p"</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fpolynomial-regression-CV-graph-2-1.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$" width="45%"><img src="fig%2Fpolynomial-regression-CV-graph-2-2.png" alt="Cross-validated 10-fold and LOO MSE as a function of polynomial degree, $p$" width="45%"><p class="caption">
Cross-validated 10-fold and LOO MSE as a function of polynomial degree,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math></p>
</div>
<p>In this example, 10-fold and LOO CV produce generally similar
results, and also results that are similar to those produced by the
estimated error variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mn>2</mn></msup><annotation encoding="application/x-tex">\widehat{\sigma}^2</annotation></semantics></math>
for each model, reported above (except for the highest-degree
polynomials, where the CV results more clearly suggest
over-fitting).</p>
</div>
</div>
<div class="section level3">
<h3 id="logistic-regression-for-the-mroz-data">Logistic regression for the <code>Mroz</code> data<a class="anchor" aria-label="anchor" href="#logistic-regression-for-the-mroz-data"></a>
</h3>
<p>The <code>Mroz</code> data set from the <strong>carData</strong>
package <span class="citation">(associated with Fox &amp; Weisberg,
2019)</span> has been used by several authors to illustrate binary
logistic regression; see, in particular <span class="citation">Fox &amp;
Weisberg (2019)</span>. The data were originally drawn from the U.S.
Panel Study of Income Dynamics and pertain to married women. Here are a
few cases in the data set:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Mroz"</span>, package <span class="op">=</span> <span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   lfp k5 k618 age wc hc    lwg   inc</span></span>
<span><span class="co">#&gt; 1 yes  1    0  32 no no 1.2102 10.91</span></span>
<span><span class="co">#&gt; 2 yes  0    2  30 no no 0.3285 19.50</span></span>
<span><span class="co">#&gt; 3 yes  1    3  35 no no 1.5141 12.04</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;     lfp k5 k618 age wc hc     lwg    inc</span></span>
<span><span class="co">#&gt; 751  no  0    0  43 no no 0.88814  9.952</span></span>
<span><span class="co">#&gt; 752  no  0    0  60 no no 1.22497 24.984</span></span>
<span><span class="co">#&gt; 753  no  0    3  39 no no 0.85321 28.363</span></span></code></pre></div>
<p>The response variable in the logistic regression is <code>lfp</code>,
labor-force participation, a factor coded <code>"yes"</code> or
<code>"no"</code>. The remaining variables are predictors:</p>
<ul>
<li>
<code>k5</code>, number of children 5 years old of younger in the
woman’s household;</li>
<li>
<code>k618</code>, number of children between 6 and 18 years
old;</li>
<li>
<code>age</code>, in years;</li>
<li>
<code>wc</code>, wife’s college attendance, <code>"yes"</code> or
<code>"no"</code>;</li>
<li>
<code>hc</code>, husband’s college attendance;</li>
<li>
<code>lwg</code>, the woman’s log wage rate if she is employed, or
her <em>imputed</em> wage rate, if she is not <span class="citation">(a
variable that Fox &amp; Weisberg, 2019 show is problematically
defined)</span>; and</li>
<li>
<code>inc</code>, family income, in $1000s, exclusive of wife’s
income.</li>
</ul>
<p>We use the <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code> function to fit a binary logistic
regression to the <code>Mroz</code> data:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Mroz</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 921.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">BayesRule</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">$</span><span class="va">lfp</span> <span class="op">==</span> <span class="st">"yes"</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>,</span>
<span>          <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.mroz</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.30677</span></span>
<span><span class="co">#&gt; attr(,"casewise loss")</span></span>
<span><span class="co">#&gt; [1] "y != round(yhat)"</span></span></code></pre></div>
<p>In addition to the usually summary output for a GLM, we show the
result of applying the <code><a href="../reference/cost-functions.html">BayesRule()</a></code> function from the
<strong>cv</strong> package to predictions derived from the fitted
model. Bayes rule, which predicts a “success” in a binary regression
model when the fitted probability of success [i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi><mo>=</mo><mo>Pr</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\phi = \Pr(y = 1)</annotation></semantics></math>]
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ϕ</mi><mo accent="true">̂</mo></mover><mo>≥</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\widehat{\phi} \ge .5</annotation></semantics></math>
and a “failure” if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ϕ</mi><mo accent="true">̂</mo></mover><mo>&lt;</mo><mn>.5</mn></mrow><annotation encoding="application/x-tex">\widehat{\phi} \lt .5</annotation></semantics></math>.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;code&gt;&lt;a href="../reference/cost-functions.html"&gt;BayesRule()&lt;/a&gt;&lt;/code&gt; does some error checking;
&lt;code&gt;&lt;a href="../reference/cost-functions.html"&gt;BayesRule2()&lt;/a&gt;&lt;/code&gt; is similar, but omits the error checking, and
so can be faster for large problems.&lt;/p&gt;'><sup>3</sup></a> The first
argument to <code><a href="../reference/cost-functions.html">BayesRule()</a></code> is the binary {0, 1} response, and
the second argument is the predicted probability of success.
<code><a href="../reference/cost-functions.html">BayesRule()</a></code> returns the proportion of predictions that are
<em>in error</em>, as appropriate for a “cost” function.</p>
<p>The value returned by <code><a href="../reference/cost-functions.html">BayesRule()</a></code> is associated with an
“attribute” named <code>"casewise loss"</code> and set to
<code>"y != round(yhat)"</code>, signifying that the Bayes rule CV
criterion is computed as the mean of casewise values, here 0 if the
prediction for a case matches the observed value and 1 if it does not
(signifying a prediction error). The <code><a href="../reference/cost-functions.html">mse()</a></code> function for
numeric responses is also calculated as a casewise average. Some other
criteria, such as the median absolute error, computed by the
<code><a href="../reference/cost-functions.html">medAbsErr()</a></code> function in the <strong>cv</strong> package,
aren’t averages of casewise components. The distinction is important
because, to our knowledge, the statistical theory of cross-validation,
for example, in <span class="citation">Davison &amp; Hinkley
(1997)</span>, <span class="citation">Bates, Hastie, &amp; Tibshirani
(2023)</span>, and <span class="citation">Arlot &amp; Celisse
(2010)</span>, is developed for CV criteria like MSE that are means of
casewise components. As a consequence, we limit computation of bias
adjustment and confidence intervals (see below) to criteria that are
casewise averages.</p>
<p>In this example, the fitted logistic regression incorrectly predicts
31% of the responses; we expect this estimate to be optimistic given
that the model is used to “predict” the data to which it is fit.</p>
<p>The <code>"glm"</code> method for <code><a href="../reference/cv.html">cv()</a></code> is largely
similar to the <code>"lm"</code> method, although the default algorithm,
selected explicitly by <code>method="exact"</code>, refits the model
with each fold removed (and is thus equivalent to
<code>method="naive"</code> for <code>"lm"</code> models). For
generalized linear models, <code>method="Woodbury"</code> or (for LOO
CV) <code>method="hatvalues"</code> provide approximate results (see the
computational and technical vignette for details):</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, criterion <span class="op">=</span> <span class="va">BayesRule</span>, seed <span class="op">=</span> <span class="fl">248</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 248</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: exact</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31952</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28607, 0.35297)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>,</span>
<span>   criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>   seed <span class="op">=</span> <span class="fl">248</span>,</span>
<span>   method <span class="op">=</span> <span class="st">"Woodbury"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 248</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31926</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28581, 0.35271)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<p>To ensure that the two methods use the same 10 folds, we specify the
seed for R’s random-number generator explicitly; here, and as is common
in our experience, the <code>"exact"</code> and <code>"Woodbury"</code>
algorithms produce nearly identical results. The CV estimates of
prediction error are slightly higher than the estimate based on all of
the cases.</p>
<p>The printed output includes a 95% confidence interval for the
bias-adjusted Bayes rule CV criterion. <span class="citation">Bates et
al. (2023)</span> show that these confidence intervals are unreliable
for models fit to small samples, and by default <code><a href="../reference/cv.html">cv()</a></code>
computes them only when the sample size is 400 or larger and when the CV
criterion employed is an average of casewise components, as is the case
for Bayes rule. See the final section of the vignette for details of the
computation of confidence intervals for bias-adjusted CV criteria.</p>
<p>Here are results of applying LOO CV to the Mroz model, using both the
exact and the approximate methods:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k <span class="op">=</span> <span class="st">"loo"</span>, criterion <span class="op">=</span> <span class="va">BayesRule</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: exact</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>,</span>
<span>   k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>   criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>   method <span class="op">=</span> <span class="st">"Woodbury"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>,</span>
<span>   k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>   criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>   method <span class="op">=</span> <span class="st">"hatvalues"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span></code></pre></div>
<p>To the number of decimal digits shown, the three methods produce
identical results for this example.</p>
</div>
</div>
<div class="section level2">
<h2 id="replicating-cross-validation">Replicating cross-validation<a class="anchor" aria-label="anchor" href="#replicating-cross-validation"></a>
</h2>
<p>Assuming that the number of cases
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
is a multiple of the number of folds
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>—a
slightly simplifying assumption—the number of possible partitions of
cases into folds is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>n</mi><mi>!</mi></mrow><msup><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mi>/</mi><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>!</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>k</mi></msup></mfrac><annotation encoding="application/x-tex">\frac{n!}{[(n/k)!]^k}</annotation></semantics></math>,
a number that grows very large very quickly. For example, for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">n = 10</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">k = 5</annotation></semantics></math>,
so that the folds are each of size
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>/</mi><mi>k</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n/k = 2</annotation></semantics></math>,
there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>113</mn><mo>,</mo><mn>400</mn></mrow><annotation encoding="application/x-tex">113,400</annotation></semantics></math>
possible partitions; for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">n=100</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">k=5</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>/</mi><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">n/k = 20</annotation></semantics></math>,
still a small problem, the number of possible partitions is truly
astronomical,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.09</mn><mo>×</mo><msup><mn>10</mn><mn>66</mn></msup></mrow><annotation encoding="application/x-tex">1.09\times 10^{66}</annotation></semantics></math>.</p>
<p>Because the partition into folds that’s employed is selected
randomly, the resulting CV criterion estimates are subject to sampling
error. (An exception is LOO cross-validation, which is not at all
random.) To get a sense of the magnitude of the sampling error, we can
repeat the CV procedure with different randomly selected partitions into
folds. All of the CV functions in the <strong>cv</strong> package are
capable of repeated cross-validation, with the number of repetitions
controlled by the <code>reps</code> argument, which defaults to
<code>1</code>.</p>
<p>Here, for example, is 10-fold CV for the Mroz logistic regression,
repeated 5 times:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">cv.mroz.reps</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="va">m.mroz</span>,</span>
<span>  criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">248</span>,</span>
<span>  reps <span class="op">=</span> <span class="fl">5</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"Woodbury"</span></span>
<span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; R RNG seed set to 248</span></span>
<span><span class="co">#&gt; R RNG seed set to 68134</span></span>
<span><span class="co">#&gt; R RNG seed set to 767359</span></span>
<span><span class="co">#&gt; R RNG seed set to 556270</span></span>
<span><span class="co">#&gt; R RNG seed set to 882966</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 1:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31301</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.27967, 0.34635)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 2:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.31607</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3117</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.27847, 0.34493)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 3:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.31474</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.30862</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.27543, 0.34181)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 4:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31807</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28462, 0.35152)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Replicate 5:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31926</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28581, 0.35271)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Average:</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.31983 (0.003887)</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31394 (0.0040093)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<p>When <code>reps</code> &gt; <code>1</code>, the result returned by
<code><a href="../reference/cv.html">cv()</a></code> is an object of class <code>"cvList"</code>—literally
a list of <code>"cv"</code> objects. The results are reported for each
repetition and then averaged across repetitions, with the standard
deviations of the CV criterion and the biased-adjusted CV criterion
given in parentheses. In this example, there is therefore little
variation across repetitions, increasing our confidence in the
reliability of the results.</p>
<p>Notice that the seed that’s set in the <code><a href="../reference/cv.html">cv()</a></code> command
pertains to the first repetition and the seeds for the remaining
repetitions are then selected pseudo-randomly.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Because of the manner in which the computation is
performed, the order of the replicates in the &lt;code&gt;"cvList"&lt;/code&gt;
object returned by &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; isn’t the same as the order in
which the replicates are computed. Each element of the result, however,
is a &lt;code&gt;"cv"&lt;/code&gt; object with the correct random-number seed saved,
and so this technical detail can be safely ignored. The individual
&lt;code&gt;"cv"&lt;/code&gt; objects are printed in the order in which they are
stored rather than the order in which they are computed.&lt;/p&gt;'><sup>4</sup></a> Setting the first
seed, however, makes the entire process easily replicable, and the seed
for each repetition is stored in the corresponding element of the
<code>"cvList"</code> object.</p>
<p>The <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method for <code>"cvList"</code> objects by
default shows the adjusted CV criterion and confidence interval for each
replication:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.mroz.reps</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fcv.mroz.reps-1.png" alt="Replicated cross-validated 10-fold CV for the logistic regression fit to the `Mroz` data." width="90%"><p class="caption">
Replicated cross-validated 10-fold CV for the logistic regression fit to
the <code>Mroz</code> data.
</p>
</div>
<p>It’s also possible to replicate CV when comparing competing models
via the <code><a href="../reference/cv.html">cv()</a></code> method for <code>"modList"</code> objects.
Recall our comparison of polynomial regressions of varying degree fit to
the <code>Auto</code> data; we performed 10-fold CV for each of 10
models. Here, we replicate that process 5 times for each model and graph
the results:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.auto.reps</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/cv.modList.html">models</a></span><span class="op">(</span><span class="va">m.1</span>, <span class="va">m.2</span>, <span class="va">m.3</span>, <span class="va">m.4</span>, <span class="va">m.5</span>,</span>
<span>         <span class="va">m.6</span>, <span class="va">m.7</span>, <span class="va">m.8</span>, <span class="va">m.9</span>, <span class="va">m.10</span><span class="op">)</span>,</span>
<span>  data <span class="op">=</span> <span class="va">Auto</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">8004</span>,</span>
<span>  reps <span class="op">=</span> <span class="fl">5</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">cv.auto.reps</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig%2Fmodel-comparison-with-reps-1.png" alt=" Replicated cross-validated 10-fold CV as a function of polynomial degree, $p$" width="100%"><p class="caption">
Replicated cross-validated 10-fold CV as a function of polynomial
degree,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math></p>
</div>
<p>The graph shows both the average CV criterion and its range for each
of the competing models.</p>
</div>
<div class="section level2">
<h2 id="manipulating-cv-and-related-objects">Manipulating “cv” and related objects<a class="anchor" aria-label="anchor" href="#manipulating-cv-and-related-objects"></a>
</h2>
<p>The <code><a href="../reference/cv.html">cv()</a></code> functions returns an object of class
<code>"cv"</code>—or a closely related object, for example, of class
<code>"cvList"</code>—which contains a variety of information about the
results of a CV procedure. The <strong>cv</strong> package provides
<code><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame()</a></code> methods to put this information in the form
of data frames for further examination and analysis.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;These &lt;code&gt;&lt;a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link"&gt;as.data.frame()&lt;/a&gt;&lt;/code&gt; methods were created
at the suggestion of Michael Friendly of York University as a mechanism
for making the output of the various &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; methods more
accessible to the user.&lt;/p&gt;'><sup>5</sup></a> There is also a
<code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> method for extracting and summarizing the
information in the resulting data frames.</p>
<p>We’ll illustrate with the replicated CV that we performed for the 10
polynomial-regression models fit to the <code>Auto</code> data:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">cv.auto.reps</span></span>
<span><span class="co">#&gt; Model model.1 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 24.185 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.2 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.253 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.3 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.349 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.4 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.449 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.5 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.095 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.6 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.034 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.7 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 18.897 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.8 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.026 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.9 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.115 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Model model.10 averaged across 5 replications:</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.427</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">class</a></span><span class="op">(</span><span class="va">cv.auto.reps</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "cvModList"</span></span></code></pre></div>
<p>In this case, there are 5 replications of 10-fold CV.</p>
<p>Converting <code>cv.auto.reps</code> into a data frame produces, by
default:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">cv.auto.reps</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 550  62</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">class</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "cvModListDataFrame" "cvListDataFrame"    "cvDataFrame"       </span></span>
<span><span class="co">#&gt; [4] "data.frame"</span></span></code></pre></div>
<p>The resulting data frame has
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>𝗋</mi><mi>𝖾</mi><mi>𝗉</mi><mi>𝗅</mi><mi>𝗂</mi><mi>𝖼</mi><mi>𝖺</mi><mi>𝗍</mi><mi>𝗂</mi><mi>𝗈</mi><mi>𝗇</mi><mi>𝗌</mi></mrow><mo>×</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mi>𝖿</mi><mi>𝗈</mi><mi>𝗅</mi><mi>𝖽</mi><mi>𝗌</mi></mrow><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mrow><mi>𝗆</mi><mi>𝗈</mi><mi>𝖽</mi><mi>𝖾</mi><mi>𝗅</mi><mi>𝗌</mi></mrow><mo>=</mo><mn>5</mn><mo>×</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>10</mn><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mn>10</mn><mo>=</mo><mn>550</mn></mrow><annotation encoding="application/x-tex">\mathsf{replications} \times (\mathsf{folds} + 1) \times \mathsf{models} = 5 \times (10 + 1) \times 10 = 550</annotation></semantics></math>
rows, the first few of which are</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span></span>
<span><span class="co">#&gt;     model rep fold criterion adjusted.criterion full.criterion coef.Intercept</span></span>
<span><span class="co">#&gt; 1 model.1   1    0      24.2               24.2           23.9           23.4</span></span>
<span><span class="co">#&gt; 2 model.1   1    1      34.3                 NA             NA           23.3</span></span>
<span><span class="co">#&gt; 3 model.1   1    2      24.5                 NA             NA           23.3</span></span>
<span><span class="co">#&gt; 4 model.1   1    3      13.9                 NA             NA           23.6</span></span>
<span><span class="co">#&gt; 5 model.1   1    4      15.5                 NA             NA           23.5</span></span>
<span><span class="co">#&gt; 6 model.1   1    5      28.6                 NA             NA           23.4</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 1) coef.poly(horsepower, 2)1 coef.poly(horsepower, 2)2</span></span>
<span><span class="co">#&gt; 1                     -120                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                     -121                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                     -119                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                     -120                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                     -118                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                     -119                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 3)1 coef.poly(horsepower, 3)2 coef.poly(horsepower, 3)3</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 4)1 coef.poly(horsepower, 4)2 coef.poly(horsepower, 4)3</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 4)4 coef.poly(horsepower, 5)1 coef.poly(horsepower, 5)2</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 5)3 coef.poly(horsepower, 5)4 coef.poly(horsepower, 5)5</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 6)1 coef.poly(horsepower, 6)2 coef.poly(horsepower, 6)3</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 6)4 coef.poly(horsepower, 6)5 coef.poly(horsepower, 6)6</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 7)1 coef.poly(horsepower, 7)2 coef.poly(horsepower, 7)3</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 7)4 coef.poly(horsepower, 7)5 coef.poly(horsepower, 7)6</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 7)7 coef.poly(horsepower, 8)1 coef.poly(horsepower, 8)2</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 8)3 coef.poly(horsepower, 8)4 coef.poly(horsepower, 8)5</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 8)6 coef.poly(horsepower, 8)7 coef.poly(horsepower, 8)8</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 9)1 coef.poly(horsepower, 9)2 coef.poly(horsepower, 9)3</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 9)4 coef.poly(horsepower, 9)5 coef.poly(horsepower, 9)6</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 9)7 coef.poly(horsepower, 9)8 coef.poly(horsepower, 9)9</span></span>
<span><span class="co">#&gt; 1                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 2                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 3                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 4                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 5                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt; 6                        NA                        NA                        NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 10)1 coef.poly(horsepower, 10)2</span></span>
<span><span class="co">#&gt; 1                         NA                         NA</span></span>
<span><span class="co">#&gt; 2                         NA                         NA</span></span>
<span><span class="co">#&gt; 3                         NA                         NA</span></span>
<span><span class="co">#&gt; 4                         NA                         NA</span></span>
<span><span class="co">#&gt; 5                         NA                         NA</span></span>
<span><span class="co">#&gt; 6                         NA                         NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 10)3 coef.poly(horsepower, 10)4</span></span>
<span><span class="co">#&gt; 1                         NA                         NA</span></span>
<span><span class="co">#&gt; 2                         NA                         NA</span></span>
<span><span class="co">#&gt; 3                         NA                         NA</span></span>
<span><span class="co">#&gt; 4                         NA                         NA</span></span>
<span><span class="co">#&gt; 5                         NA                         NA</span></span>
<span><span class="co">#&gt; 6                         NA                         NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 10)5 coef.poly(horsepower, 10)6</span></span>
<span><span class="co">#&gt; 1                         NA                         NA</span></span>
<span><span class="co">#&gt; 2                         NA                         NA</span></span>
<span><span class="co">#&gt; 3                         NA                         NA</span></span>
<span><span class="co">#&gt; 4                         NA                         NA</span></span>
<span><span class="co">#&gt; 5                         NA                         NA</span></span>
<span><span class="co">#&gt; 6                         NA                         NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 10)7 coef.poly(horsepower, 10)8</span></span>
<span><span class="co">#&gt; 1                         NA                         NA</span></span>
<span><span class="co">#&gt; 2                         NA                         NA</span></span>
<span><span class="co">#&gt; 3                         NA                         NA</span></span>
<span><span class="co">#&gt; 4                         NA                         NA</span></span>
<span><span class="co">#&gt; 5                         NA                         NA</span></span>
<span><span class="co">#&gt; 6                         NA                         NA</span></span>
<span><span class="co">#&gt;   coef.poly(horsepower, 10)9 coef.poly(horsepower, 10)10</span></span>
<span><span class="co">#&gt; 1                         NA                          NA</span></span>
<span><span class="co">#&gt; 2                         NA                          NA</span></span>
<span><span class="co">#&gt; 3                         NA                          NA</span></span>
<span><span class="co">#&gt; 4                         NA                          NA</span></span>
<span><span class="co">#&gt; 5                         NA                          NA</span></span>
<span><span class="co">#&gt; 6                         NA                          NA</span></span></code></pre></div>
<p>All of these rows pertain to the first model and the first
replication, and fold = 0 indicates the overall results for the first
replication of the first model.</p>
<p>The regression coefficients appear as columns in the data frame.
Because the first model includes only an intercept and a linear
polynomial term, the other coefficients are all <code>NA</code>.</p>
<p>It’s possible to suppress the regression coefficients by specifying
the argument <code>columns="criteria"</code> to
<code><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame()</a></code>:</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">D</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">cv.auto.reps</span>, columns<span class="op">=</span><span class="st">"criteria"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">D</span><span class="op">)</span></span>
<span><span class="co">#&gt;     model rep fold criterion adjusted.criterion full.criterion</span></span>
<span><span class="co">#&gt; 1 model.1   1    0      24.2               24.2           23.9</span></span>
<span><span class="co">#&gt; 2 model.1   1    1      34.3                 NA             NA</span></span>
<span><span class="co">#&gt; 3 model.1   1    2      24.5                 NA             NA</span></span>
<span><span class="co">#&gt; 4 model.1   1    3      13.9                 NA             NA</span></span>
<span><span class="co">#&gt; 5 model.1   1    4      15.5                 NA             NA</span></span>
<span><span class="co">#&gt; 6 model.1   1    5      28.6                 NA             NA</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/subset.html" class="external-link">subset</a></span><span class="op">(</span><span class="va">D</span>, <span class="va">fold</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;      model rep fold criterion adjusted.criterion full.criterion</span></span>
<span><span class="co">#&gt; 1  model.1   1    0      24.2               24.2           23.9</span></span>
<span><span class="co">#&gt; 12 model.1   2    0      24.1               24.1           23.9</span></span>
<span><span class="co">#&gt; 23 model.1   3    0      24.2               24.2           23.9</span></span>
<span><span class="co">#&gt; 34 model.1   4    0      24.2               24.1           23.9</span></span>
<span><span class="co">#&gt; 45 model.1   5    0      24.2               24.2           23.9</span></span>
<span><span class="co">#&gt; 56 model.2   1    0      19.2               19.2           19.0</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> method for <code>"cvDataFrame"</code> and
related objects has a formula interface, and may be used, for example,
as follows:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">D</span>, <span class="va">adjusted.criterion</span> <span class="op">~</span> <span class="va">model</span> <span class="op">+</span> <span class="va">rep</span><span class="op">)</span> <span class="co"># fold "0" only</span></span>
<span><span class="co">#&gt;           rep</span></span>
<span><span class="co">#&gt; model           1      2      3      4      5</span></span>
<span><span class="co">#&gt;   model.1  24.193 24.113 24.226 24.144 24.184</span></span>
<span><span class="co">#&gt;   model.2  19.209 19.285 19.240 19.205 19.256</span></span>
<span><span class="co">#&gt;   model.3  19.309 19.445 19.348 19.255 19.282</span></span>
<span><span class="co">#&gt;   model.4  19.521 19.524 19.402 19.343 19.301</span></span>
<span><span class="co">#&gt;   model.5  19.242 19.139 19.137 18.877 18.899</span></span>
<span><span class="co">#&gt;   model.6  19.157 19.145 19.082 18.730 18.838</span></span>
<span><span class="co">#&gt;   model.7  18.935 19.056 18.960 18.596 18.715</span></span>
<span><span class="co">#&gt;   model.8  19.043 19.174 19.111 18.675 18.861</span></span>
<span><span class="co">#&gt;   model.9  19.133 19.286 19.161 18.811 18.880</span></span>
<span><span class="co">#&gt;   model.10 19.524 19.586 19.345 19.027 19.214</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">D</span>, <span class="va">criterion</span> <span class="op">~</span> <span class="va">model</span> <span class="op">+</span> <span class="va">rep</span>, </span>
<span>        include<span class="op">=</span><span class="st">"folds"</span><span class="op">)</span> <span class="co"># mean over folds</span></span>
<span><span class="co">#&gt;           rep</span></span>
<span><span class="co">#&gt; model           1      2      3      4      5</span></span>
<span><span class="co">#&gt;   model.1  24.181 24.128 24.226 24.134 24.229</span></span>
<span><span class="co">#&gt;   model.2  19.202 19.310 19.236 19.193 19.306</span></span>
<span><span class="co">#&gt;   model.3  19.309 19.483 19.352 19.247 19.336</span></span>
<span><span class="co">#&gt;   model.4  19.536 19.571 19.415 19.342 19.360</span></span>
<span><span class="co">#&gt;   model.5  19.262 19.195 19.163 18.874 18.963</span></span>
<span><span class="co">#&gt;   model.6  19.182 19.217 19.116 18.731 18.910</span></span>
<span><span class="co">#&gt;   model.7  18.954 19.135 18.997 18.601 18.787</span></span>
<span><span class="co">#&gt;   model.8  19.071 19.259 19.162 18.686 18.944</span></span>
<span><span class="co">#&gt;   model.9  19.171 19.379 19.214 18.834 18.965</span></span>
<span><span class="co">#&gt;   model.10 19.603 19.710 19.416 19.068 19.329</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">D</span>, <span class="va">criterion</span> <span class="op">~</span> <span class="va">model</span> <span class="op">+</span> <span class="va">rep</span>, fun<span class="op">=</span><span class="va">sd</span>, </span>
<span>        include<span class="op">=</span><span class="st">"folds"</span><span class="op">)</span></span>
<span><span class="co">#&gt;           rep</span></span>
<span><span class="co">#&gt; model           1      2      3      4      5</span></span>
<span><span class="co">#&gt;   model.1  7.5627 5.2658 5.7947 4.7165 7.3591</span></span>
<span><span class="co">#&gt;   model.2  5.9324 5.2518 5.8166 5.1730 6.2126</span></span>
<span><span class="co">#&gt;   model.3  6.0754 5.4492 5.8168 5.2288 6.2387</span></span>
<span><span class="co">#&gt;   model.4  6.1737 5.2488 5.9952 5.4866 6.3122</span></span>
<span><span class="co">#&gt;   model.5  5.7598 5.2136 6.2658 5.3923 6.3016</span></span>
<span><span class="co">#&gt;   model.6  5.7094 5.0444 6.4010 5.2868 5.9796</span></span>
<span><span class="co">#&gt;   model.7  5.6307 5.1164 6.6751 5.1084 5.8796</span></span>
<span><span class="co">#&gt;   model.8  5.7009 5.1229 6.4827 5.1068 5.9460</span></span>
<span><span class="co">#&gt;   model.9  5.7979 5.2289 6.3960 5.3181 6.0902</span></span>
<span><span class="co">#&gt;   model.10 6.1825 4.8554 6.2557 5.7029 6.1985</span></span></code></pre></div>
<p>See <code><a href="../reference/cv.html">?summary.cvDataFrame</a></code> for details.</p>
</div>
<div class="section level2">
<h2 id="parallel-computations">Parallel computations<a class="anchor" aria-label="anchor" href="#parallel-computations"></a>
</h2>
<p>The CV functions in the <strong>cv</strong> package are all capable
of performing parallel computations by setting the <code>ncores</code>
argument (specifying the number of computer cores to be used) to a
number &gt; <code>1</code> (which is the default). Parallel computation
can be advantageous for large problems, reducing the execution time of
the program.</p>
<p>To illustrate, let’s time model selection in Mroz’s logistic
regression, repeating the computation as performed previously (but with
LOO CV to lengthen the calculation) and then doing it in parallel using
2 cores:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span></span>
<span>  <span class="va">m.mroz.sel.cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>    <span class="va">selectStepAIC</span>,</span>
<span>    <span class="va">Mroz</span>,</span>
<span>    k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>    criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>    working.model <span class="op">=</span> <span class="va">m.mroz</span>,</span>
<span>    AIC <span class="op">=</span> <span class="cn">FALSE</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;    user  system elapsed </span></span>
<span><span class="co">#&gt;  22.314   0.572  22.910</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/system.time.html" class="external-link">system.time</a></span><span class="op">(</span></span>
<span>  <span class="va">m.mroz.sel.cv.p</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>    <span class="va">selectStepAIC</span>,</span>
<span>    <span class="va">Mroz</span>,</span>
<span>    k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>    criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>    working.model <span class="op">=</span> <span class="va">m.mroz</span>,</span>
<span>    AIC <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>    ncores <span class="op">=</span> <span class="fl">2</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt;    user  system elapsed </span></span>
<span><span class="co">#&gt;   0.265   0.048  13.157</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/all.equal-methods.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">m.mroz.sel.cv</span>, <span class="va">m.mroz.sel.cv.p</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>On our computer, the parallel computation with 2 cores is nearly
twice as fast, and produces the same result as the non-parallel
computation.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-ArlotCelisse:2010" class="csl-entry">
Arlot, S., &amp; Celisse, A. (2010). <span class="nocase">A survey of
cross-validation procedures for model selection</span>. <em>Statistics
Surveys</em>, <em>4</em>, 40–79. Retrieved from <a href="https://doi.org/10.1214/09-SS054" class="external-link">https://doi.org/10.1214/09-SS054</a>
</div>
<div id="ref-BatesHastieTibshirani:2023" class="csl-entry">
Bates, S., Hastie, T., &amp; Tibshirani, R. (2023). Cross-validation:
What does it estimate and how well does it do it? <em>Journal of the
American Statistical Association</em>, <em>in press</em>. Retrieved from
<a href="https://doi.org/10.1080/01621459.2023.2197686" class="external-link">https://doi.org/10.1080/01621459.2023.2197686</a>
</div>
<div id="ref-CantyRipley2022" class="csl-entry">
Canty, A., &amp; Ripley, B. D. (2022). <em>Boot: Bootstrap
<span>R</span> (<span>S</span>-plus) functions</em>.
</div>
<div id="ref-DavisonHinkley:1997" class="csl-entry">
Davison, A. C., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and
their applications</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-FoxWeisberg:2019" class="csl-entry">
Fox, J., &amp; Weisberg, S. (2019). <em>An <span>R</span> companion to
applied regression</em> (Third edition). Thousand Oaks <span>CA</span>:
Sage.
</div>
<div id="ref-Hager:1989" class="csl-entry">
Hager, W. W. (1989). Updating the inverse of a matrix.
<em><span>SIAM</span> Review</em>, <em>31</em>(2), 221–239.
</div>
<div id="ref-Harrell:2015" class="csl-entry">
Harrell, F., Jr. (2015). <em>Regression modeling strategies</em> (Second
edition). New York: Springer.
</div>
<div id="ref-JamesEtAl:2021" class="csl-entry">
James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). <em>An
introduction to statistical learning with applications in
<span>R</span></em> (Second edition). New York: Springer.
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by John Fox, Georges Monette.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
