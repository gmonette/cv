<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="cv">
<title>Computational and technical notes on cross-validating regression models • cv</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Computational and technical notes on cross-validating regression models">
<meta property="og:description" content="cv">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cv</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">2.0.1</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/cv.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/cv-extend.html">Extending the cv package</a>
    <a class="dropdown-item" href="../articles/cv-mixed.html">Cross-validating mixed-effects models</a>
    <a class="dropdown-item" href="../articles/cv-notes.html">Computational and technical notes on cross-validating regression models</a>
    <a class="dropdown-item" href="../articles/cv-selection.html">Cross-validating model selection</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/gmonette/cv/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Computational and technical notes on cross-validating regression models</h1>
                        <h4 data-toc-skip class="author">John Fox and
Georges Monette</h4>
            
            <h4 data-toc-skip class="date">2024-06-28</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/gmonette/cv/blob/HEAD/vignettes/cv-notes.Rmd" class="external-link"><code>vignettes/cv-notes.Rmd</code></a></small>
      <div class="d-none name"><code>cv-notes.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="efficient-computations-for-linear-and-generalized-linear-models">Efficient computations for linear and generalized linear models<a class="anchor" aria-label="anchor" href="#efficient-computations-for-linear-and-generalized-linear-models"></a>
</h2>
<p>The most straightforward way to implement cross-validation in R for
statistical modeling functions that are written in the canonical manner
is to use <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code> to refit the model with each fold
removed. This is the approach taken in the default method for
<code><a href="../reference/cv.html">cv()</a></code>, and it is appropriate if the cases are independently
sampled. Refitting the model in this manner for each fold is generally
feasible when the number of folds in modest, but can be prohibitively
costly for leave-one-out cross-validation when the number of cases is
large.</p>
<p>The <code>"lm"</code> and <code>"glm"</code> methods for
<code><a href="../reference/cv.html">cv()</a></code> take advantage of computational efficiencies by
avoiding refitting the model with each fold removed. Consider, in
particular, the weighted linear model <span class="math inline">\(\mathbf{y}_{n \times 1} = \mathbf{X}_{n \times
p}\boldsymbol{\beta}_{p \times 1} + \boldsymbol{\varepsilon}_{n \times
1}\)</span>, where <span class="math inline">\(\boldsymbol{\varepsilon}
\sim \mathbf{N}_n \left(\mathbf{0}, \sigma^2 \mathbf{W}^{-1}_{n \times
n}\right)\)</span>. Here, <span class="math inline">\(\mathbf{y}\)</span> is the response vector, <span class="math inline">\(\mathbf{X}\)</span> the model matrix, and <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> the error
vector, each for <span class="math inline">\(n\)</span> cases, and <span class="math inline">\(\boldsymbol{\beta}\)</span> is the vector of <span class="math inline">\(p\)</span> population regression coefficients. The
errors are assumed to be multivariately normally distributed with 0
means and covariance matrix <span class="math inline">\(\sigma^2
\mathbf{W}^{-1}\)</span>, where <span class="math inline">\(\mathbf{W} =
\mathrm{diag}(w_i)\)</span> is a diagonal matrix of inverse-variance
weights. For the linear model with constant error variance, the weight
matrix is taken to be <span class="math inline">\(\mathbf{W} =
\mathbf{I}_n\)</span>, the order-<span class="math inline">\(n\)</span>
identity matrix.</p>
<p>The weighted-least-squares (WLS) estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span> is <span class="citation">(see, e.g., Fox, 2016, sec. 12.2.2)</span> <a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;This is a definitional formula, which assumes that the
model matrix &lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is of full
column rank, and which can be subject to numerical instability when
&lt;span class="math inline"&gt;\(\mathbf{X}\)&lt;/span&gt; is ill-conditioned.
&lt;code&gt;&lt;a href="https://rdrr.io/r/stats/lm.html" class="external-link"&gt;lm()&lt;/a&gt;&lt;/code&gt; uses the singular-value decomposition of the model
matrix to obtain computationally more stable results.&lt;/p&gt;'><sup>1</sup></a> <span class="math display">\[
\mathbf{b}_{\mathrm{WLS}} = \left( \mathbf{X}^T \mathbf{W} \mathbf{X}
\right)^{-1}
  \mathbf{X}^T \mathbf{W} \mathbf{y}
\]</span></p>
<p>Fitted values are then <span class="math inline">\(\widehat{\mathbf{y}} =
\mathbf{X}\mathbf{b}_{\mathrm{WLS}}\)</span>.</p>
<p>The LOO fitted value for the <span class="math inline">\(i\)</span>th
case can be efficiently computed by <span class="math inline">\(\widehat{y}_{-i} = y_i - e_i/(1 - h_i)\)</span>
where <span class="math inline">\(h_i = \mathbf{x}^T_i \left(
\mathbf{X}^T \mathbf{W} \mathbf{X} \right)^{-1} \mathbf{x}_i\)</span>
(the so-called “hatvalue”). Here, <span class="math inline">\(\mathbf{x}^T_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mathbf{X}\)</span>, and <span class="math inline">\(\mathbf{x}_i\)</span> is the <span class="math inline">\(i\)</span>th row written as a column vector. This
approach can break down when one or more hatvalues are equal to 1, in
which case the formula for <span class="math inline">\(\widehat{y}_{-i}\)</span> requires division by
0.</p>
<p>To compute cross-validated fitted values when the folds contain more
than one case, we make use of the Woodbury matrix identity <span class="citation">(Hager, 1989)</span>, <span class="math display">\[
\left(\mathbf{A}_{m \times m} + \mathbf{U}_{m \times k}
\mathbf{C}_{k \times k} \mathbf{V}_{k \times m} \right)^{-1} =
\mathbf{A}^{-1} - \mathbf{A}^{-1}\mathbf{U} \left(\mathbf{C}^{-1} +
\mathbf{VA}^{-1}\mathbf{U} \right)^{-1} \mathbf{VA}^{-1}
\]</span> where <span class="math inline">\(\mathbf{A}\)</span> is a
nonsingular order-<span class="math inline">\(n\)</span> matrix. We
apply this result by letting <span class="math display">\[\begin{align*}
    \mathbf{A} &amp;= \mathbf{X}^T \mathbf{W} \mathbf{X} \\
    \mathbf{U} &amp;= \mathbf{X}_\mathbf{j}^T \\
    \mathbf{V} &amp;= - \mathbf{X}_\mathbf{j} \\
    \mathbf{C} &amp;= \mathbf{W}_\mathbf{j} \\
\end{align*}\]</span> where the subscript <span class="math inline">\(\mathbf{j} = (i_{j1}, \ldots, i_{jm})^T\)</span>
represents the vector of indices for the cases in the <span class="math inline">\(j\)</span>th fold, <span class="math inline">\(j =
1, \ldots, k\)</span>. The negative sign in <span class="math inline">\(\mathbf{V} = - \mathbf{X}_\mathbf{j}\)</span>
reflects the <em>removal</em>, rather than addition, of the cases in
<span class="math inline">\(\mathbf{j}\)</span>.</p>
<p>Applying the Woodbury identity isn’t quite as fast as using the
hatvalues, but it is generally much faster than refitting the model. A
disadvantage of the Woodbury identity, however, is that it entails
explicit matrix inversion and thus may be numerically unstable. The
inverse of <span class="math inline">\(\mathbf{A} = \mathbf{X}^T
\mathbf{W} \mathbf{X}\)</span> is available directly in the
<code>"lm"</code> object, but the second term on the right-hand side of
the Woodbury identity requires a matrix inversion with each fold
deleted. (In contrast, the inverse of each <span class="math inline">\(\mathbf{C} = \mathbf{W}_\mathbf{j}\)</span> is
straightforward because <span class="math inline">\(\mathbf{W}\)</span>
is diagonal.)</p>
<p>The Woodbury identity also requires that the model matrix be of full
rank. We impose that restriction in our code by removing redundant
regressors from the model matrix for all of the cases, but that doesn’t
preclude rank deficiency from surfacing when a fold is removed. Rank
deficiency of <span class="math inline">\(\mathbf{X}\)</span> doesn’t
disqualify cross-validation because all we need are fitted values under
the estimated model.</p>
<p><code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code> computes the maximum-likelihood estimates for a
generalized linear model by iterated weighted least squares <span class="citation">(see, e.g., Fox &amp; Weisberg, 2019, sec.
6.12)</span>. The last iteration is therefore just a WLS fit of the
“working response” on the model matrix using “working weights.” Both the
working weights and the working response at convergence are available
from the information in the object returned by <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code>.</p>
<p>We then treat re-estimation of the model with a case or cases deleted
as a WLS problem, using the hatvalues or the Woodbury matrix identity.
The resulting fitted values for the deleted fold aren’t exact—that is,
except for the Gaussian family, the result isn’t identical to what we
would obtain by literally refitting the model—but in our (limited)
experience, the approximation is very good, especially for LOO CV, which
is when we would be most tempted to use it. Nevertheless, because these
results are approximate, the default for the <code>"glm"</code>
<code><a href="../reference/cv.html">cv()</a></code> method is to perform the exact computation, which
entails refitting the model with each fold omitted.</p>
<p>Let’s compare the efficiency of the various computational methods for
linear and generalized linear models. Consider, for example,
leave-one-out cross-validation for the quadratic regression of
<code>mpg</code> on <code>horsepower</code> in the <code>Auto</code>
data, from the introductory “Cross-validating regression models”
vignette, repeated here:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Auto"</span>, package<span class="op">=</span><span class="st">"ISLR2"</span><span class="op">)</span></span>
<span><span class="va">m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 2), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://gmonette.github.io/cv/">"cv"</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: doParallel</span></span>
<span><span class="co">#&gt; Loading required package: foreach</span></span>
<span><span class="co">#&gt; Loading required package: iterators</span></span>
<span><span class="co">#&gt; Loading required package: parallel</span></span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span><span class="op">)</span>  <span class="co"># default method = "hatvalues"</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span>, method <span class="op">=</span> <span class="st">"naive"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: naive</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span>, method <span class="op">=</span> <span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>This is a small regression problem and all three computational
approaches are essentially instantaneous, but it is still of interest to
investigate their relative speed. In this comparison, we include the
<code><a href="../reference/cv.html">cv.glm()</a></code> function from the <strong>boot</strong> package
<span class="citation">(Canty &amp; Ripley, 2022; Davison &amp; Hinkley,
1997)</span>, which takes the naive approach, and for which we have to
fit the linear model as an equivalent Gaussian GLM. We use the
<code>microbenchmark()</code> function from the package of the same name
for the timings <span class="citation">(Mersmann, 2023)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto.glm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Auto</span>, <span class="va">m.auto.glm</span><span class="op">)</span><span class="op">$</span><span class="va">delta</span></span>
<span><span class="co">#&gt; [1] 19.248 19.248</span></span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu">microbenchmark</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>  hatvalues <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span><span class="op">)</span>,</span>
<span>  Woodbury <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span>, method <span class="op">=</span> <span class="st">"Woodbury"</span><span class="op">)</span>,</span>
<span>  naive <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k <span class="op">=</span> <span class="st">"loo"</span>, method <span class="op">=</span> <span class="st">"naive"</span><span class="op">)</span>,</span>
<span>  cv.glm <span class="op">=</span> <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Auto</span>, <span class="va">m.auto.glm</span><span class="op">)</span>,</span>
<span>  times <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Warning in microbenchmark::microbenchmark(hatvalues = cv(m.auto, k = "loo"), :</span></span>
<span><span class="co">#&gt; less accurate nanosecond times to avoid potential integer overflows</span></span>
<span><span class="co">#&gt; Unit: microseconds</span></span>
<span><span class="co">#&gt;       expr       min       lq     mean   median       uq      max neval</span></span>
<span><span class="co">#&gt;  hatvalues    990.15   1000.7   1172.3   1182.1   1241.4   1402.6    10</span></span>
<span><span class="co">#&gt;   Woodbury  12293.28  12511.7  12926.3  12619.0  12691.6  15827.9    10</span></span>
<span><span class="co">#&gt;      naive 218903.30 223974.6 230250.5 225540.3 226144.2 284285.8    10</span></span>
<span><span class="co">#&gt;     cv.glm 384901.81 388644.2 402139.0 389392.3 403210.6 452841.4    10</span></span></code></pre></div>
<p>On our computer, using the hatvalues is about an order of magnitude
faster than employing Woodbury matrix updates, and more than two orders
of magnitude faster than refitting the model.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Out of impatience, we asked
&lt;code&gt;microbenchmark()&lt;/code&gt; to execute each command only 10 times
rather than the default 100. With the exception of the last columns, the
output is self-explanatory. The last column shows which methods have
average timings that are statistically distinguishable. Because of the
small number of repetitions (i.e., 10), the &lt;code&gt;"hatvalues"&lt;/code&gt; and
&lt;code&gt;"Woodbury"&lt;/code&gt; methods aren’t distinguishable, but the
difference between these methods persists when we perform more
repetitions—we invite the reader to redo this computation with the
default &lt;code&gt;times=100&lt;/code&gt; repetitions.&lt;/p&gt;'><sup>2</sup></a></p>
<p>Similarly, let’s return to the logistic-regression model fit to
Mroz’s data on women’s labor-force participation, also employed as an
example in the introductory vignette:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Mroz"</span>, package<span class="op">=</span><span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="va">m.mroz</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">Mroz</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 921.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span></code></pre></div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, <span class="co"># default method = "exact"</span></span>
<span>   k <span class="op">=</span> <span class="st">"loo"</span>, </span>
<span>   criterion <span class="op">=</span> <span class="va">BayesRule</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: exact</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>,</span>
<span>   k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>   criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>   method <span class="op">=</span> <span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; 95% CI for bias-adjusted CV criterion = (0.28496, 0.35164)</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>,</span>
<span>   k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>   criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>   method <span class="op">=</span> <span class="st">"hatvalues"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span></code></pre></div>
<p>As for linear models, we report some timings for the various
<code><a href="../reference/cv.html">cv()</a></code> methods of computation in LOO CV as well as for the
<code><a href="../reference/cv.html">cv.glm()</a></code> function from the <strong>boot</strong> package
(which, recall, refits the model with each case removed, and thus is
comparable to <code><a href="../reference/cv.html">cv()</a></code> with <code>method="exact"</code>):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">microbenchmark</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>  hatvalues <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>    <span class="va">m.mroz</span>,</span>
<span>    k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>    criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>    method <span class="op">=</span> <span class="st">"hatvalues"</span></span>
<span>  <span class="op">)</span>,</span>
<span>  Woodbury <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span></span>
<span>    <span class="va">m.mroz</span>,</span>
<span>    k <span class="op">=</span> <span class="st">"loo"</span>,</span>
<span>    criterion <span class="op">=</span> <span class="va">BayesRule</span>,</span>
<span>    method <span class="op">=</span> <span class="st">"Woodbury"</span></span>
<span>  <span class="op">)</span>,</span>
<span>  exact <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k <span class="op">=</span> <span class="st">"loo"</span>, criterion <span class="op">=</span> <span class="va">BayesRule</span><span class="op">)</span>,</span>
<span>  cv.glm <span class="op">=</span> <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="va">m.mroz</span>,</span>
<span>                        cost <span class="op">=</span> <span class="va">BayesRule</span><span class="op">)</span>,</span>
<span>  times <span class="op">=</span> <span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Unit: milliseconds</span></span>
<span><span class="co">#&gt;       expr       min        lq      mean    median        uq       max neval</span></span>
<span><span class="co">#&gt;  hatvalues    1.1304    1.1505    1.3167    1.3738    1.4002    1.4346    10</span></span>
<span><span class="co">#&gt;   Woodbury   40.6850   42.6783   43.0040   43.1424   43.7039   44.0449    10</span></span>
<span><span class="co">#&gt;      exact 1755.3378 1786.8266 1828.7085 1830.6415 1867.1220 1908.3682    10</span></span>
<span><span class="co">#&gt;     cv.glm 2033.1012 2044.5248 2077.0463 2078.2190 2110.1732 2128.7473    10</span></span></code></pre></div>
<p>There is a substantial time penalty associated with exact
computations.</p>
</div>
<div class="section level2">
<h2 id="computation-of-the-bias-corrected-cv-criterion-and-confidence-intervals">Computation of the bias-corrected CV criterion and confidence
intervals<a class="anchor" aria-label="anchor" href="#computation-of-the-bias-corrected-cv-criterion-and-confidence-intervals"></a>
</h2>
<p>Let <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}})\)</span> represent a cross-validation cost
criterion, such as mean-squared error, computed for all of the <span class="math inline">\(n\)</span> values of the response <span class="math inline">\(\mathbf{y}\)</span> based on fitted values <span class="math inline">\(\widehat{\mathbf{y}}\)</span> from the model fit
to all of the data. We require that <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}})\)</span> is the mean of casewise components, that
is, <span class="math inline">\(\mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}) = \frac{1}{n}\sum_{i=1}^n\mathrm{cv}(y_i,
\widehat{y}_i)\)</span>.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;span class="citation"&gt;Arlot &amp;amp; Celisse (2010)&lt;/span&gt;
term the casewise loss, &lt;span class="math inline"&gt;\(\mathrm{cv}(y_i,
\widehat{y}_i)\)&lt;/span&gt;, the “contrast function.”&lt;/p&gt;'><sup>3</sup></a> For example, <span class="math inline">\(\mathrm{MSE}(\mathbf{y}, \widehat{\mathbf{y}}) =
\frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_i)^2\)</span>.</p>
<p>We divide the <span class="math inline">\(n\)</span> cases into <span class="math inline">\(k\)</span> folds of approximately <span class="math inline">\(n_j \approx n/k\)</span> cases each, where <span class="math inline">\(n = \sum n_j\)</span>. As above, let <span class="math inline">\(\mathbf{j}\)</span> denote the indices of the
cases in the <span class="math inline">\(j\)</span>th fold.</p>
<p>Now define <span class="math inline">\(\mathrm{CV}_j =
\mathrm{CV}(\mathbf{y}, \widehat{\mathbf{y}}^{(j)})\)</span>. The
superscript <span class="math inline">\((j)\)</span> on <span class="math inline">\(\widehat{\mathbf{y}}^{(j)}\)</span> represents
fitted values computed for all of the cases from the model with fold
<span class="math inline">\(j\)</span> omitted. Let <span class="math inline">\(\widehat{\mathbf{y}}^{(-i)}\)</span> represent the
vector of fitted values for all <span class="math inline">\(n\)</span>
cases where the fitted value for the <span class="math inline">\(i\)</span>th case is computed from the model fit
with the fold including the <span class="math inline">\(i\)</span>th
case omitted (i.e., fold <span class="math inline">\(j\)</span> for
which <span class="math inline">\(i \in \mathbf{j}\)</span>).</p>
<p>Then the cross-validation criterion is just <span class="math inline">\(\mathrm{CV} = \mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}^{(-i)})\)</span>. Following <span class="citation">Davison &amp; Hinkley (1997, pp. 293–295)</span>, the
bias-adjusted cross-validation criterion is <span class="math display">\[
\mathrm{CV}_{\mathrm{adj}} = \mathrm{CV} + \mathrm{CV}(\mathbf{y},
\widehat{\mathbf{y}}) - \frac{1}{n} \sum_{j=1}^{k} n_j \mathrm{CV}_j
\]</span></p>
<p>We compute the standard error of CV as <span class="math display">\[
\mathrm{SE}(\mathrm{CV}) = \frac{1}{\sqrt n} \sqrt{ \frac{\sum_{i=1}^n
\left[ \mathrm{cv}(y_i, \widehat{y}_i^{(-i)} ) - \mathrm{CV} \right]^2
}{n - 1} }
\]</span> that is, as the standard deviation of the casewise components
of CV divided by the square-root of the number of cases.</p>
<p>We then use <span class="math inline">\(\mathrm{SE}(\mathrm{CV})\)</span> to construct a
<span class="math inline">\(100 \times (1 - \alpha)\)</span>% confidence
interval around the <em>adjusted</em> CV estimate of error: <span class="math display">\[
\left[ \mathrm{CV}_{\mathrm{adj}} - z_{1 -
\alpha/2}\mathrm{SE}(\mathrm{CV}), \mathrm{CV}_{\mathrm{adj}} + z_{1 -
\alpha/2}\mathrm{SE}(\mathrm{CV})  \right]
\]</span> where <span class="math inline">\(z_{1 - \alpha/2}\)</span> is
the <span class="math inline">\(1 - \alpha/2\)</span> quantile of the
standard-normal distribution (e.g, <span class="math inline">\(z \approx
1.96\)</span> for a 95% confidence interval, for which <span class="math inline">\(1 - \alpha/2 = .975\)</span>).</p>
<p><span class="citation">Bates, Hastie, &amp; Tibshirani (2023)</span>
show that the coverage of this confidence interval is poor for small
samples, and they suggest a much more computationally intensive
procedure, called <em>nested cross-validation</em>, to compute better
estimates of error and confidence intervals with better coverage for
small samples. We may implement Bates et al.’s approach in a later
release of the <strong>cv</strong> package. At present we use the
confidence interval above for sufficiently large <span class="math inline">\(n\)</span>, which, based on Bates et al.’s
results, we take by default to be <span class="math inline">\(n \ge
400\)</span>.</p>
</div>
<div class="section level2">
<h2 id="why-the-complement-of-auc-isnt-a-casewise-cv-criterion">Why the complement of AUC isn’t a casewise CV criterion<a class="anchor" aria-label="anchor" href="#why-the-complement-of-auc-isnt-a-casewise-cv-criterion"></a>
</h2>
<p>Consider calculating AUC for folds in which a validation set contains
<span class="math inline">\(n_v\)</span> observations. To calculate AUC
in the validation set, we need the vector of prediction criteria, <span class="math inline">\(\widehat{\mathbf{y}}_{v_{(n_v \times 1)}} =
(\widehat{y}_1, ..., \widehat{y}_{n_v})^T\)</span>, and the vector of
observed responses in the validation set, <span class="math inline">\(\mathbf{y}_{v_{(n_v \times 1)}} = (y_1, \ldots,
y_{n_v})^T\)</span> with <span class="math inline">\(y_i \in \{0,1\}, \;
i = 1, \ldots, n_v\)</span>.</p>
<p>To construct the ROC curve, only the ordering of the values in <span class="math inline">\(\mathbf{\widehat{y}}_v\)</span> is relevant. Thus,
assuming that there are no ties, and reordering observations if
necessary, we can set <span class="math inline">\(\mathbf{\widehat{y}}_v
= (1, 2, \ldots, n_v)^T\)</span>.</p>
<p>If the AUC can be expressed as the casewise mean or sum of a function
<span class="math inline">\(\mathrm{cv}(\widehat{y}_i,y_i)\)</span>,
where <span class="math inline">\(\mathrm{cv}:
\{1,2,...,n_v\}\times\{0,1\} \rightarrow [0,1]\)</span>, then <span class="math display">\[\begin{equation}
\label{eq:cw}
\tag{1}
\sum_{i=1}^{n_v} \mathrm{cv}(\widehat{y}_i,y_i) =
\mathrm{AUC}(\mathbf{\widehat{y}}_v,\mathbf{y}_v)
\end{equation}\]</span> must hold for all <span class="math inline">\(2^{n_v}\)</span> possible values of <span class="math inline">\(\mathbf{y}_v = (y_1,...,y_{n_v})^T\)</span>. If
all <span class="math inline">\(y\mathrm{s}\)</span> have the same
value, either 1 or 0, then the definition of AUC is ambiguous. AUC could
be considered undefined, or it could be set to 0 if all <span class="math inline">\(y\)</span>s are 0 and to 1 if all <span class="math inline">\(y\)</span>s are 1. If AUC is considered to be
undefined in these cases, we have <span class="math inline">\(2^{n_v} -
2\)</span> admissible values for <span class="math inline">\(\mathbf{y}_v\)</span>.</p>
<p>Thus, equation (<span class="math inline">\(\ref{eq:cw}\)</span>)
produces either <span class="math inline">\(2^{n_v}\)</span> or <span class="math inline">\(2^{n_v}-2\)</span> constraints. Although there are
only <span class="math inline">\(2n_v\)</span> possible values for the
<span class="math inline">\(\mathrm{cv(\cdot)}\)</span> function,
equation (<span class="math inline">\(\ref{eq:cw}\)</span>) could,
nevertheless, have consistent solutions. We therefore need to determine
whether there is a value of <span class="math inline">\(n_v\)</span> for
which (<span class="math inline">\(\ref{eq:cw}\)</span>) has no
consistent solution for all admissible values of <span class="math inline">\(\mathbf{y}_v\)</span>. In that eventuality, we
will have shown that AUC cannot, in general, be expressed through a
casewise sum.</p>
<p>If <span class="math inline">\(n_v=3\)</span>, we show below that
(<span class="math inline">\(\ref{eq:cw}\)</span>) has no consistent
solution if we include all possibilities for <span class="math inline">\(\mathbf{y}_v\)</span>, but does if we exclude
cases where all <span class="math inline">\(y\)</span>s have the same
value. If <span class="math inline">\(n_v=4\)</span>, we show that there
are no consistent solutions in either case.</p>
<p>The following R function computes AUC from <span class="math inline">\(\mathbf{\widehat{y}}_v\)</span> and <span class="math inline">\(\mathbf{y}_v\)</span>, accommodating the cases
where <span class="math inline">\(\mathbf{y}_v\)</span> is all 0s or all
1s:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">AUC</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">y</span>, <span class="va">yhat</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">s</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">s</span> <span class="op">==</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html" class="external-link">return</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="fu">Metrics</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Metrics/man/auc.html" class="external-link">auc</a></span><span class="op">(</span><span class="va">y</span>, <span class="va">yhat</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>We then define a function to generate all possible <span class="math inline">\(\mathbf{y}_v\)</span>s of length <span class="math inline">\(n_v\)</span> as rows of the matrix <span class="math inline">\(\mathbf{Y}_{(2^{n_v} \times n_v)}\)</span>:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Ymat</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n_v</span>, <span class="va">exclude_identical</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/stopifnot.html" class="external-link">stopifnot</a></span><span class="op">(</span><span class="va">n_v</span> <span class="op">&gt;</span> <span class="fl">0</span> <span class="op">&amp;&amp;</span></span>
<span>              <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">n_v</span><span class="op">)</span> <span class="op">==</span> <span class="va">n_v</span><span class="op">)</span>    <span class="co"># n_v must be a positive integer</span></span>
<span>  <span class="va">ret</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="op">(</span><span class="fl">2</span> <span class="op">^</span> <span class="va">n_v</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span>,</span>
<span>                <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>                  <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rawConversion.html" class="external-link">intToBits</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">n_v</span>,<span class="op">]</span></span>
<span>  <span class="va">ret</span> <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">is.matrix</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span></span>
<span>  <span class="kw">else</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"y"</span>, <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">exclude_identical</span><span class="op">)</span></span>
<span>    <span class="va">ret</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">ret</span><span class="op">)</span><span class="op">)</span>,<span class="op">]</span></span>
<span>  <span class="kw">else</span></span>
<span>    <span class="va">ret</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>For <span class="math inline">\(n_v=3\)</span>,</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">Ymat</span><span class="op">(</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;      y1 y2 y3</span></span>
<span><span class="co">#&gt; [1,]  0  0  0</span></span>
<span><span class="co">#&gt; [2,]  1  0  0</span></span>
<span><span class="co">#&gt; [3,]  0  1  0</span></span>
<span><span class="co">#&gt; [4,]  1  1  0</span></span>
<span><span class="co">#&gt; [5,]  0  0  1</span></span>
<span><span class="co">#&gt; [6,]  1  0  1</span></span>
<span><span class="co">#&gt; [7,]  0  1  1</span></span>
<span><span class="co">#&gt; [8,]  1  1  1</span></span></code></pre></div>
<p>If we exclude <span class="math inline">\(\mathbf{y}_v\)</span>s with
identical values, then</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">Ymat</span><span class="op">(</span><span class="fl">3</span>, exclude_identical <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt;      y1 y2 y3</span></span>
<span><span class="co">#&gt; [1,]  1  0  0</span></span>
<span><span class="co">#&gt; [2,]  0  1  0</span></span>
<span><span class="co">#&gt; [3,]  1  1  0</span></span>
<span><span class="co">#&gt; [4,]  0  0  1</span></span>
<span><span class="co">#&gt; [5,]  1  0  1</span></span>
<span><span class="co">#&gt; [6,]  0  1  1</span></span></code></pre></div>
<p>Here is <span class="math inline">\(\mathbf{Y}\)</span> with
corresponding values of AUC:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="fu">Ymat</span><span class="op">(</span><span class="fl">3</span><span class="op">)</span>, AUC <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="fu">Ymat</span><span class="op">(</span><span class="fl">3</span><span class="op">)</span>, <span class="fl">1</span>, <span class="va">AUC</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;      y1 y2 y3 AUC</span></span>
<span><span class="co">#&gt; [1,]  0  0  0 0.0</span></span>
<span><span class="co">#&gt; [2,]  1  0  0 0.0</span></span>
<span><span class="co">#&gt; [3,]  0  1  0 0.5</span></span>
<span><span class="co">#&gt; [4,]  1  1  0 0.0</span></span>
<span><span class="co">#&gt; [5,]  0  0  1 1.0</span></span>
<span><span class="co">#&gt; [6,]  1  0  1 0.5</span></span>
<span><span class="co">#&gt; [7,]  0  1  1 1.0</span></span>
<span><span class="co">#&gt; [8,]  1  1  1 1.0</span></span></code></pre></div>
<p>The values of <span class="math inline">\(\mathrm{cv}(\widehat{y}_i,
y_i)\)</span> that express AUC as a sum of casewise values are solutions
of equation (<span class="math inline">\(\ref{eq:cw}\)</span>), which
can be written as solutions of the following system of <span class="math inline">\(2^{n_v}\)</span> linear simultaneous equations in
<span class="math inline">\(2n_v\)</span> unknowns: <span class="math display">\[\begin{equation}
\label{eq:lin}
\tag{2}
(\mathbf{U} -\mathbf{Y}) \mathbf{c}_0 + \mathbf{Y} \mathbf{c}_1
=
[\mathbf{U} -\mathbf{Y}, \mathbf{Y}]
\begin{bmatrix}
\mathbf{c}_0 \\ \mathbf{c}_1
\end{bmatrix}
= \mathrm{AUC}(\mathbf{\widehat{Y}},\mathbf{Y})
\end{equation}\]</span> where <span class="math inline">\(\mathbf{U}_{(2^{n_v} \times n_v)}\)</span> is a
matrix of 1s conformable with <span class="math inline">\(\mathbf{Y}\)</span>; <span class="math inline">\(\mathbf{c}_0 = [\mathrm{cv}(1,0), c(2,0), ...,
\mathrm{cv}(n_v,0)]^T\)</span>; <span class="math inline">\(\mathbf{c}_1
= [\mathrm{cv}(1,1), c(2,1), ..., \mathrm{cv}(n_v,1)]^T\)</span>; <span class="math inline">\([\mathbf{U} -\mathbf{Y}, \mathbf{Y}]_{(2^{n_v}
\times 2n_v)}\)</span> and <span class="math inline">\(\begin{bmatrix}\begin{aligned}
\mathbf{c}_0 \\ \mathbf{c}_1
\end{aligned}
\end{bmatrix}_{(2n_v \times 1)}\)</span> are partitioned matrices; and
<span class="math inline">\(\mathbf{\widehat{Y}}_{(2^{n_v} \times
n_v)}\)</span> is a matrix each of whose rows consists of the integers 1
to <span class="math inline">\(n_v\)</span>.</p>
<p>We can test whether equation (<span class="math inline">\(\ref{eq:lin}\)</span>) has a solution for any
given <span class="math inline">\(n_v\)</span> by trying to solve it as
a least-squares problem, considering whether the residuals of the
associated linear model are all 0, using the “design matrix” <span class="math inline">\([\mathbf{U} -\mathbf{Y}, \mathbf{Y}]\)</span> to
predict the “outcome” <span class="math inline">\(\mathrm{AUC}(\mathbf{\widehat{Y}},\mathbf{Y})_{(2^{n_v}
\times 1)}\)</span>:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">resids</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n_v</span>,</span>
<span>                   <span class="va">exclude_identical</span> <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>                   <span class="va">tol</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">.Machine</span><span class="op">$</span><span class="va">double.eps</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu">Ymat</span><span class="op">(</span><span class="va">n_v</span>, exclude_identical <span class="op">=</span> <span class="va">exclude_identical</span><span class="op">)</span></span>
<span>  <span class="va">AUC</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">Y</span>, <span class="fl">1</span>, <span class="va">AUC</span><span class="op">)</span></span>
<span>  <span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">Y</span>, <span class="va">Y</span><span class="op">)</span></span>
<span>  <span class="va">opts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>warn <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/on.exit.html" class="external-link">on.exit</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span><span class="va">opts</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lsfit.html" class="external-link">lsfit</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">AUC</span>, intercept <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span>  <span class="va">ret</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">abs</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/residuals.html" class="external-link">residuals</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">ret</span> <span class="op">&lt;</span> <span class="va">tol</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">ret</span> <span class="op">&lt;-</span> <span class="fl">0</span></span>
<span>    <span class="va">solution</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">solution</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"c("</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n_v</span>, <span class="fl">1</span><span class="op">:</span><span class="va">n_v</span><span class="op">)</span>, <span class="st">","</span>,</span>
<span>                              <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span>, each <span class="op">=</span> <span class="va">n_v</span><span class="op">)</span>, <span class="st">")"</span><span class="op">)</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">ret</span>, <span class="st">"solution"</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/zapsmall.html" class="external-link">zapsmall</a></span><span class="op">(</span><span class="va">solution</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span>  <span class="va">ret</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p>The case <span class="math inline">\(n_v=3\)</span>, excluding
identical <span class="math inline">\(y\)</span>s, has a solution:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">resids</span><span class="op">(</span><span class="fl">3</span>, exclude_identical <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0</span></span>
<span><span class="co">#&gt; attr(,"solution")</span></span>
<span><span class="co">#&gt; c(1,0) c(2,0) c(3,0) c(1,1) c(2,1) c(3,1) </span></span>
<span><span class="co">#&gt;    1.0    0.0   -0.5    0.5    0.0    0.0</span></span></code></pre></div>
<p>But, if identical <span class="math inline">\(y\)</span>s are
included, the equation is not consistent:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">resids</span><span class="op">(</span><span class="fl">3</span>, exclude_identical <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.125</span></span></code></pre></div>
<p>For <span class="math inline">\(n_v=4\)</span>, there are no
solutions in either case:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">resids</span><span class="op">(</span><span class="fl">4</span>, exclude_identical <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.083333</span></span></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">resids</span><span class="op">(</span><span class="fl">4</span>, exclude_identical <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.25</span></span></code></pre></div>
<p>Consequently, the widely employed AUC measure of fit for binary
regression cannot in general be used for a casewise cross-validation
criterion.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-ArlotCelisse:2010" class="csl-entry">
Arlot, S., &amp; Celisse, A. (2010). <span class="nocase">A survey of
cross-validation procedures for model selection</span>. <em>Statistics
Surveys</em>, <em>4</em>, 40–79. Retrieved from <a href="https://doi.org/10.1214/09-SS054" class="external-link">https://doi.org/10.1214/09-SS054</a>
</div>
<div id="ref-BatesHastieTibshirani:2023" class="csl-entry">
Bates, S., Hastie, T., &amp; Tibshirani, R. (2023). Cross-validation:
What does it estimate and how well does it do it? <em>Journal of the
American Statistical Association</em>, <em>in press</em>. Retrieved from
<a href="https://doi.org/10.1080/01621459.2023.2197686" class="external-link">https://doi.org/10.1080/01621459.2023.2197686</a>
</div>
<div id="ref-CantyRipley2022" class="csl-entry">
Canty, A., &amp; Ripley, B. D. (2022). <em>Boot: Bootstrap
<span>R</span> (<span>S</span>-plus) functions</em>.
</div>
<div id="ref-DavisonHinkley:1997" class="csl-entry">
Davison, A. C., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and
their applications</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Fox:2016" class="csl-entry">
Fox, J. (2016). <em>Applied regression analysis and generalized linear
models</em> (Second edition). Thousand Oaks <span>CA</span>: Sage.
</div>
<div id="ref-FoxWeisberg:2019" class="csl-entry">
Fox, J., &amp; Weisberg, S. (2019). <em>An <span>R</span> companion to
applied regression</em> (Third edition). Thousand Oaks <span>CA</span>:
Sage.
</div>
<div id="ref-Hager:1989" class="csl-entry">
Hager, W. W. (1989). Updating the inverse of a matrix.
<em><span>SIAM</span> Review</em>, <em>31</em>(2), 221–239.
</div>
<div id="ref-Mersmann:2023" class="csl-entry">
Mersmann, O. (2023). <em>Microbenchmark: Accurate timing functions</em>.
Retrieved from <a href="https://CRAN.R-project.org/package=microbenchmark" class="external-link">https://CRAN.R-project.org/package=microbenchmark</a>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by John Fox, Georges Monette.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
